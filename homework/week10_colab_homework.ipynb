{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n44rMWcLS-BY"
   },
   "source": [
    "# Week 10: Colab Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCMvaDkMTJkT"
   },
   "source": [
    "# I. Introduction\n",
    "In this exercise, we apply CNN to MNIST data to classify the hand written digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2jxq00nbuCwt"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdXkw_9pkfn5"
   },
   "source": [
    "# Data Loading\n",
    "Load the data from the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PoUAesyDuL0n"
   },
   "outputs": [],
   "source": [
    "# Run this once to load the train and test data straight into a dataloader class\n",
    "# that will provide the batches\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('../data/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('../data/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRXOY0Tzkfn6"
   },
   "source": [
    "# Visualize dataset sample\n",
    "Show some sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "boEAxlB5uPZx"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's draw some of the training data\n",
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_94InEd1TkC6"
   },
   "source": [
    "# II. Methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "y_kwOzXQuWNK"
   },
   "outputs": [],
   "source": [
    "from os import X_OK\n",
    "\n",
    "# This class implements a minimal network (which still does okay)\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Valid convolution, 1 channel in, 2 channels out, stride 1, kernel size = 3\n",
    "        self.conv1 = nn.Conv2d(1, 2, kernel_size=3)\n",
    "        # Dropout for convolutions\n",
    "        self.drop = nn.Dropout2d()\n",
    "        # Fully connected layer\n",
    "        self.fc1 = nn.Linear(338, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.drop(x)\n",
    "        x = F.max_pool2d(x,2)\n",
    "        x = F.relu(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.log_softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "d4Ue45Pnf8gZ"
   },
   "outputs": [],
   "source": [
    "# TODO: Change above Net to Net2 class to implement\n",
    "\n",
    "# 1. A valid convolution with kernel size 5, 1 input channel and 10 output channels\n",
    "# 2. A max pooling operation over a 2x2 area\n",
    "# 3. A Relu\n",
    "# 4. A valid convolution with kernel size 5, 10 input channels and 20 output channels\n",
    "# 5. A 2D Dropout layer\n",
    "# 6. A max pooling operation over a 2x2 area\n",
    "# 7. A relu\n",
    "# 8. A flattening operation\n",
    "# 9. A fully connected layer mapping from (whatever dimensions we are at-- find out using .shape) to 50\n",
    "# 10. A ReLU\n",
    "# 11. A fully connected layer mapping from 50 to 10 dimensions\n",
    "# 12. A softmax function.\n",
    "\n",
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Net2, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 10, kernel_size=5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(10, 20, kernel_size=5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d()\n",
    "        )\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(1216)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9sN5hsK2uan8"
   },
   "outputs": [],
   "source": [
    "# He initialization of weights\n",
    "def weights_init(layer_in):\n",
    "  if isinstance(layer_in, nn.Linear):\n",
    "    nn.init.kaiming_uniform_(layer_in.weight)\n",
    "    layer_in.bias.data.fill_(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2pBDgYp2ufUi"
   },
   "outputs": [],
   "source": [
    "# Main training routine\n",
    "# TODO: Read it and understand what it does, you would need to implement it in the next colab HW\n",
    "\n",
    "def train(epoch, model, optimizer):\n",
    "  model.train()\n",
    "  # Get each\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = F.nll_loss(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # Store results\n",
    "    if batch_idx % 10 == 0:\n",
    "\n",
    "      pred = output.data.max(1, keepdim=True)[1]\n",
    "      correct = pred.eq(target.data.view_as(pred)).sum()\n",
    "      print('Train Epoch: {} [{}/{}]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_idx * len(data), len(train_loader.dataset), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Xr6yXzWduhbU"
   },
   "outputs": [],
   "source": [
    "# Run on test data\n",
    "# TODO: Read it and understand what it does, you would need to implement it in the next colab HW\n",
    "\n",
    "def test(model):\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "      output = model(data)\n",
    "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "      pred = output.data.max(1, keepdim=True)[1]\n",
    "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))\n",
    "  return 100. * correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "EVUrbYiamki8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\codeamon\\AppData\\Local\\Temp\\ipykernel_106904\\1795650187.py:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.log_softmax(x)\n",
      "c:\\Users\\codeamon\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.5911, Accuracy: 1002/10000 (10%)\n",
      "\n",
      "Train Epoch: 1 [0/60000]\tLoss: 2.762738\n",
      "Train Epoch: 1 [640/60000]\tLoss: 2.432890\n",
      "Train Epoch: 1 [1280/60000]\tLoss: 2.275907\n",
      "Train Epoch: 1 [1920/60000]\tLoss: 2.243393\n",
      "Train Epoch: 1 [2560/60000]\tLoss: 2.205828\n",
      "Train Epoch: 1 [3200/60000]\tLoss: 2.098188\n",
      "Train Epoch: 1 [3840/60000]\tLoss: 2.121952\n",
      "Train Epoch: 1 [4480/60000]\tLoss: 1.931935\n",
      "Train Epoch: 1 [5120/60000]\tLoss: 1.940472\n",
      "Train Epoch: 1 [5760/60000]\tLoss: 2.020670\n",
      "Train Epoch: 1 [6400/60000]\tLoss: 1.814095\n",
      "Train Epoch: 1 [7040/60000]\tLoss: 1.604658\n",
      "Train Epoch: 1 [7680/60000]\tLoss: 1.602771\n",
      "Train Epoch: 1 [8320/60000]\tLoss: 1.577708\n",
      "Train Epoch: 1 [8960/60000]\tLoss: 1.465954\n",
      "Train Epoch: 1 [9600/60000]\tLoss: 1.235356\n",
      "Train Epoch: 1 [10240/60000]\tLoss: 1.072694\n",
      "Train Epoch: 1 [10880/60000]\tLoss: 1.366254\n",
      "Train Epoch: 1 [11520/60000]\tLoss: 1.169053\n",
      "Train Epoch: 1 [12160/60000]\tLoss: 1.393694\n",
      "Train Epoch: 1 [12800/60000]\tLoss: 1.171778\n",
      "Train Epoch: 1 [13440/60000]\tLoss: 1.426847\n",
      "Train Epoch: 1 [14080/60000]\tLoss: 1.375982\n",
      "Train Epoch: 1 [14720/60000]\tLoss: 0.942653\n",
      "Train Epoch: 1 [15360/60000]\tLoss: 0.984291\n",
      "Train Epoch: 1 [16000/60000]\tLoss: 1.136934\n",
      "Train Epoch: 1 [16640/60000]\tLoss: 1.482183\n",
      "Train Epoch: 1 [17280/60000]\tLoss: 1.090477\n",
      "Train Epoch: 1 [17920/60000]\tLoss: 1.021039\n",
      "Train Epoch: 1 [18560/60000]\tLoss: 1.178521\n",
      "Train Epoch: 1 [19200/60000]\tLoss: 1.168586\n",
      "Train Epoch: 1 [19840/60000]\tLoss: 1.006155\n",
      "Train Epoch: 1 [20480/60000]\tLoss: 1.014420\n",
      "Train Epoch: 1 [21120/60000]\tLoss: 0.919939\n",
      "Train Epoch: 1 [21760/60000]\tLoss: 0.896194\n",
      "Train Epoch: 1 [22400/60000]\tLoss: 1.021513\n",
      "Train Epoch: 1 [23040/60000]\tLoss: 1.105639\n",
      "Train Epoch: 1 [23680/60000]\tLoss: 1.093933\n",
      "Train Epoch: 1 [24320/60000]\tLoss: 0.965500\n",
      "Train Epoch: 1 [24960/60000]\tLoss: 0.969583\n",
      "Train Epoch: 1 [25600/60000]\tLoss: 1.100296\n",
      "Train Epoch: 1 [26240/60000]\tLoss: 1.138158\n",
      "Train Epoch: 1 [26880/60000]\tLoss: 1.007006\n",
      "Train Epoch: 1 [27520/60000]\tLoss: 1.015995\n",
      "Train Epoch: 1 [28160/60000]\tLoss: 0.999421\n",
      "Train Epoch: 1 [28800/60000]\tLoss: 0.977589\n",
      "Train Epoch: 1 [29440/60000]\tLoss: 0.828548\n",
      "Train Epoch: 1 [30080/60000]\tLoss: 1.162834\n",
      "Train Epoch: 1 [30720/60000]\tLoss: 0.587911\n",
      "Train Epoch: 1 [31360/60000]\tLoss: 0.860236\n",
      "Train Epoch: 1 [32000/60000]\tLoss: 1.036460\n",
      "Train Epoch: 1 [32640/60000]\tLoss: 0.928109\n",
      "Train Epoch: 1 [33280/60000]\tLoss: 0.808570\n",
      "Train Epoch: 1 [33920/60000]\tLoss: 0.958581\n",
      "Train Epoch: 1 [34560/60000]\tLoss: 0.784909\n",
      "Train Epoch: 1 [35200/60000]\tLoss: 0.964524\n",
      "Train Epoch: 1 [35840/60000]\tLoss: 0.915377\n",
      "Train Epoch: 1 [36480/60000]\tLoss: 0.862984\n",
      "Train Epoch: 1 [37120/60000]\tLoss: 1.050035\n",
      "Train Epoch: 1 [37760/60000]\tLoss: 0.962402\n",
      "Train Epoch: 1 [38400/60000]\tLoss: 1.110152\n",
      "Train Epoch: 1 [39040/60000]\tLoss: 1.066692\n",
      "Train Epoch: 1 [39680/60000]\tLoss: 0.778464\n",
      "Train Epoch: 1 [40320/60000]\tLoss: 1.111131\n",
      "Train Epoch: 1 [40960/60000]\tLoss: 0.787848\n",
      "Train Epoch: 1 [41600/60000]\tLoss: 1.144980\n",
      "Train Epoch: 1 [42240/60000]\tLoss: 0.926224\n",
      "Train Epoch: 1 [42880/60000]\tLoss: 1.012123\n",
      "Train Epoch: 1 [43520/60000]\tLoss: 0.983930\n",
      "Train Epoch: 1 [44160/60000]\tLoss: 1.024343\n",
      "Train Epoch: 1 [44800/60000]\tLoss: 0.880558\n",
      "Train Epoch: 1 [45440/60000]\tLoss: 0.755966\n",
      "Train Epoch: 1 [46080/60000]\tLoss: 1.033246\n",
      "Train Epoch: 1 [46720/60000]\tLoss: 0.897714\n",
      "Train Epoch: 1 [47360/60000]\tLoss: 0.959356\n",
      "Train Epoch: 1 [48000/60000]\tLoss: 1.182370\n",
      "Train Epoch: 1 [48640/60000]\tLoss: 1.030218\n",
      "Train Epoch: 1 [49280/60000]\tLoss: 0.648861\n",
      "Train Epoch: 1 [49920/60000]\tLoss: 0.947175\n",
      "Train Epoch: 1 [50560/60000]\tLoss: 0.840905\n",
      "Train Epoch: 1 [51200/60000]\tLoss: 1.024026\n",
      "Train Epoch: 1 [51840/60000]\tLoss: 1.029697\n",
      "Train Epoch: 1 [52480/60000]\tLoss: 0.871383\n",
      "Train Epoch: 1 [53120/60000]\tLoss: 1.230563\n",
      "Train Epoch: 1 [53760/60000]\tLoss: 1.102122\n",
      "Train Epoch: 1 [54400/60000]\tLoss: 0.927177\n",
      "Train Epoch: 1 [55040/60000]\tLoss: 0.975919\n",
      "Train Epoch: 1 [55680/60000]\tLoss: 0.767197\n",
      "Train Epoch: 1 [56320/60000]\tLoss: 1.095639\n",
      "Train Epoch: 1 [56960/60000]\tLoss: 0.741668\n",
      "Train Epoch: 1 [57600/60000]\tLoss: 0.902088\n",
      "Train Epoch: 1 [58240/60000]\tLoss: 0.725825\n",
      "Train Epoch: 1 [58880/60000]\tLoss: 0.982302\n",
      "Train Epoch: 1 [59520/60000]\tLoss: 0.800922\n",
      "Train Epoch: 2 [0/60000]\tLoss: 0.896339\n",
      "Train Epoch: 2 [640/60000]\tLoss: 1.024276\n",
      "Train Epoch: 2 [1280/60000]\tLoss: 0.885431\n",
      "Train Epoch: 2 [1920/60000]\tLoss: 0.983628\n",
      "Train Epoch: 2 [2560/60000]\tLoss: 0.851089\n",
      "Train Epoch: 2 [3200/60000]\tLoss: 0.688676\n",
      "Train Epoch: 2 [3840/60000]\tLoss: 0.775595\n",
      "Train Epoch: 2 [4480/60000]\tLoss: 1.216163\n",
      "Train Epoch: 2 [5120/60000]\tLoss: 1.151595\n",
      "Train Epoch: 2 [5760/60000]\tLoss: 1.189268\n",
      "Train Epoch: 2 [6400/60000]\tLoss: 1.143876\n",
      "Train Epoch: 2 [7040/60000]\tLoss: 0.799336\n",
      "Train Epoch: 2 [7680/60000]\tLoss: 0.957646\n",
      "Train Epoch: 2 [8320/60000]\tLoss: 0.885697\n",
      "Train Epoch: 2 [8960/60000]\tLoss: 0.971608\n",
      "Train Epoch: 2 [9600/60000]\tLoss: 0.895878\n",
      "Train Epoch: 2 [10240/60000]\tLoss: 1.067112\n",
      "Train Epoch: 2 [10880/60000]\tLoss: 0.911476\n",
      "Train Epoch: 2 [11520/60000]\tLoss: 0.958603\n",
      "Train Epoch: 2 [12160/60000]\tLoss: 0.816774\n",
      "Train Epoch: 2 [12800/60000]\tLoss: 0.968840\n",
      "Train Epoch: 2 [13440/60000]\tLoss: 0.808885\n",
      "Train Epoch: 2 [14080/60000]\tLoss: 0.862944\n",
      "Train Epoch: 2 [14720/60000]\tLoss: 1.001604\n",
      "Train Epoch: 2 [15360/60000]\tLoss: 0.870017\n",
      "Train Epoch: 2 [16000/60000]\tLoss: 0.859053\n",
      "Train Epoch: 2 [16640/60000]\tLoss: 0.827732\n",
      "Train Epoch: 2 [17280/60000]\tLoss: 1.014756\n",
      "Train Epoch: 2 [17920/60000]\tLoss: 0.872954\n",
      "Train Epoch: 2 [18560/60000]\tLoss: 0.935078\n",
      "Train Epoch: 2 [19200/60000]\tLoss: 0.819298\n",
      "Train Epoch: 2 [19840/60000]\tLoss: 0.904580\n",
      "Train Epoch: 2 [20480/60000]\tLoss: 0.973371\n",
      "Train Epoch: 2 [21120/60000]\tLoss: 0.851742\n",
      "Train Epoch: 2 [21760/60000]\tLoss: 0.956164\n",
      "Train Epoch: 2 [22400/60000]\tLoss: 1.159710\n",
      "Train Epoch: 2 [23040/60000]\tLoss: 0.749607\n",
      "Train Epoch: 2 [23680/60000]\tLoss: 0.744347\n",
      "Train Epoch: 2 [24320/60000]\tLoss: 0.853433\n",
      "Train Epoch: 2 [24960/60000]\tLoss: 1.223366\n",
      "Train Epoch: 2 [25600/60000]\tLoss: 0.653593\n",
      "Train Epoch: 2 [26240/60000]\tLoss: 0.415518\n",
      "Train Epoch: 2 [26880/60000]\tLoss: 0.810452\n",
      "Train Epoch: 2 [27520/60000]\tLoss: 0.714825\n",
      "Train Epoch: 2 [28160/60000]\tLoss: 0.816243\n",
      "Train Epoch: 2 [28800/60000]\tLoss: 0.986067\n",
      "Train Epoch: 2 [29440/60000]\tLoss: 1.072656\n",
      "Train Epoch: 2 [30080/60000]\tLoss: 0.768057\n",
      "Train Epoch: 2 [30720/60000]\tLoss: 0.822327\n",
      "Train Epoch: 2 [31360/60000]\tLoss: 1.185195\n",
      "Train Epoch: 2 [32000/60000]\tLoss: 0.752296\n",
      "Train Epoch: 2 [32640/60000]\tLoss: 0.883508\n",
      "Train Epoch: 2 [33280/60000]\tLoss: 1.046222\n",
      "Train Epoch: 2 [33920/60000]\tLoss: 0.861226\n",
      "Train Epoch: 2 [34560/60000]\tLoss: 1.032342\n",
      "Train Epoch: 2 [35200/60000]\tLoss: 0.722345\n",
      "Train Epoch: 2 [35840/60000]\tLoss: 0.840198\n",
      "Train Epoch: 2 [36480/60000]\tLoss: 0.887465\n",
      "Train Epoch: 2 [37120/60000]\tLoss: 0.719590\n",
      "Train Epoch: 2 [37760/60000]\tLoss: 0.837420\n",
      "Train Epoch: 2 [38400/60000]\tLoss: 1.120228\n",
      "Train Epoch: 2 [39040/60000]\tLoss: 0.866027\n",
      "Train Epoch: 2 [39680/60000]\tLoss: 0.771391\n",
      "Train Epoch: 2 [40320/60000]\tLoss: 0.791196\n",
      "Train Epoch: 2 [40960/60000]\tLoss: 0.763541\n",
      "Train Epoch: 2 [41600/60000]\tLoss: 0.891446\n",
      "Train Epoch: 2 [42240/60000]\tLoss: 0.876566\n",
      "Train Epoch: 2 [42880/60000]\tLoss: 0.849056\n",
      "Train Epoch: 2 [43520/60000]\tLoss: 0.837634\n",
      "Train Epoch: 2 [44160/60000]\tLoss: 0.712902\n",
      "Train Epoch: 2 [44800/60000]\tLoss: 1.219221\n",
      "Train Epoch: 2 [45440/60000]\tLoss: 0.898466\n",
      "Train Epoch: 2 [46080/60000]\tLoss: 0.880060\n",
      "Train Epoch: 2 [46720/60000]\tLoss: 1.074045\n",
      "Train Epoch: 2 [47360/60000]\tLoss: 0.724910\n",
      "Train Epoch: 2 [48000/60000]\tLoss: 0.791585\n",
      "Train Epoch: 2 [48640/60000]\tLoss: 0.810679\n",
      "Train Epoch: 2 [49280/60000]\tLoss: 1.139318\n",
      "Train Epoch: 2 [49920/60000]\tLoss: 0.893724\n",
      "Train Epoch: 2 [50560/60000]\tLoss: 0.819374\n",
      "Train Epoch: 2 [51200/60000]\tLoss: 0.883250\n",
      "Train Epoch: 2 [51840/60000]\tLoss: 0.741396\n",
      "Train Epoch: 2 [52480/60000]\tLoss: 0.828121\n",
      "Train Epoch: 2 [53120/60000]\tLoss: 0.777335\n",
      "Train Epoch: 2 [53760/60000]\tLoss: 0.779433\n",
      "Train Epoch: 2 [54400/60000]\tLoss: 0.732336\n",
      "Train Epoch: 2 [55040/60000]\tLoss: 1.008033\n",
      "Train Epoch: 2 [55680/60000]\tLoss: 0.617356\n",
      "Train Epoch: 2 [56320/60000]\tLoss: 0.893238\n",
      "Train Epoch: 2 [56960/60000]\tLoss: 0.884669\n",
      "Train Epoch: 2 [57600/60000]\tLoss: 1.084377\n",
      "Train Epoch: 2 [58240/60000]\tLoss: 0.829086\n",
      "Train Epoch: 2 [58880/60000]\tLoss: 0.638983\n",
      "Train Epoch: 2 [59520/60000]\tLoss: 0.681172\n",
      "Train Epoch: 3 [0/60000]\tLoss: 0.867835\n",
      "Train Epoch: 3 [640/60000]\tLoss: 0.894279\n",
      "Train Epoch: 3 [1280/60000]\tLoss: 0.556617\n",
      "Train Epoch: 3 [1920/60000]\tLoss: 0.962763\n",
      "Train Epoch: 3 [2560/60000]\tLoss: 0.918560\n",
      "Train Epoch: 3 [3200/60000]\tLoss: 0.932156\n",
      "Train Epoch: 3 [3840/60000]\tLoss: 0.679337\n",
      "Train Epoch: 3 [4480/60000]\tLoss: 0.993792\n",
      "Train Epoch: 3 [5120/60000]\tLoss: 0.779768\n",
      "Train Epoch: 3 [5760/60000]\tLoss: 1.160532\n",
      "Train Epoch: 3 [6400/60000]\tLoss: 1.013154\n",
      "Train Epoch: 3 [7040/60000]\tLoss: 0.919132\n",
      "Train Epoch: 3 [7680/60000]\tLoss: 0.934341\n",
      "Train Epoch: 3 [8320/60000]\tLoss: 0.760519\n",
      "Train Epoch: 3 [8960/60000]\tLoss: 0.742856\n",
      "Train Epoch: 3 [9600/60000]\tLoss: 0.606630\n",
      "Train Epoch: 3 [10240/60000]\tLoss: 0.875159\n",
      "Train Epoch: 3 [10880/60000]\tLoss: 1.023219\n",
      "Train Epoch: 3 [11520/60000]\tLoss: 1.046267\n",
      "Train Epoch: 3 [12160/60000]\tLoss: 0.850274\n",
      "Train Epoch: 3 [12800/60000]\tLoss: 0.738397\n",
      "Train Epoch: 3 [13440/60000]\tLoss: 1.229946\n",
      "Train Epoch: 3 [14080/60000]\tLoss: 0.919362\n",
      "Train Epoch: 3 [14720/60000]\tLoss: 0.819109\n",
      "Train Epoch: 3 [15360/60000]\tLoss: 0.882113\n",
      "Train Epoch: 3 [16000/60000]\tLoss: 0.705523\n",
      "Train Epoch: 3 [16640/60000]\tLoss: 0.900482\n",
      "Train Epoch: 3 [17280/60000]\tLoss: 0.644792\n",
      "Train Epoch: 3 [17920/60000]\tLoss: 1.020621\n",
      "Train Epoch: 3 [18560/60000]\tLoss: 0.718436\n",
      "Train Epoch: 3 [19200/60000]\tLoss: 0.833146\n",
      "Train Epoch: 3 [19840/60000]\tLoss: 0.918902\n",
      "Train Epoch: 3 [20480/60000]\tLoss: 0.801329\n",
      "Train Epoch: 3 [21120/60000]\tLoss: 0.605947\n",
      "Train Epoch: 3 [21760/60000]\tLoss: 0.763516\n",
      "Train Epoch: 3 [22400/60000]\tLoss: 1.214885\n",
      "Train Epoch: 3 [23040/60000]\tLoss: 0.817221\n",
      "Train Epoch: 3 [23680/60000]\tLoss: 0.588490\n",
      "Train Epoch: 3 [24320/60000]\tLoss: 0.773650\n",
      "Train Epoch: 3 [24960/60000]\tLoss: 0.994088\n",
      "Train Epoch: 3 [25600/60000]\tLoss: 1.084608\n",
      "Train Epoch: 3 [26240/60000]\tLoss: 0.805622\n",
      "Train Epoch: 3 [26880/60000]\tLoss: 0.783025\n",
      "Train Epoch: 3 [27520/60000]\tLoss: 0.833894\n",
      "Train Epoch: 3 [28160/60000]\tLoss: 0.641790\n",
      "Train Epoch: 3 [28800/60000]\tLoss: 0.699424\n",
      "Train Epoch: 3 [29440/60000]\tLoss: 0.732224\n",
      "Train Epoch: 3 [30080/60000]\tLoss: 0.875938\n",
      "Train Epoch: 3 [30720/60000]\tLoss: 1.086584\n",
      "Train Epoch: 3 [31360/60000]\tLoss: 0.757428\n",
      "Train Epoch: 3 [32000/60000]\tLoss: 0.681921\n",
      "Train Epoch: 3 [32640/60000]\tLoss: 0.847697\n",
      "Train Epoch: 3 [33280/60000]\tLoss: 0.819362\n",
      "Train Epoch: 3 [33920/60000]\tLoss: 0.934370\n",
      "Train Epoch: 3 [34560/60000]\tLoss: 0.590729\n",
      "Train Epoch: 3 [35200/60000]\tLoss: 0.771643\n",
      "Train Epoch: 3 [35840/60000]\tLoss: 0.911334\n",
      "Train Epoch: 3 [36480/60000]\tLoss: 0.939506\n",
      "Train Epoch: 3 [37120/60000]\tLoss: 1.005194\n",
      "Train Epoch: 3 [37760/60000]\tLoss: 0.814017\n",
      "Train Epoch: 3 [38400/60000]\tLoss: 0.798468\n",
      "Train Epoch: 3 [39040/60000]\tLoss: 0.793600\n",
      "Train Epoch: 3 [39680/60000]\tLoss: 0.978895\n",
      "Train Epoch: 3 [40320/60000]\tLoss: 0.796344\n",
      "Train Epoch: 3 [40960/60000]\tLoss: 0.861575\n",
      "Train Epoch: 3 [41600/60000]\tLoss: 1.370135\n",
      "Train Epoch: 3 [42240/60000]\tLoss: 0.791184\n",
      "Train Epoch: 3 [42880/60000]\tLoss: 0.953117\n",
      "Train Epoch: 3 [43520/60000]\tLoss: 0.723966\n",
      "Train Epoch: 3 [44160/60000]\tLoss: 0.727299\n",
      "Train Epoch: 3 [44800/60000]\tLoss: 0.709104\n",
      "Train Epoch: 3 [45440/60000]\tLoss: 0.862392\n",
      "Train Epoch: 3 [46080/60000]\tLoss: 1.443067\n",
      "Train Epoch: 3 [46720/60000]\tLoss: 0.909634\n",
      "Train Epoch: 3 [47360/60000]\tLoss: 0.918182\n",
      "Train Epoch: 3 [48000/60000]\tLoss: 0.906927\n",
      "Train Epoch: 3 [48640/60000]\tLoss: 1.119457\n",
      "Train Epoch: 3 [49280/60000]\tLoss: 1.417048\n",
      "Train Epoch: 3 [49920/60000]\tLoss: 0.793869\n",
      "Train Epoch: 3 [50560/60000]\tLoss: 0.873338\n",
      "Train Epoch: 3 [51200/60000]\tLoss: 0.882290\n",
      "Train Epoch: 3 [51840/60000]\tLoss: 1.054952\n",
      "Train Epoch: 3 [52480/60000]\tLoss: 0.751821\n",
      "Train Epoch: 3 [53120/60000]\tLoss: 0.725266\n",
      "Train Epoch: 3 [53760/60000]\tLoss: 0.889920\n",
      "Train Epoch: 3 [54400/60000]\tLoss: 1.019847\n",
      "Train Epoch: 3 [55040/60000]\tLoss: 0.746575\n",
      "Train Epoch: 3 [55680/60000]\tLoss: 1.024538\n",
      "Train Epoch: 3 [56320/60000]\tLoss: 0.848773\n",
      "Train Epoch: 3 [56960/60000]\tLoss: 0.935403\n",
      "Train Epoch: 3 [57600/60000]\tLoss: 0.815124\n",
      "Train Epoch: 3 [58240/60000]\tLoss: 0.580045\n",
      "Train Epoch: 3 [58880/60000]\tLoss: 1.007580\n",
      "Train Epoch: 3 [59520/60000]\tLoss: 0.896527\n",
      "Train Epoch: 4 [0/60000]\tLoss: 0.837201\n",
      "Train Epoch: 4 [640/60000]\tLoss: 1.041850\n",
      "Train Epoch: 4 [1280/60000]\tLoss: 0.746853\n",
      "Train Epoch: 4 [1920/60000]\tLoss: 0.874236\n",
      "Train Epoch: 4 [2560/60000]\tLoss: 0.921436\n",
      "Train Epoch: 4 [3200/60000]\tLoss: 0.720218\n",
      "Train Epoch: 4 [3840/60000]\tLoss: 0.859909\n",
      "Train Epoch: 4 [4480/60000]\tLoss: 1.007830\n",
      "Train Epoch: 4 [5120/60000]\tLoss: 0.868763\n",
      "Train Epoch: 4 [5760/60000]\tLoss: 0.622120\n",
      "Train Epoch: 4 [6400/60000]\tLoss: 0.771560\n",
      "Train Epoch: 4 [7040/60000]\tLoss: 0.907542\n",
      "Train Epoch: 4 [7680/60000]\tLoss: 0.879451\n",
      "Train Epoch: 4 [8320/60000]\tLoss: 0.815425\n",
      "Train Epoch: 4 [8960/60000]\tLoss: 0.947977\n",
      "Train Epoch: 4 [9600/60000]\tLoss: 0.787416\n",
      "Train Epoch: 4 [10240/60000]\tLoss: 0.928834\n",
      "Train Epoch: 4 [10880/60000]\tLoss: 0.864875\n",
      "Train Epoch: 4 [11520/60000]\tLoss: 0.819347\n",
      "Train Epoch: 4 [12160/60000]\tLoss: 1.017492\n",
      "Train Epoch: 4 [12800/60000]\tLoss: 0.928626\n",
      "Train Epoch: 4 [13440/60000]\tLoss: 0.825706\n",
      "Train Epoch: 4 [14080/60000]\tLoss: 0.811657\n",
      "Train Epoch: 4 [14720/60000]\tLoss: 0.844190\n",
      "Train Epoch: 4 [15360/60000]\tLoss: 1.017322\n",
      "Train Epoch: 4 [16000/60000]\tLoss: 0.628683\n",
      "Train Epoch: 4 [16640/60000]\tLoss: 0.716233\n",
      "Train Epoch: 4 [17280/60000]\tLoss: 1.014161\n",
      "Train Epoch: 4 [17920/60000]\tLoss: 0.732379\n",
      "Train Epoch: 4 [18560/60000]\tLoss: 0.753248\n",
      "Train Epoch: 4 [19200/60000]\tLoss: 0.714640\n",
      "Train Epoch: 4 [19840/60000]\tLoss: 0.757898\n",
      "Train Epoch: 4 [20480/60000]\tLoss: 0.570065\n",
      "Train Epoch: 4 [21120/60000]\tLoss: 0.723638\n",
      "Train Epoch: 4 [21760/60000]\tLoss: 0.829063\n",
      "Train Epoch: 4 [22400/60000]\tLoss: 1.205931\n",
      "Train Epoch: 4 [23040/60000]\tLoss: 0.986843\n",
      "Train Epoch: 4 [23680/60000]\tLoss: 0.975175\n",
      "Train Epoch: 4 [24320/60000]\tLoss: 0.716492\n",
      "Train Epoch: 4 [24960/60000]\tLoss: 0.926400\n",
      "Train Epoch: 4 [25600/60000]\tLoss: 0.637277\n",
      "Train Epoch: 4 [26240/60000]\tLoss: 0.792518\n",
      "Train Epoch: 4 [26880/60000]\tLoss: 0.909170\n",
      "Train Epoch: 4 [27520/60000]\tLoss: 0.813049\n",
      "Train Epoch: 4 [28160/60000]\tLoss: 0.935258\n",
      "Train Epoch: 4 [28800/60000]\tLoss: 1.168580\n",
      "Train Epoch: 4 [29440/60000]\tLoss: 0.868772\n",
      "Train Epoch: 4 [30080/60000]\tLoss: 1.142891\n",
      "Train Epoch: 4 [30720/60000]\tLoss: 0.761152\n",
      "Train Epoch: 4 [31360/60000]\tLoss: 0.899482\n",
      "Train Epoch: 4 [32000/60000]\tLoss: 0.790441\n",
      "Train Epoch: 4 [32640/60000]\tLoss: 0.960279\n",
      "Train Epoch: 4 [33280/60000]\tLoss: 0.801455\n",
      "Train Epoch: 4 [33920/60000]\tLoss: 0.894377\n",
      "Train Epoch: 4 [34560/60000]\tLoss: 0.748923\n",
      "Train Epoch: 4 [35200/60000]\tLoss: 1.058421\n",
      "Train Epoch: 4 [35840/60000]\tLoss: 0.905703\n",
      "Train Epoch: 4 [36480/60000]\tLoss: 1.055783\n",
      "Train Epoch: 4 [37120/60000]\tLoss: 0.872549\n",
      "Train Epoch: 4 [37760/60000]\tLoss: 0.686953\n",
      "Train Epoch: 4 [38400/60000]\tLoss: 1.022364\n",
      "Train Epoch: 4 [39040/60000]\tLoss: 0.879079\n",
      "Train Epoch: 4 [39680/60000]\tLoss: 0.958639\n",
      "Train Epoch: 4 [40320/60000]\tLoss: 0.986795\n",
      "Train Epoch: 4 [40960/60000]\tLoss: 0.700084\n",
      "Train Epoch: 4 [41600/60000]\tLoss: 0.681653\n",
      "Train Epoch: 4 [42240/60000]\tLoss: 1.077585\n",
      "Train Epoch: 4 [42880/60000]\tLoss: 1.033693\n",
      "Train Epoch: 4 [43520/60000]\tLoss: 0.820320\n",
      "Train Epoch: 4 [44160/60000]\tLoss: 0.801192\n",
      "Train Epoch: 4 [44800/60000]\tLoss: 1.147781\n",
      "Train Epoch: 4 [45440/60000]\tLoss: 0.738050\n",
      "Train Epoch: 4 [46080/60000]\tLoss: 0.819698\n",
      "Train Epoch: 4 [46720/60000]\tLoss: 0.750056\n",
      "Train Epoch: 4 [47360/60000]\tLoss: 1.000471\n",
      "Train Epoch: 4 [48000/60000]\tLoss: 0.859765\n",
      "Train Epoch: 4 [48640/60000]\tLoss: 0.901552\n",
      "Train Epoch: 4 [49280/60000]\tLoss: 1.103107\n",
      "Train Epoch: 4 [49920/60000]\tLoss: 1.003097\n",
      "Train Epoch: 4 [50560/60000]\tLoss: 0.790068\n",
      "Train Epoch: 4 [51200/60000]\tLoss: 0.920294\n",
      "Train Epoch: 4 [51840/60000]\tLoss: 0.810008\n",
      "Train Epoch: 4 [52480/60000]\tLoss: 0.900095\n",
      "Train Epoch: 4 [53120/60000]\tLoss: 0.964963\n",
      "Train Epoch: 4 [53760/60000]\tLoss: 0.657341\n",
      "Train Epoch: 4 [54400/60000]\tLoss: 0.664834\n",
      "Train Epoch: 4 [55040/60000]\tLoss: 0.717581\n",
      "Train Epoch: 4 [55680/60000]\tLoss: 0.787858\n",
      "Train Epoch: 4 [56320/60000]\tLoss: 0.649148\n",
      "Train Epoch: 4 [56960/60000]\tLoss: 0.944601\n",
      "Train Epoch: 4 [57600/60000]\tLoss: 0.883665\n",
      "Train Epoch: 4 [58240/60000]\tLoss: 0.755694\n",
      "Train Epoch: 4 [58880/60000]\tLoss: 0.817848\n",
      "Train Epoch: 4 [59520/60000]\tLoss: 1.023566\n",
      "Train Epoch: 5 [0/60000]\tLoss: 0.665899\n",
      "Train Epoch: 5 [640/60000]\tLoss: 0.855166\n",
      "Train Epoch: 5 [1280/60000]\tLoss: 0.630160\n",
      "Train Epoch: 5 [1920/60000]\tLoss: 0.796618\n",
      "Train Epoch: 5 [2560/60000]\tLoss: 0.570498\n",
      "Train Epoch: 5 [3200/60000]\tLoss: 0.902979\n",
      "Train Epoch: 5 [3840/60000]\tLoss: 0.840385\n",
      "Train Epoch: 5 [4480/60000]\tLoss: 1.017256\n",
      "Train Epoch: 5 [5120/60000]\tLoss: 0.847203\n",
      "Train Epoch: 5 [5760/60000]\tLoss: 0.945987\n",
      "Train Epoch: 5 [6400/60000]\tLoss: 0.880445\n",
      "Train Epoch: 5 [7040/60000]\tLoss: 0.995551\n",
      "Train Epoch: 5 [7680/60000]\tLoss: 0.664922\n",
      "Train Epoch: 5 [8320/60000]\tLoss: 0.883639\n",
      "Train Epoch: 5 [8960/60000]\tLoss: 0.836186\n",
      "Train Epoch: 5 [9600/60000]\tLoss: 0.797412\n",
      "Train Epoch: 5 [10240/60000]\tLoss: 0.749050\n",
      "Train Epoch: 5 [10880/60000]\tLoss: 1.024368\n",
      "Train Epoch: 5 [11520/60000]\tLoss: 1.063980\n",
      "Train Epoch: 5 [12160/60000]\tLoss: 0.786344\n",
      "Train Epoch: 5 [12800/60000]\tLoss: 0.886396\n",
      "Train Epoch: 5 [13440/60000]\tLoss: 0.681012\n",
      "Train Epoch: 5 [14080/60000]\tLoss: 1.027156\n",
      "Train Epoch: 5 [14720/60000]\tLoss: 0.808475\n",
      "Train Epoch: 5 [15360/60000]\tLoss: 0.951569\n",
      "Train Epoch: 5 [16000/60000]\tLoss: 0.751500\n",
      "Train Epoch: 5 [16640/60000]\tLoss: 1.032029\n",
      "Train Epoch: 5 [17280/60000]\tLoss: 0.891499\n",
      "Train Epoch: 5 [17920/60000]\tLoss: 0.851333\n",
      "Train Epoch: 5 [18560/60000]\tLoss: 0.904526\n",
      "Train Epoch: 5 [19200/60000]\tLoss: 0.735435\n",
      "Train Epoch: 5 [19840/60000]\tLoss: 0.798517\n",
      "Train Epoch: 5 [20480/60000]\tLoss: 0.710305\n",
      "Train Epoch: 5 [21120/60000]\tLoss: 0.904129\n",
      "Train Epoch: 5 [21760/60000]\tLoss: 0.769306\n",
      "Train Epoch: 5 [22400/60000]\tLoss: 0.844006\n",
      "Train Epoch: 5 [23040/60000]\tLoss: 0.673153\n",
      "Train Epoch: 5 [23680/60000]\tLoss: 1.037433\n",
      "Train Epoch: 5 [24320/60000]\tLoss: 0.801384\n",
      "Train Epoch: 5 [24960/60000]\tLoss: 0.955634\n",
      "Train Epoch: 5 [25600/60000]\tLoss: 0.902585\n",
      "Train Epoch: 5 [26240/60000]\tLoss: 0.711310\n",
      "Train Epoch: 5 [26880/60000]\tLoss: 0.780073\n",
      "Train Epoch: 5 [27520/60000]\tLoss: 0.770615\n",
      "Train Epoch: 5 [28160/60000]\tLoss: 0.828431\n",
      "Train Epoch: 5 [28800/60000]\tLoss: 0.524334\n",
      "Train Epoch: 5 [29440/60000]\tLoss: 0.822100\n",
      "Train Epoch: 5 [30080/60000]\tLoss: 0.931909\n",
      "Train Epoch: 5 [30720/60000]\tLoss: 0.871437\n",
      "Train Epoch: 5 [31360/60000]\tLoss: 0.705365\n",
      "Train Epoch: 5 [32000/60000]\tLoss: 0.688162\n",
      "Train Epoch: 5 [32640/60000]\tLoss: 0.782768\n",
      "Train Epoch: 5 [33280/60000]\tLoss: 0.913666\n",
      "Train Epoch: 5 [33920/60000]\tLoss: 0.678510\n",
      "Train Epoch: 5 [34560/60000]\tLoss: 0.730117\n",
      "Train Epoch: 5 [35200/60000]\tLoss: 0.964864\n",
      "Train Epoch: 5 [35840/60000]\tLoss: 1.090885\n",
      "Train Epoch: 5 [36480/60000]\tLoss: 0.634918\n",
      "Train Epoch: 5 [37120/60000]\tLoss: 0.867068\n",
      "Train Epoch: 5 [37760/60000]\tLoss: 0.892212\n",
      "Train Epoch: 5 [38400/60000]\tLoss: 1.068973\n",
      "Train Epoch: 5 [39040/60000]\tLoss: 0.563073\n",
      "Train Epoch: 5 [39680/60000]\tLoss: 0.590905\n",
      "Train Epoch: 5 [40320/60000]\tLoss: 0.963462\n",
      "Train Epoch: 5 [40960/60000]\tLoss: 1.016564\n",
      "Train Epoch: 5 [41600/60000]\tLoss: 0.889450\n",
      "Train Epoch: 5 [42240/60000]\tLoss: 0.870237\n",
      "Train Epoch: 5 [42880/60000]\tLoss: 0.918928\n",
      "Train Epoch: 5 [43520/60000]\tLoss: 0.927712\n",
      "Train Epoch: 5 [44160/60000]\tLoss: 0.741013\n",
      "Train Epoch: 5 [44800/60000]\tLoss: 0.954536\n",
      "Train Epoch: 5 [45440/60000]\tLoss: 0.970662\n",
      "Train Epoch: 5 [46080/60000]\tLoss: 0.801151\n",
      "Train Epoch: 5 [46720/60000]\tLoss: 0.803617\n",
      "Train Epoch: 5 [47360/60000]\tLoss: 0.782940\n",
      "Train Epoch: 5 [48000/60000]\tLoss: 0.621093\n",
      "Train Epoch: 5 [48640/60000]\tLoss: 0.740794\n",
      "Train Epoch: 5 [49280/60000]\tLoss: 0.867802\n",
      "Train Epoch: 5 [49920/60000]\tLoss: 1.010083\n",
      "Train Epoch: 5 [50560/60000]\tLoss: 0.884109\n",
      "Train Epoch: 5 [51200/60000]\tLoss: 0.757289\n",
      "Train Epoch: 5 [51840/60000]\tLoss: 0.963311\n",
      "Train Epoch: 5 [52480/60000]\tLoss: 0.971569\n",
      "Train Epoch: 5 [53120/60000]\tLoss: 0.692012\n",
      "Train Epoch: 5 [53760/60000]\tLoss: 0.810724\n",
      "Train Epoch: 5 [54400/60000]\tLoss: 0.818384\n",
      "Train Epoch: 5 [55040/60000]\tLoss: 0.823634\n",
      "Train Epoch: 5 [55680/60000]\tLoss: 0.744450\n",
      "Train Epoch: 5 [56320/60000]\tLoss: 0.696072\n",
      "Train Epoch: 5 [56960/60000]\tLoss: 0.932442\n",
      "Train Epoch: 5 [57600/60000]\tLoss: 0.692069\n",
      "Train Epoch: 5 [58240/60000]\tLoss: 0.831342\n",
      "Train Epoch: 5 [58880/60000]\tLoss: 0.803711\n",
      "Train Epoch: 5 [59520/60000]\tLoss: 0.691294\n",
      "Train Epoch: 6 [0/60000]\tLoss: 0.848763\n",
      "Train Epoch: 6 [640/60000]\tLoss: 0.937553\n",
      "Train Epoch: 6 [1280/60000]\tLoss: 0.804707\n",
      "Train Epoch: 6 [1920/60000]\tLoss: 1.121958\n",
      "Train Epoch: 6 [2560/60000]\tLoss: 0.699681\n",
      "Train Epoch: 6 [3200/60000]\tLoss: 0.711405\n",
      "Train Epoch: 6 [3840/60000]\tLoss: 0.808402\n",
      "Train Epoch: 6 [4480/60000]\tLoss: 1.024982\n",
      "Train Epoch: 6 [5120/60000]\tLoss: 0.991549\n",
      "Train Epoch: 6 [5760/60000]\tLoss: 0.948145\n",
      "Train Epoch: 6 [6400/60000]\tLoss: 0.937153\n",
      "Train Epoch: 6 [7040/60000]\tLoss: 0.795077\n",
      "Train Epoch: 6 [7680/60000]\tLoss: 1.088343\n",
      "Train Epoch: 6 [8320/60000]\tLoss: 0.756386\n",
      "Train Epoch: 6 [8960/60000]\tLoss: 1.130711\n",
      "Train Epoch: 6 [9600/60000]\tLoss: 0.890415\n",
      "Train Epoch: 6 [10240/60000]\tLoss: 1.024355\n",
      "Train Epoch: 6 [10880/60000]\tLoss: 0.627264\n",
      "Train Epoch: 6 [11520/60000]\tLoss: 0.779269\n",
      "Train Epoch: 6 [12160/60000]\tLoss: 0.590138\n",
      "Train Epoch: 6 [12800/60000]\tLoss: 0.833037\n",
      "Train Epoch: 6 [13440/60000]\tLoss: 0.810376\n",
      "Train Epoch: 6 [14080/60000]\tLoss: 0.969881\n",
      "Train Epoch: 6 [14720/60000]\tLoss: 0.781859\n",
      "Train Epoch: 6 [15360/60000]\tLoss: 0.837016\n",
      "Train Epoch: 6 [16000/60000]\tLoss: 0.919096\n",
      "Train Epoch: 6 [16640/60000]\tLoss: 0.660329\n",
      "Train Epoch: 6 [17280/60000]\tLoss: 0.916044\n",
      "Train Epoch: 6 [17920/60000]\tLoss: 0.984747\n",
      "Train Epoch: 6 [18560/60000]\tLoss: 0.641058\n",
      "Train Epoch: 6 [19200/60000]\tLoss: 1.267420\n",
      "Train Epoch: 6 [19840/60000]\tLoss: 1.072814\n",
      "Train Epoch: 6 [20480/60000]\tLoss: 1.144364\n",
      "Train Epoch: 6 [21120/60000]\tLoss: 0.879385\n",
      "Train Epoch: 6 [21760/60000]\tLoss: 0.854154\n",
      "Train Epoch: 6 [22400/60000]\tLoss: 0.711545\n",
      "Train Epoch: 6 [23040/60000]\tLoss: 0.992420\n",
      "Train Epoch: 6 [23680/60000]\tLoss: 0.844980\n",
      "Train Epoch: 6 [24320/60000]\tLoss: 0.706439\n",
      "Train Epoch: 6 [24960/60000]\tLoss: 0.714907\n",
      "Train Epoch: 6 [25600/60000]\tLoss: 0.730589\n",
      "Train Epoch: 6 [26240/60000]\tLoss: 0.792423\n",
      "Train Epoch: 6 [26880/60000]\tLoss: 0.835222\n",
      "Train Epoch: 6 [27520/60000]\tLoss: 0.969484\n",
      "Train Epoch: 6 [28160/60000]\tLoss: 0.716934\n",
      "Train Epoch: 6 [28800/60000]\tLoss: 0.557537\n",
      "Train Epoch: 6 [29440/60000]\tLoss: 1.052052\n",
      "Train Epoch: 6 [30080/60000]\tLoss: 0.710404\n",
      "Train Epoch: 6 [30720/60000]\tLoss: 0.986864\n",
      "Train Epoch: 6 [31360/60000]\tLoss: 0.973711\n",
      "Train Epoch: 6 [32000/60000]\tLoss: 0.986390\n",
      "Train Epoch: 6 [32640/60000]\tLoss: 1.047609\n",
      "Train Epoch: 6 [33280/60000]\tLoss: 0.827716\n",
      "Train Epoch: 6 [33920/60000]\tLoss: 0.902340\n",
      "Train Epoch: 6 [34560/60000]\tLoss: 0.680784\n",
      "Train Epoch: 6 [35200/60000]\tLoss: 0.859116\n",
      "Train Epoch: 6 [35840/60000]\tLoss: 0.741840\n",
      "Train Epoch: 6 [36480/60000]\tLoss: 1.101748\n",
      "Train Epoch: 6 [37120/60000]\tLoss: 0.793097\n",
      "Train Epoch: 6 [37760/60000]\tLoss: 1.107497\n",
      "Train Epoch: 6 [38400/60000]\tLoss: 0.920606\n",
      "Train Epoch: 6 [39040/60000]\tLoss: 1.103767\n",
      "Train Epoch: 6 [39680/60000]\tLoss: 0.706662\n",
      "Train Epoch: 6 [40320/60000]\tLoss: 0.860439\n",
      "Train Epoch: 6 [40960/60000]\tLoss: 0.691829\n",
      "Train Epoch: 6 [41600/60000]\tLoss: 0.675357\n",
      "Train Epoch: 6 [42240/60000]\tLoss: 0.832677\n",
      "Train Epoch: 6 [42880/60000]\tLoss: 0.672411\n",
      "Train Epoch: 6 [43520/60000]\tLoss: 0.751636\n",
      "Train Epoch: 6 [44160/60000]\tLoss: 0.450115\n",
      "Train Epoch: 6 [44800/60000]\tLoss: 0.983141\n",
      "Train Epoch: 6 [45440/60000]\tLoss: 0.825643\n",
      "Train Epoch: 6 [46080/60000]\tLoss: 0.854157\n",
      "Train Epoch: 6 [46720/60000]\tLoss: 0.937093\n",
      "Train Epoch: 6 [47360/60000]\tLoss: 0.749363\n",
      "Train Epoch: 6 [48000/60000]\tLoss: 0.890339\n",
      "Train Epoch: 6 [48640/60000]\tLoss: 0.781735\n",
      "Train Epoch: 6 [49280/60000]\tLoss: 0.905163\n",
      "Train Epoch: 6 [49920/60000]\tLoss: 0.709649\n",
      "Train Epoch: 6 [50560/60000]\tLoss: 0.952125\n",
      "Train Epoch: 6 [51200/60000]\tLoss: 0.720277\n",
      "Train Epoch: 6 [51840/60000]\tLoss: 0.707056\n",
      "Train Epoch: 6 [52480/60000]\tLoss: 0.652483\n",
      "Train Epoch: 6 [53120/60000]\tLoss: 1.024922\n",
      "Train Epoch: 6 [53760/60000]\tLoss: 1.101290\n",
      "Train Epoch: 6 [54400/60000]\tLoss: 0.688509\n",
      "Train Epoch: 6 [55040/60000]\tLoss: 0.602205\n",
      "Train Epoch: 6 [55680/60000]\tLoss: 0.787783\n",
      "Train Epoch: 6 [56320/60000]\tLoss: 0.697032\n",
      "Train Epoch: 6 [56960/60000]\tLoss: 0.856571\n",
      "Train Epoch: 6 [57600/60000]\tLoss: 0.858584\n",
      "Train Epoch: 6 [58240/60000]\tLoss: 0.631661\n",
      "Train Epoch: 6 [58880/60000]\tLoss: 1.240847\n",
      "Train Epoch: 6 [59520/60000]\tLoss: 0.985113\n",
      "Train Epoch: 7 [0/60000]\tLoss: 0.757638\n",
      "Train Epoch: 7 [640/60000]\tLoss: 0.805764\n",
      "Train Epoch: 7 [1280/60000]\tLoss: 0.908594\n",
      "Train Epoch: 7 [1920/60000]\tLoss: 0.857858\n",
      "Train Epoch: 7 [2560/60000]\tLoss: 0.731185\n",
      "Train Epoch: 7 [3200/60000]\tLoss: 0.905083\n",
      "Train Epoch: 7 [3840/60000]\tLoss: 0.617920\n",
      "Train Epoch: 7 [4480/60000]\tLoss: 0.869347\n",
      "Train Epoch: 7 [5120/60000]\tLoss: 0.728328\n",
      "Train Epoch: 7 [5760/60000]\tLoss: 0.597978\n",
      "Train Epoch: 7 [6400/60000]\tLoss: 1.030114\n",
      "Train Epoch: 7 [7040/60000]\tLoss: 0.933625\n",
      "Train Epoch: 7 [7680/60000]\tLoss: 0.895215\n",
      "Train Epoch: 7 [8320/60000]\tLoss: 0.843948\n",
      "Train Epoch: 7 [8960/60000]\tLoss: 1.081296\n",
      "Train Epoch: 7 [9600/60000]\tLoss: 0.861612\n",
      "Train Epoch: 7 [10240/60000]\tLoss: 0.909635\n",
      "Train Epoch: 7 [10880/60000]\tLoss: 0.726916\n",
      "Train Epoch: 7 [11520/60000]\tLoss: 0.965567\n",
      "Train Epoch: 7 [12160/60000]\tLoss: 0.658031\n",
      "Train Epoch: 7 [12800/60000]\tLoss: 1.015571\n",
      "Train Epoch: 7 [13440/60000]\tLoss: 0.777181\n",
      "Train Epoch: 7 [14080/60000]\tLoss: 0.716192\n",
      "Train Epoch: 7 [14720/60000]\tLoss: 0.852597\n",
      "Train Epoch: 7 [15360/60000]\tLoss: 1.172857\n",
      "Train Epoch: 7 [16000/60000]\tLoss: 0.950764\n",
      "Train Epoch: 7 [16640/60000]\tLoss: 0.746171\n",
      "Train Epoch: 7 [17280/60000]\tLoss: 0.714752\n",
      "Train Epoch: 7 [17920/60000]\tLoss: 0.990033\n",
      "Train Epoch: 7 [18560/60000]\tLoss: 0.903414\n",
      "Train Epoch: 7 [19200/60000]\tLoss: 0.641129\n",
      "Train Epoch: 7 [19840/60000]\tLoss: 0.967784\n",
      "Train Epoch: 7 [20480/60000]\tLoss: 1.044297\n",
      "Train Epoch: 7 [21120/60000]\tLoss: 0.709954\n",
      "Train Epoch: 7 [21760/60000]\tLoss: 0.928814\n",
      "Train Epoch: 7 [22400/60000]\tLoss: 0.986546\n",
      "Train Epoch: 7 [23040/60000]\tLoss: 0.778343\n",
      "Train Epoch: 7 [23680/60000]\tLoss: 0.847250\n",
      "Train Epoch: 7 [24320/60000]\tLoss: 1.000764\n",
      "Train Epoch: 7 [24960/60000]\tLoss: 0.934292\n",
      "Train Epoch: 7 [25600/60000]\tLoss: 0.769471\n",
      "Train Epoch: 7 [26240/60000]\tLoss: 0.992370\n",
      "Train Epoch: 7 [26880/60000]\tLoss: 1.186845\n",
      "Train Epoch: 7 [27520/60000]\tLoss: 0.987555\n",
      "Train Epoch: 7 [28160/60000]\tLoss: 1.155034\n",
      "Train Epoch: 7 [28800/60000]\tLoss: 0.898046\n",
      "Train Epoch: 7 [29440/60000]\tLoss: 1.041178\n",
      "Train Epoch: 7 [30080/60000]\tLoss: 0.869350\n",
      "Train Epoch: 7 [30720/60000]\tLoss: 0.914560\n",
      "Train Epoch: 7 [31360/60000]\tLoss: 0.844400\n",
      "Train Epoch: 7 [32000/60000]\tLoss: 0.876345\n",
      "Train Epoch: 7 [32640/60000]\tLoss: 0.580330\n",
      "Train Epoch: 7 [33280/60000]\tLoss: 1.068734\n",
      "Train Epoch: 7 [33920/60000]\tLoss: 0.886802\n",
      "Train Epoch: 7 [34560/60000]\tLoss: 0.682940\n",
      "Train Epoch: 7 [35200/60000]\tLoss: 0.837975\n",
      "Train Epoch: 7 [35840/60000]\tLoss: 0.843940\n",
      "Train Epoch: 7 [36480/60000]\tLoss: 0.880740\n",
      "Train Epoch: 7 [37120/60000]\tLoss: 0.783653\n",
      "Train Epoch: 7 [37760/60000]\tLoss: 1.007910\n",
      "Train Epoch: 7 [38400/60000]\tLoss: 0.567148\n",
      "Train Epoch: 7 [39040/60000]\tLoss: 0.630868\n",
      "Train Epoch: 7 [39680/60000]\tLoss: 1.208819\n",
      "Train Epoch: 7 [40320/60000]\tLoss: 0.982285\n",
      "Train Epoch: 7 [40960/60000]\tLoss: 0.865612\n",
      "Train Epoch: 7 [41600/60000]\tLoss: 0.736755\n",
      "Train Epoch: 7 [42240/60000]\tLoss: 0.839524\n",
      "Train Epoch: 7 [42880/60000]\tLoss: 0.717096\n",
      "Train Epoch: 7 [43520/60000]\tLoss: 1.180050\n",
      "Train Epoch: 7 [44160/60000]\tLoss: 0.648124\n",
      "Train Epoch: 7 [44800/60000]\tLoss: 0.901706\n",
      "Train Epoch: 7 [45440/60000]\tLoss: 1.013667\n",
      "Train Epoch: 7 [46080/60000]\tLoss: 0.825304\n",
      "Train Epoch: 7 [46720/60000]\tLoss: 0.853474\n",
      "Train Epoch: 7 [47360/60000]\tLoss: 0.826667\n",
      "Train Epoch: 7 [48000/60000]\tLoss: 0.710439\n",
      "Train Epoch: 7 [48640/60000]\tLoss: 1.059222\n",
      "Train Epoch: 7 [49280/60000]\tLoss: 0.892207\n",
      "Train Epoch: 7 [49920/60000]\tLoss: 1.008785\n",
      "Train Epoch: 7 [50560/60000]\tLoss: 0.630882\n",
      "Train Epoch: 7 [51200/60000]\tLoss: 1.016736\n",
      "Train Epoch: 7 [51840/60000]\tLoss: 0.568743\n",
      "Train Epoch: 7 [52480/60000]\tLoss: 0.831999\n",
      "Train Epoch: 7 [53120/60000]\tLoss: 0.825505\n",
      "Train Epoch: 7 [53760/60000]\tLoss: 0.928227\n",
      "Train Epoch: 7 [54400/60000]\tLoss: 0.976913\n",
      "Train Epoch: 7 [55040/60000]\tLoss: 0.760464\n",
      "Train Epoch: 7 [55680/60000]\tLoss: 0.688904\n",
      "Train Epoch: 7 [56320/60000]\tLoss: 0.661330\n",
      "Train Epoch: 7 [56960/60000]\tLoss: 0.676573\n",
      "Train Epoch: 7 [57600/60000]\tLoss: 0.891362\n",
      "Train Epoch: 7 [58240/60000]\tLoss: 0.762933\n",
      "Train Epoch: 7 [58880/60000]\tLoss: 0.730845\n",
      "Train Epoch: 7 [59520/60000]\tLoss: 0.965027\n",
      "Train Epoch: 8 [0/60000]\tLoss: 0.883075\n",
      "Train Epoch: 8 [640/60000]\tLoss: 0.693411\n",
      "Train Epoch: 8 [1280/60000]\tLoss: 0.546753\n",
      "Train Epoch: 8 [1920/60000]\tLoss: 0.673611\n",
      "Train Epoch: 8 [2560/60000]\tLoss: 0.623719\n",
      "Train Epoch: 8 [3200/60000]\tLoss: 1.090567\n",
      "Train Epoch: 8 [3840/60000]\tLoss: 0.754230\n",
      "Train Epoch: 8 [4480/60000]\tLoss: 0.874274\n",
      "Train Epoch: 8 [5120/60000]\tLoss: 0.963938\n",
      "Train Epoch: 8 [5760/60000]\tLoss: 0.645982\n",
      "Train Epoch: 8 [6400/60000]\tLoss: 0.806382\n",
      "Train Epoch: 8 [7040/60000]\tLoss: 0.755639\n",
      "Train Epoch: 8 [7680/60000]\tLoss: 1.026223\n",
      "Train Epoch: 8 [8320/60000]\tLoss: 0.841055\n",
      "Train Epoch: 8 [8960/60000]\tLoss: 0.806140\n",
      "Train Epoch: 8 [9600/60000]\tLoss: 0.640792\n",
      "Train Epoch: 8 [10240/60000]\tLoss: 0.868490\n",
      "Train Epoch: 8 [10880/60000]\tLoss: 0.621987\n",
      "Train Epoch: 8 [11520/60000]\tLoss: 0.816691\n",
      "Train Epoch: 8 [12160/60000]\tLoss: 0.701861\n",
      "Train Epoch: 8 [12800/60000]\tLoss: 0.923458\n",
      "Train Epoch: 8 [13440/60000]\tLoss: 0.976943\n",
      "Train Epoch: 8 [14080/60000]\tLoss: 0.771531\n",
      "Train Epoch: 8 [14720/60000]\tLoss: 0.731965\n",
      "Train Epoch: 8 [15360/60000]\tLoss: 0.882066\n",
      "Train Epoch: 8 [16000/60000]\tLoss: 1.015697\n",
      "Train Epoch: 8 [16640/60000]\tLoss: 0.958650\n",
      "Train Epoch: 8 [17280/60000]\tLoss: 0.851350\n",
      "Train Epoch: 8 [17920/60000]\tLoss: 0.799338\n",
      "Train Epoch: 8 [18560/60000]\tLoss: 1.088655\n",
      "Train Epoch: 8 [19200/60000]\tLoss: 0.724270\n",
      "Train Epoch: 8 [19840/60000]\tLoss: 0.943307\n",
      "Train Epoch: 8 [20480/60000]\tLoss: 0.684289\n",
      "Train Epoch: 8 [21120/60000]\tLoss: 0.747013\n",
      "Train Epoch: 8 [21760/60000]\tLoss: 0.822456\n",
      "Train Epoch: 8 [22400/60000]\tLoss: 0.814854\n",
      "Train Epoch: 8 [23040/60000]\tLoss: 0.969019\n",
      "Train Epoch: 8 [23680/60000]\tLoss: 0.582112\n",
      "Train Epoch: 8 [24320/60000]\tLoss: 0.695162\n",
      "Train Epoch: 8 [24960/60000]\tLoss: 0.690231\n",
      "Train Epoch: 8 [25600/60000]\tLoss: 0.507715\n",
      "Train Epoch: 8 [26240/60000]\tLoss: 0.721974\n",
      "Train Epoch: 8 [26880/60000]\tLoss: 0.812366\n",
      "Train Epoch: 8 [27520/60000]\tLoss: 0.751995\n",
      "Train Epoch: 8 [28160/60000]\tLoss: 0.885705\n",
      "Train Epoch: 8 [28800/60000]\tLoss: 0.801417\n",
      "Train Epoch: 8 [29440/60000]\tLoss: 0.949745\n",
      "Train Epoch: 8 [30080/60000]\tLoss: 0.867567\n",
      "Train Epoch: 8 [30720/60000]\tLoss: 0.867474\n",
      "Train Epoch: 8 [31360/60000]\tLoss: 0.781759\n",
      "Train Epoch: 8 [32000/60000]\tLoss: 0.468488\n",
      "Train Epoch: 8 [32640/60000]\tLoss: 0.685072\n",
      "Train Epoch: 8 [33280/60000]\tLoss: 0.786252\n",
      "Train Epoch: 8 [33920/60000]\tLoss: 0.755173\n",
      "Train Epoch: 8 [34560/60000]\tLoss: 0.499258\n",
      "Train Epoch: 8 [35200/60000]\tLoss: 0.715140\n",
      "Train Epoch: 8 [35840/60000]\tLoss: 0.706388\n",
      "Train Epoch: 8 [36480/60000]\tLoss: 0.872665\n",
      "Train Epoch: 8 [37120/60000]\tLoss: 0.660088\n",
      "Train Epoch: 8 [37760/60000]\tLoss: 0.862087\n",
      "Train Epoch: 8 [38400/60000]\tLoss: 0.959185\n",
      "Train Epoch: 8 [39040/60000]\tLoss: 0.802421\n",
      "Train Epoch: 8 [39680/60000]\tLoss: 0.954168\n",
      "Train Epoch: 8 [40320/60000]\tLoss: 1.024094\n",
      "Train Epoch: 8 [40960/60000]\tLoss: 0.589074\n",
      "Train Epoch: 8 [41600/60000]\tLoss: 0.620036\n",
      "Train Epoch: 8 [42240/60000]\tLoss: 0.753975\n",
      "Train Epoch: 8 [42880/60000]\tLoss: 0.757952\n",
      "Train Epoch: 8 [43520/60000]\tLoss: 0.861800\n",
      "Train Epoch: 8 [44160/60000]\tLoss: 0.993541\n",
      "Train Epoch: 8 [44800/60000]\tLoss: 1.107013\n",
      "Train Epoch: 8 [45440/60000]\tLoss: 0.658515\n",
      "Train Epoch: 8 [46080/60000]\tLoss: 1.048256\n",
      "Train Epoch: 8 [46720/60000]\tLoss: 0.777393\n",
      "Train Epoch: 8 [47360/60000]\tLoss: 0.836210\n",
      "Train Epoch: 8 [48000/60000]\tLoss: 0.855231\n",
      "Train Epoch: 8 [48640/60000]\tLoss: 0.855146\n",
      "Train Epoch: 8 [49280/60000]\tLoss: 1.152131\n",
      "Train Epoch: 8 [49920/60000]\tLoss: 0.904058\n",
      "Train Epoch: 8 [50560/60000]\tLoss: 0.710144\n",
      "Train Epoch: 8 [51200/60000]\tLoss: 0.734200\n",
      "Train Epoch: 8 [51840/60000]\tLoss: 0.817501\n",
      "Train Epoch: 8 [52480/60000]\tLoss: 0.925886\n",
      "Train Epoch: 8 [53120/60000]\tLoss: 0.839844\n",
      "Train Epoch: 8 [53760/60000]\tLoss: 0.517856\n",
      "Train Epoch: 8 [54400/60000]\tLoss: 0.751773\n",
      "Train Epoch: 8 [55040/60000]\tLoss: 0.642855\n",
      "Train Epoch: 8 [55680/60000]\tLoss: 0.728290\n",
      "Train Epoch: 8 [56320/60000]\tLoss: 0.837000\n",
      "Train Epoch: 8 [56960/60000]\tLoss: 0.806396\n",
      "Train Epoch: 8 [57600/60000]\tLoss: 0.949305\n",
      "Train Epoch: 8 [58240/60000]\tLoss: 0.774841\n",
      "Train Epoch: 8 [58880/60000]\tLoss: 0.899574\n",
      "Train Epoch: 8 [59520/60000]\tLoss: 1.137352\n",
      "Train Epoch: 9 [0/60000]\tLoss: 0.746999\n",
      "Train Epoch: 9 [640/60000]\tLoss: 0.978258\n",
      "Train Epoch: 9 [1280/60000]\tLoss: 0.849874\n",
      "Train Epoch: 9 [1920/60000]\tLoss: 0.676036\n",
      "Train Epoch: 9 [2560/60000]\tLoss: 0.895752\n",
      "Train Epoch: 9 [3200/60000]\tLoss: 0.710539\n",
      "Train Epoch: 9 [3840/60000]\tLoss: 0.707880\n",
      "Train Epoch: 9 [4480/60000]\tLoss: 0.637924\n",
      "Train Epoch: 9 [5120/60000]\tLoss: 0.848929\n",
      "Train Epoch: 9 [5760/60000]\tLoss: 0.988553\n",
      "Train Epoch: 9 [6400/60000]\tLoss: 0.954338\n",
      "Train Epoch: 9 [7040/60000]\tLoss: 0.561625\n",
      "Train Epoch: 9 [7680/60000]\tLoss: 0.641935\n",
      "Train Epoch: 9 [8320/60000]\tLoss: 0.912856\n",
      "Train Epoch: 9 [8960/60000]\tLoss: 0.899432\n",
      "Train Epoch: 9 [9600/60000]\tLoss: 0.788999\n",
      "Train Epoch: 9 [10240/60000]\tLoss: 1.020055\n",
      "Train Epoch: 9 [10880/60000]\tLoss: 0.820787\n",
      "Train Epoch: 9 [11520/60000]\tLoss: 0.926644\n",
      "Train Epoch: 9 [12160/60000]\tLoss: 0.847458\n",
      "Train Epoch: 9 [12800/60000]\tLoss: 0.656439\n",
      "Train Epoch: 9 [13440/60000]\tLoss: 0.667090\n",
      "Train Epoch: 9 [14080/60000]\tLoss: 1.009837\n",
      "Train Epoch: 9 [14720/60000]\tLoss: 0.938709\n",
      "Train Epoch: 9 [15360/60000]\tLoss: 0.765425\n",
      "Train Epoch: 9 [16000/60000]\tLoss: 0.852336\n",
      "Train Epoch: 9 [16640/60000]\tLoss: 0.655516\n",
      "Train Epoch: 9 [17280/60000]\tLoss: 0.761096\n",
      "Train Epoch: 9 [17920/60000]\tLoss: 0.892085\n",
      "Train Epoch: 9 [18560/60000]\tLoss: 0.718816\n",
      "Train Epoch: 9 [19200/60000]\tLoss: 0.893489\n",
      "Train Epoch: 9 [19840/60000]\tLoss: 0.794337\n",
      "Train Epoch: 9 [20480/60000]\tLoss: 0.654200\n",
      "Train Epoch: 9 [21120/60000]\tLoss: 0.714429\n",
      "Train Epoch: 9 [21760/60000]\tLoss: 0.838319\n",
      "Train Epoch: 9 [22400/60000]\tLoss: 1.122888\n",
      "Train Epoch: 9 [23040/60000]\tLoss: 1.279840\n",
      "Train Epoch: 9 [23680/60000]\tLoss: 0.905921\n",
      "Train Epoch: 9 [24320/60000]\tLoss: 0.827718\n",
      "Train Epoch: 9 [24960/60000]\tLoss: 0.664091\n",
      "Train Epoch: 9 [25600/60000]\tLoss: 0.630597\n",
      "Train Epoch: 9 [26240/60000]\tLoss: 0.890488\n",
      "Train Epoch: 9 [26880/60000]\tLoss: 0.674168\n",
      "Train Epoch: 9 [27520/60000]\tLoss: 0.928564\n",
      "Train Epoch: 9 [28160/60000]\tLoss: 1.084811\n",
      "Train Epoch: 9 [28800/60000]\tLoss: 0.846365\n",
      "Train Epoch: 9 [29440/60000]\tLoss: 0.901134\n",
      "Train Epoch: 9 [30080/60000]\tLoss: 0.861840\n",
      "Train Epoch: 9 [30720/60000]\tLoss: 0.728438\n",
      "Train Epoch: 9 [31360/60000]\tLoss: 0.859613\n",
      "Train Epoch: 9 [32000/60000]\tLoss: 0.665764\n",
      "Train Epoch: 9 [32640/60000]\tLoss: 0.801816\n",
      "Train Epoch: 9 [33280/60000]\tLoss: 0.754228\n",
      "Train Epoch: 9 [33920/60000]\tLoss: 0.693014\n",
      "Train Epoch: 9 [34560/60000]\tLoss: 0.790996\n",
      "Train Epoch: 9 [35200/60000]\tLoss: 0.964037\n",
      "Train Epoch: 9 [35840/60000]\tLoss: 0.846204\n",
      "Train Epoch: 9 [36480/60000]\tLoss: 0.730984\n",
      "Train Epoch: 9 [37120/60000]\tLoss: 0.802193\n",
      "Train Epoch: 9 [37760/60000]\tLoss: 0.710142\n",
      "Train Epoch: 9 [38400/60000]\tLoss: 0.905341\n",
      "Train Epoch: 9 [39040/60000]\tLoss: 0.867115\n",
      "Train Epoch: 9 [39680/60000]\tLoss: 0.817706\n",
      "Train Epoch: 9 [40320/60000]\tLoss: 0.818653\n",
      "Train Epoch: 9 [40960/60000]\tLoss: 1.087977\n",
      "Train Epoch: 9 [41600/60000]\tLoss: 0.798506\n",
      "Train Epoch: 9 [42240/60000]\tLoss: 0.712766\n",
      "Train Epoch: 9 [42880/60000]\tLoss: 0.739321\n",
      "Train Epoch: 9 [43520/60000]\tLoss: 0.688303\n",
      "Train Epoch: 9 [44160/60000]\tLoss: 0.782141\n",
      "Train Epoch: 9 [44800/60000]\tLoss: 0.806755\n",
      "Train Epoch: 9 [45440/60000]\tLoss: 0.826205\n",
      "Train Epoch: 9 [46080/60000]\tLoss: 0.718989\n",
      "Train Epoch: 9 [46720/60000]\tLoss: 0.787807\n",
      "Train Epoch: 9 [47360/60000]\tLoss: 0.640716\n",
      "Train Epoch: 9 [48000/60000]\tLoss: 0.820776\n",
      "Train Epoch: 9 [48640/60000]\tLoss: 0.754568\n",
      "Train Epoch: 9 [49280/60000]\tLoss: 0.747160\n",
      "Train Epoch: 9 [49920/60000]\tLoss: 0.751369\n",
      "Train Epoch: 9 [50560/60000]\tLoss: 0.665625\n",
      "Train Epoch: 9 [51200/60000]\tLoss: 0.803621\n",
      "Train Epoch: 9 [51840/60000]\tLoss: 0.561590\n",
      "Train Epoch: 9 [52480/60000]\tLoss: 0.972001\n",
      "Train Epoch: 9 [53120/60000]\tLoss: 0.867324\n",
      "Train Epoch: 9 [53760/60000]\tLoss: 0.661491\n",
      "Train Epoch: 9 [54400/60000]\tLoss: 0.672138\n",
      "Train Epoch: 9 [55040/60000]\tLoss: 0.872411\n",
      "Train Epoch: 9 [55680/60000]\tLoss: 0.841755\n",
      "Train Epoch: 9 [56320/60000]\tLoss: 0.929769\n",
      "Train Epoch: 9 [56960/60000]\tLoss: 0.895114\n",
      "Train Epoch: 9 [57600/60000]\tLoss: 0.914829\n",
      "Train Epoch: 9 [58240/60000]\tLoss: 0.996869\n",
      "Train Epoch: 9 [58880/60000]\tLoss: 0.857464\n",
      "Train Epoch: 9 [59520/60000]\tLoss: 0.950349\n",
      "Train Epoch: 10 [0/60000]\tLoss: 1.115066\n",
      "Train Epoch: 10 [640/60000]\tLoss: 0.939051\n",
      "Train Epoch: 10 [1280/60000]\tLoss: 0.800269\n",
      "Train Epoch: 10 [1920/60000]\tLoss: 1.257226\n",
      "Train Epoch: 10 [2560/60000]\tLoss: 0.573305\n",
      "Train Epoch: 10 [3200/60000]\tLoss: 0.864406\n",
      "Train Epoch: 10 [3840/60000]\tLoss: 1.029792\n",
      "Train Epoch: 10 [4480/60000]\tLoss: 0.632116\n",
      "Train Epoch: 10 [5120/60000]\tLoss: 0.691363\n",
      "Train Epoch: 10 [5760/60000]\tLoss: 0.789995\n",
      "Train Epoch: 10 [6400/60000]\tLoss: 0.634572\n",
      "Train Epoch: 10 [7040/60000]\tLoss: 0.768369\n",
      "Train Epoch: 10 [7680/60000]\tLoss: 0.696186\n",
      "Train Epoch: 10 [8320/60000]\tLoss: 0.787799\n",
      "Train Epoch: 10 [8960/60000]\tLoss: 0.765213\n",
      "Train Epoch: 10 [9600/60000]\tLoss: 0.646036\n",
      "Train Epoch: 10 [10240/60000]\tLoss: 1.095176\n",
      "Train Epoch: 10 [10880/60000]\tLoss: 0.956058\n",
      "Train Epoch: 10 [11520/60000]\tLoss: 0.704407\n",
      "Train Epoch: 10 [12160/60000]\tLoss: 0.958997\n",
      "Train Epoch: 10 [12800/60000]\tLoss: 0.865643\n",
      "Train Epoch: 10 [13440/60000]\tLoss: 0.821603\n",
      "Train Epoch: 10 [14080/60000]\tLoss: 0.770858\n",
      "Train Epoch: 10 [14720/60000]\tLoss: 0.642217\n",
      "Train Epoch: 10 [15360/60000]\tLoss: 0.917567\n",
      "Train Epoch: 10 [16000/60000]\tLoss: 0.813653\n",
      "Train Epoch: 10 [16640/60000]\tLoss: 0.707669\n",
      "Train Epoch: 10 [17280/60000]\tLoss: 0.638452\n",
      "Train Epoch: 10 [17920/60000]\tLoss: 0.788435\n",
      "Train Epoch: 10 [18560/60000]\tLoss: 0.877455\n",
      "Train Epoch: 10 [19200/60000]\tLoss: 0.776637\n",
      "Train Epoch: 10 [19840/60000]\tLoss: 1.029316\n",
      "Train Epoch: 10 [20480/60000]\tLoss: 0.704995\n",
      "Train Epoch: 10 [21120/60000]\tLoss: 0.974970\n",
      "Train Epoch: 10 [21760/60000]\tLoss: 1.051964\n",
      "Train Epoch: 10 [22400/60000]\tLoss: 1.093006\n",
      "Train Epoch: 10 [23040/60000]\tLoss: 0.901054\n",
      "Train Epoch: 10 [23680/60000]\tLoss: 0.978356\n",
      "Train Epoch: 10 [24320/60000]\tLoss: 0.931505\n",
      "Train Epoch: 10 [24960/60000]\tLoss: 0.666532\n",
      "Train Epoch: 10 [25600/60000]\tLoss: 0.668044\n",
      "Train Epoch: 10 [26240/60000]\tLoss: 0.879383\n",
      "Train Epoch: 10 [26880/60000]\tLoss: 0.474505\n",
      "Train Epoch: 10 [27520/60000]\tLoss: 0.768976\n",
      "Train Epoch: 10 [28160/60000]\tLoss: 0.986058\n",
      "Train Epoch: 10 [28800/60000]\tLoss: 0.721505\n",
      "Train Epoch: 10 [29440/60000]\tLoss: 0.650873\n",
      "Train Epoch: 10 [30080/60000]\tLoss: 0.745376\n",
      "Train Epoch: 10 [30720/60000]\tLoss: 0.780211\n",
      "Train Epoch: 10 [31360/60000]\tLoss: 0.666766\n",
      "Train Epoch: 10 [32000/60000]\tLoss: 1.079643\n",
      "Train Epoch: 10 [32640/60000]\tLoss: 0.631554\n",
      "Train Epoch: 10 [33280/60000]\tLoss: 0.661360\n",
      "Train Epoch: 10 [33920/60000]\tLoss: 0.821955\n",
      "Train Epoch: 10 [34560/60000]\tLoss: 0.794248\n",
      "Train Epoch: 10 [35200/60000]\tLoss: 0.668163\n",
      "Train Epoch: 10 [35840/60000]\tLoss: 0.786725\n",
      "Train Epoch: 10 [36480/60000]\tLoss: 0.942479\n",
      "Train Epoch: 10 [37120/60000]\tLoss: 0.711108\n",
      "Train Epoch: 10 [37760/60000]\tLoss: 0.847872\n",
      "Train Epoch: 10 [38400/60000]\tLoss: 0.869754\n",
      "Train Epoch: 10 [39040/60000]\tLoss: 0.793476\n",
      "Train Epoch: 10 [39680/60000]\tLoss: 0.864284\n",
      "Train Epoch: 10 [40320/60000]\tLoss: 0.794044\n",
      "Train Epoch: 10 [40960/60000]\tLoss: 1.091834\n",
      "Train Epoch: 10 [41600/60000]\tLoss: 0.873738\n",
      "Train Epoch: 10 [42240/60000]\tLoss: 0.776130\n",
      "Train Epoch: 10 [42880/60000]\tLoss: 0.795638\n",
      "Train Epoch: 10 [43520/60000]\tLoss: 0.705218\n",
      "Train Epoch: 10 [44160/60000]\tLoss: 1.082894\n",
      "Train Epoch: 10 [44800/60000]\tLoss: 0.797097\n",
      "Train Epoch: 10 [45440/60000]\tLoss: 0.711695\n",
      "Train Epoch: 10 [46080/60000]\tLoss: 0.950105\n",
      "Train Epoch: 10 [46720/60000]\tLoss: 1.026615\n",
      "Train Epoch: 10 [47360/60000]\tLoss: 0.844231\n",
      "Train Epoch: 10 [48000/60000]\tLoss: 1.061814\n",
      "Train Epoch: 10 [48640/60000]\tLoss: 0.872100\n",
      "Train Epoch: 10 [49280/60000]\tLoss: 0.947970\n",
      "Train Epoch: 10 [49920/60000]\tLoss: 0.823509\n",
      "Train Epoch: 10 [50560/60000]\tLoss: 0.704535\n",
      "Train Epoch: 10 [51200/60000]\tLoss: 0.899150\n",
      "Train Epoch: 10 [51840/60000]\tLoss: 1.042972\n",
      "Train Epoch: 10 [52480/60000]\tLoss: 0.887684\n",
      "Train Epoch: 10 [53120/60000]\tLoss: 0.913483\n",
      "Train Epoch: 10 [53760/60000]\tLoss: 0.601435\n",
      "Train Epoch: 10 [54400/60000]\tLoss: 0.678980\n",
      "Train Epoch: 10 [55040/60000]\tLoss: 0.555703\n",
      "Train Epoch: 10 [55680/60000]\tLoss: 0.580671\n",
      "Train Epoch: 10 [56320/60000]\tLoss: 0.798196\n",
      "Train Epoch: 10 [56960/60000]\tLoss: 0.644084\n",
      "Train Epoch: 10 [57600/60000]\tLoss: 0.922427\n",
      "Train Epoch: 10 [58240/60000]\tLoss: 0.784611\n",
      "Train Epoch: 10 [58880/60000]\tLoss: 0.849505\n",
      "Train Epoch: 10 [59520/60000]\tLoss: 0.977110\n",
      "\n",
      "Test set: Avg. loss: 0.2689, Accuracy: 9254/10000 (93%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and test Model 1\n",
    "\n",
    "# Create network\n",
    "model = Net()\n",
    "# Initialize model weights\n",
    "model.apply(weights_init)\n",
    "# Define optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "# Get initial performance\n",
    "test(model)\n",
    "# Train for ten epochs\n",
    "n_epochs = 10\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  train(epoch, model, optimizer)\n",
    "accuracy1 = test(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "r2PVnghrmr0F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.4005, Accuracy: 1326/10000 (13%)\n",
      "\n",
      "Train Epoch: 1 [0/60000]\tLoss: 2.507610\n",
      "Train Epoch: 1 [640/60000]\tLoss: 2.102225\n",
      "Train Epoch: 1 [1280/60000]\tLoss: 1.794129\n",
      "Train Epoch: 1 [1920/60000]\tLoss: 1.852212\n",
      "Train Epoch: 1 [2560/60000]\tLoss: 1.475319\n",
      "Train Epoch: 1 [3200/60000]\tLoss: 1.494001\n",
      "Train Epoch: 1 [3840/60000]\tLoss: 1.450663\n",
      "Train Epoch: 1 [4480/60000]\tLoss: 0.828227\n",
      "Train Epoch: 1 [5120/60000]\tLoss: 1.322323\n",
      "Train Epoch: 1 [5760/60000]\tLoss: 0.979544\n",
      "Train Epoch: 1 [6400/60000]\tLoss: 1.067897\n",
      "Train Epoch: 1 [7040/60000]\tLoss: 0.952976\n",
      "Train Epoch: 1 [7680/60000]\tLoss: 0.789140\n",
      "Train Epoch: 1 [8320/60000]\tLoss: 0.684815\n",
      "Train Epoch: 1 [8960/60000]\tLoss: 1.076699\n",
      "Train Epoch: 1 [9600/60000]\tLoss: 0.745523\n",
      "Train Epoch: 1 [10240/60000]\tLoss: 0.714706\n",
      "Train Epoch: 1 [10880/60000]\tLoss: 0.986711\n",
      "Train Epoch: 1 [11520/60000]\tLoss: 0.793512\n",
      "Train Epoch: 1 [12160/60000]\tLoss: 0.684430\n",
      "Train Epoch: 1 [12800/60000]\tLoss: 0.699340\n",
      "Train Epoch: 1 [13440/60000]\tLoss: 0.607492\n",
      "Train Epoch: 1 [14080/60000]\tLoss: 0.644437\n",
      "Train Epoch: 1 [14720/60000]\tLoss: 0.482263\n",
      "Train Epoch: 1 [15360/60000]\tLoss: 0.737349\n",
      "Train Epoch: 1 [16000/60000]\tLoss: 0.701599\n",
      "Train Epoch: 1 [16640/60000]\tLoss: 0.305848\n",
      "Train Epoch: 1 [17280/60000]\tLoss: 0.654640\n",
      "Train Epoch: 1 [17920/60000]\tLoss: 0.751552\n",
      "Train Epoch: 1 [18560/60000]\tLoss: 0.548813\n",
      "Train Epoch: 1 [19200/60000]\tLoss: 0.732850\n",
      "Train Epoch: 1 [19840/60000]\tLoss: 0.463422\n",
      "Train Epoch: 1 [20480/60000]\tLoss: 0.480606\n",
      "Train Epoch: 1 [21120/60000]\tLoss: 0.407826\n",
      "Train Epoch: 1 [21760/60000]\tLoss: 0.616872\n",
      "Train Epoch: 1 [22400/60000]\tLoss: 0.501629\n",
      "Train Epoch: 1 [23040/60000]\tLoss: 0.483402\n",
      "Train Epoch: 1 [23680/60000]\tLoss: 0.466591\n",
      "Train Epoch: 1 [24320/60000]\tLoss: 0.565418\n",
      "Train Epoch: 1 [24960/60000]\tLoss: 0.292100\n",
      "Train Epoch: 1 [25600/60000]\tLoss: 0.200015\n",
      "Train Epoch: 1 [26240/60000]\tLoss: 0.286226\n",
      "Train Epoch: 1 [26880/60000]\tLoss: 0.255077\n",
      "Train Epoch: 1 [27520/60000]\tLoss: 0.440310\n",
      "Train Epoch: 1 [28160/60000]\tLoss: 0.553070\n",
      "Train Epoch: 1 [28800/60000]\tLoss: 0.445248\n",
      "Train Epoch: 1 [29440/60000]\tLoss: 0.461304\n",
      "Train Epoch: 1 [30080/60000]\tLoss: 0.422662\n",
      "Train Epoch: 1 [30720/60000]\tLoss: 0.299641\n",
      "Train Epoch: 1 [31360/60000]\tLoss: 0.375417\n",
      "Train Epoch: 1 [32000/60000]\tLoss: 0.379395\n",
      "Train Epoch: 1 [32640/60000]\tLoss: 0.283288\n",
      "Train Epoch: 1 [33280/60000]\tLoss: 0.260933\n",
      "Train Epoch: 1 [33920/60000]\tLoss: 0.231681\n",
      "Train Epoch: 1 [34560/60000]\tLoss: 0.428041\n",
      "Train Epoch: 1 [35200/60000]\tLoss: 0.399802\n",
      "Train Epoch: 1 [35840/60000]\tLoss: 0.358194\n",
      "Train Epoch: 1 [36480/60000]\tLoss: 0.233283\n",
      "Train Epoch: 1 [37120/60000]\tLoss: 0.371243\n",
      "Train Epoch: 1 [37760/60000]\tLoss: 0.263541\n",
      "Train Epoch: 1 [38400/60000]\tLoss: 0.522250\n",
      "Train Epoch: 1 [39040/60000]\tLoss: 0.416875\n",
      "Train Epoch: 1 [39680/60000]\tLoss: 0.309336\n",
      "Train Epoch: 1 [40320/60000]\tLoss: 0.274478\n",
      "Train Epoch: 1 [40960/60000]\tLoss: 0.361304\n",
      "Train Epoch: 1 [41600/60000]\tLoss: 0.285769\n",
      "Train Epoch: 1 [42240/60000]\tLoss: 0.190904\n",
      "Train Epoch: 1 [42880/60000]\tLoss: 0.468799\n",
      "Train Epoch: 1 [43520/60000]\tLoss: 0.377333\n",
      "Train Epoch: 1 [44160/60000]\tLoss: 0.138257\n",
      "Train Epoch: 1 [44800/60000]\tLoss: 0.560122\n",
      "Train Epoch: 1 [45440/60000]\tLoss: 0.203783\n",
      "Train Epoch: 1 [46080/60000]\tLoss: 0.422435\n",
      "Train Epoch: 1 [46720/60000]\tLoss: 0.296609\n",
      "Train Epoch: 1 [47360/60000]\tLoss: 0.349236\n",
      "Train Epoch: 1 [48000/60000]\tLoss: 0.339916\n",
      "Train Epoch: 1 [48640/60000]\tLoss: 0.272012\n",
      "Train Epoch: 1 [49280/60000]\tLoss: 0.250439\n",
      "Train Epoch: 1 [49920/60000]\tLoss: 0.202623\n",
      "Train Epoch: 1 [50560/60000]\tLoss: 0.224507\n",
      "Train Epoch: 1 [51200/60000]\tLoss: 0.451818\n",
      "Train Epoch: 1 [51840/60000]\tLoss: 0.407600\n",
      "Train Epoch: 1 [52480/60000]\tLoss: 0.197616\n",
      "Train Epoch: 1 [53120/60000]\tLoss: 0.385926\n",
      "Train Epoch: 1 [53760/60000]\tLoss: 0.244769\n",
      "Train Epoch: 1 [54400/60000]\tLoss: 0.304763\n",
      "Train Epoch: 1 [55040/60000]\tLoss: 0.238123\n",
      "Train Epoch: 1 [55680/60000]\tLoss: 0.337914\n",
      "Train Epoch: 1 [56320/60000]\tLoss: 0.131883\n",
      "Train Epoch: 1 [56960/60000]\tLoss: 0.120983\n",
      "Train Epoch: 1 [57600/60000]\tLoss: 0.204613\n",
      "Train Epoch: 1 [58240/60000]\tLoss: 0.224400\n",
      "Train Epoch: 1 [58880/60000]\tLoss: 0.175400\n",
      "Train Epoch: 1 [59520/60000]\tLoss: 0.403633\n",
      "Train Epoch: 2 [0/60000]\tLoss: 0.359714\n",
      "Train Epoch: 2 [640/60000]\tLoss: 0.326975\n",
      "Train Epoch: 2 [1280/60000]\tLoss: 0.305538\n",
      "Train Epoch: 2 [1920/60000]\tLoss: 0.487001\n",
      "Train Epoch: 2 [2560/60000]\tLoss: 0.286000\n",
      "Train Epoch: 2 [3200/60000]\tLoss: 0.495809\n",
      "Train Epoch: 2 [3840/60000]\tLoss: 0.233823\n",
      "Train Epoch: 2 [4480/60000]\tLoss: 0.255155\n",
      "Train Epoch: 2 [5120/60000]\tLoss: 0.154341\n",
      "Train Epoch: 2 [5760/60000]\tLoss: 0.362557\n",
      "Train Epoch: 2 [6400/60000]\tLoss: 0.186271\n",
      "Train Epoch: 2 [7040/60000]\tLoss: 0.097176\n",
      "Train Epoch: 2 [7680/60000]\tLoss: 0.197001\n",
      "Train Epoch: 2 [8320/60000]\tLoss: 0.179504\n",
      "Train Epoch: 2 [8960/60000]\tLoss: 0.201770\n",
      "Train Epoch: 2 [9600/60000]\tLoss: 0.133069\n",
      "Train Epoch: 2 [10240/60000]\tLoss: 0.202750\n",
      "Train Epoch: 2 [10880/60000]\tLoss: 0.222656\n",
      "Train Epoch: 2 [11520/60000]\tLoss: 0.294527\n",
      "Train Epoch: 2 [12160/60000]\tLoss: 0.181081\n",
      "Train Epoch: 2 [12800/60000]\tLoss: 0.130038\n",
      "Train Epoch: 2 [13440/60000]\tLoss: 0.256334\n",
      "Train Epoch: 2 [14080/60000]\tLoss: 0.178500\n",
      "Train Epoch: 2 [14720/60000]\tLoss: 0.204797\n",
      "Train Epoch: 2 [15360/60000]\tLoss: 0.257026\n",
      "Train Epoch: 2 [16000/60000]\tLoss: 0.219753\n",
      "Train Epoch: 2 [16640/60000]\tLoss: 0.264118\n",
      "Train Epoch: 2 [17280/60000]\tLoss: 0.244167\n",
      "Train Epoch: 2 [17920/60000]\tLoss: 0.139256\n",
      "Train Epoch: 2 [18560/60000]\tLoss: 0.182536\n",
      "Train Epoch: 2 [19200/60000]\tLoss: 0.192575\n",
      "Train Epoch: 2 [19840/60000]\tLoss: 0.199266\n",
      "Train Epoch: 2 [20480/60000]\tLoss: 0.291179\n",
      "Train Epoch: 2 [21120/60000]\tLoss: 0.250321\n",
      "Train Epoch: 2 [21760/60000]\tLoss: 0.269124\n",
      "Train Epoch: 2 [22400/60000]\tLoss: 0.132889\n",
      "Train Epoch: 2 [23040/60000]\tLoss: 0.305496\n",
      "Train Epoch: 2 [23680/60000]\tLoss: 0.313301\n",
      "Train Epoch: 2 [24320/60000]\tLoss: 0.176309\n",
      "Train Epoch: 2 [24960/60000]\tLoss: 0.195740\n",
      "Train Epoch: 2 [25600/60000]\tLoss: 0.117425\n",
      "Train Epoch: 2 [26240/60000]\tLoss: 0.159527\n",
      "Train Epoch: 2 [26880/60000]\tLoss: 0.227714\n",
      "Train Epoch: 2 [27520/60000]\tLoss: 0.137269\n",
      "Train Epoch: 2 [28160/60000]\tLoss: 0.191483\n",
      "Train Epoch: 2 [28800/60000]\tLoss: 0.105674\n",
      "Train Epoch: 2 [29440/60000]\tLoss: 0.207468\n",
      "Train Epoch: 2 [30080/60000]\tLoss: 0.196485\n",
      "Train Epoch: 2 [30720/60000]\tLoss: 0.063332\n",
      "Train Epoch: 2 [31360/60000]\tLoss: 0.288666\n",
      "Train Epoch: 2 [32000/60000]\tLoss: 0.152536\n",
      "Train Epoch: 2 [32640/60000]\tLoss: 0.204843\n",
      "Train Epoch: 2 [33280/60000]\tLoss: 0.234726\n",
      "Train Epoch: 2 [33920/60000]\tLoss: 0.112416\n",
      "Train Epoch: 2 [34560/60000]\tLoss: 0.084278\n",
      "Train Epoch: 2 [35200/60000]\tLoss: 0.096146\n",
      "Train Epoch: 2 [35840/60000]\tLoss: 0.122151\n",
      "Train Epoch: 2 [36480/60000]\tLoss: 0.313547\n",
      "Train Epoch: 2 [37120/60000]\tLoss: 0.309291\n",
      "Train Epoch: 2 [37760/60000]\tLoss: 0.249252\n",
      "Train Epoch: 2 [38400/60000]\tLoss: 0.263016\n",
      "Train Epoch: 2 [39040/60000]\tLoss: 0.263156\n",
      "Train Epoch: 2 [39680/60000]\tLoss: 0.195831\n",
      "Train Epoch: 2 [40320/60000]\tLoss: 0.259627\n",
      "Train Epoch: 2 [40960/60000]\tLoss: 0.095621\n",
      "Train Epoch: 2 [41600/60000]\tLoss: 0.295449\n",
      "Train Epoch: 2 [42240/60000]\tLoss: 0.225241\n",
      "Train Epoch: 2 [42880/60000]\tLoss: 0.134137\n",
      "Train Epoch: 2 [43520/60000]\tLoss: 0.273893\n",
      "Train Epoch: 2 [44160/60000]\tLoss: 0.099998\n",
      "Train Epoch: 2 [44800/60000]\tLoss: 0.181174\n",
      "Train Epoch: 2 [45440/60000]\tLoss: 0.380879\n",
      "Train Epoch: 2 [46080/60000]\tLoss: 0.148458\n",
      "Train Epoch: 2 [46720/60000]\tLoss: 0.244936\n",
      "Train Epoch: 2 [47360/60000]\tLoss: 0.060497\n",
      "Train Epoch: 2 [48000/60000]\tLoss: 0.237407\n",
      "Train Epoch: 2 [48640/60000]\tLoss: 0.167663\n",
      "Train Epoch: 2 [49280/60000]\tLoss: 0.439630\n",
      "Train Epoch: 2 [49920/60000]\tLoss: 0.292819\n",
      "Train Epoch: 2 [50560/60000]\tLoss: 0.171952\n",
      "Train Epoch: 2 [51200/60000]\tLoss: 0.197461\n",
      "Train Epoch: 2 [51840/60000]\tLoss: 0.295596\n",
      "Train Epoch: 2 [52480/60000]\tLoss: 0.282272\n",
      "Train Epoch: 2 [53120/60000]\tLoss: 0.191875\n",
      "Train Epoch: 2 [53760/60000]\tLoss: 0.120043\n",
      "Train Epoch: 2 [54400/60000]\tLoss: 0.245266\n",
      "Train Epoch: 2 [55040/60000]\tLoss: 0.092568\n",
      "Train Epoch: 2 [55680/60000]\tLoss: 0.150919\n",
      "Train Epoch: 2 [56320/60000]\tLoss: 0.052826\n",
      "Train Epoch: 2 [56960/60000]\tLoss: 0.186102\n",
      "Train Epoch: 2 [57600/60000]\tLoss: 0.165189\n",
      "Train Epoch: 2 [58240/60000]\tLoss: 0.253642\n",
      "Train Epoch: 2 [58880/60000]\tLoss: 0.260004\n",
      "Train Epoch: 2 [59520/60000]\tLoss: 0.057039\n",
      "Train Epoch: 3 [0/60000]\tLoss: 0.258115\n",
      "Train Epoch: 3 [640/60000]\tLoss: 0.140532\n",
      "Train Epoch: 3 [1280/60000]\tLoss: 0.048465\n",
      "Train Epoch: 3 [1920/60000]\tLoss: 0.205605\n",
      "Train Epoch: 3 [2560/60000]\tLoss: 0.122874\n",
      "Train Epoch: 3 [3200/60000]\tLoss: 0.154393\n",
      "Train Epoch: 3 [3840/60000]\tLoss: 0.196982\n",
      "Train Epoch: 3 [4480/60000]\tLoss: 0.065824\n",
      "Train Epoch: 3 [5120/60000]\tLoss: 0.070507\n",
      "Train Epoch: 3 [5760/60000]\tLoss: 0.385835\n",
      "Train Epoch: 3 [6400/60000]\tLoss: 0.072488\n",
      "Train Epoch: 3 [7040/60000]\tLoss: 0.138237\n",
      "Train Epoch: 3 [7680/60000]\tLoss: 0.200472\n",
      "Train Epoch: 3 [8320/60000]\tLoss: 0.158362\n",
      "Train Epoch: 3 [8960/60000]\tLoss: 0.097059\n",
      "Train Epoch: 3 [9600/60000]\tLoss: 0.210641\n",
      "Train Epoch: 3 [10240/60000]\tLoss: 0.270974\n",
      "Train Epoch: 3 [10880/60000]\tLoss: 0.188859\n",
      "Train Epoch: 3 [11520/60000]\tLoss: 0.120743\n",
      "Train Epoch: 3 [12160/60000]\tLoss: 0.149949\n",
      "Train Epoch: 3 [12800/60000]\tLoss: 0.097544\n",
      "Train Epoch: 3 [13440/60000]\tLoss: 0.151145\n",
      "Train Epoch: 3 [14080/60000]\tLoss: 0.165842\n",
      "Train Epoch: 3 [14720/60000]\tLoss: 0.112454\n",
      "Train Epoch: 3 [15360/60000]\tLoss: 0.041201\n",
      "Train Epoch: 3 [16000/60000]\tLoss: 0.138368\n",
      "Train Epoch: 3 [16640/60000]\tLoss: 0.160598\n",
      "Train Epoch: 3 [17280/60000]\tLoss: 0.135540\n",
      "Train Epoch: 3 [17920/60000]\tLoss: 0.256680\n",
      "Train Epoch: 3 [18560/60000]\tLoss: 0.222570\n",
      "Train Epoch: 3 [19200/60000]\tLoss: 0.103357\n",
      "Train Epoch: 3 [19840/60000]\tLoss: 0.304096\n",
      "Train Epoch: 3 [20480/60000]\tLoss: 0.166864\n",
      "Train Epoch: 3 [21120/60000]\tLoss: 0.183171\n",
      "Train Epoch: 3 [21760/60000]\tLoss: 0.046504\n",
      "Train Epoch: 3 [22400/60000]\tLoss: 0.167624\n",
      "Train Epoch: 3 [23040/60000]\tLoss: 0.202676\n",
      "Train Epoch: 3 [23680/60000]\tLoss: 0.054991\n",
      "Train Epoch: 3 [24320/60000]\tLoss: 0.162782\n",
      "Train Epoch: 3 [24960/60000]\tLoss: 0.135913\n",
      "Train Epoch: 3 [25600/60000]\tLoss: 0.334404\n",
      "Train Epoch: 3 [26240/60000]\tLoss: 0.514123\n",
      "Train Epoch: 3 [26880/60000]\tLoss: 0.123095\n",
      "Train Epoch: 3 [27520/60000]\tLoss: 0.224314\n",
      "Train Epoch: 3 [28160/60000]\tLoss: 0.070495\n",
      "Train Epoch: 3 [28800/60000]\tLoss: 0.193810\n",
      "Train Epoch: 3 [29440/60000]\tLoss: 0.228373\n",
      "Train Epoch: 3 [30080/60000]\tLoss: 0.177751\n",
      "Train Epoch: 3 [30720/60000]\tLoss: 0.155247\n",
      "Train Epoch: 3 [31360/60000]\tLoss: 0.172099\n",
      "Train Epoch: 3 [32000/60000]\tLoss: 0.183486\n",
      "Train Epoch: 3 [32640/60000]\tLoss: 0.134661\n",
      "Train Epoch: 3 [33280/60000]\tLoss: 0.221265\n",
      "Train Epoch: 3 [33920/60000]\tLoss: 0.097042\n",
      "Train Epoch: 3 [34560/60000]\tLoss: 0.159775\n",
      "Train Epoch: 3 [35200/60000]\tLoss: 0.056170\n",
      "Train Epoch: 3 [35840/60000]\tLoss: 0.288269\n",
      "Train Epoch: 3 [36480/60000]\tLoss: 0.119893\n",
      "Train Epoch: 3 [37120/60000]\tLoss: 0.077388\n",
      "Train Epoch: 3 [37760/60000]\tLoss: 0.131531\n",
      "Train Epoch: 3 [38400/60000]\tLoss: 0.074205\n",
      "Train Epoch: 3 [39040/60000]\tLoss: 0.044009\n",
      "Train Epoch: 3 [39680/60000]\tLoss: 0.263426\n",
      "Train Epoch: 3 [40320/60000]\tLoss: 0.109330\n",
      "Train Epoch: 3 [40960/60000]\tLoss: 0.170776\n",
      "Train Epoch: 3 [41600/60000]\tLoss: 0.127335\n",
      "Train Epoch: 3 [42240/60000]\tLoss: 0.198020\n",
      "Train Epoch: 3 [42880/60000]\tLoss: 0.072806\n",
      "Train Epoch: 3 [43520/60000]\tLoss: 0.230316\n",
      "Train Epoch: 3 [44160/60000]\tLoss: 0.068463\n",
      "Train Epoch: 3 [44800/60000]\tLoss: 0.212559\n",
      "Train Epoch: 3 [45440/60000]\tLoss: 0.092326\n",
      "Train Epoch: 3 [46080/60000]\tLoss: 0.177738\n",
      "Train Epoch: 3 [46720/60000]\tLoss: 0.020824\n",
      "Train Epoch: 3 [47360/60000]\tLoss: 0.273862\n",
      "Train Epoch: 3 [48000/60000]\tLoss: 0.132566\n",
      "Train Epoch: 3 [48640/60000]\tLoss: 0.253807\n",
      "Train Epoch: 3 [49280/60000]\tLoss: 0.192663\n",
      "Train Epoch: 3 [49920/60000]\tLoss: 0.138370\n",
      "Train Epoch: 3 [50560/60000]\tLoss: 0.263468\n",
      "Train Epoch: 3 [51200/60000]\tLoss: 0.162798\n",
      "Train Epoch: 3 [51840/60000]\tLoss: 0.108142\n",
      "Train Epoch: 3 [52480/60000]\tLoss: 0.122969\n",
      "Train Epoch: 3 [53120/60000]\tLoss: 0.146609\n",
      "Train Epoch: 3 [53760/60000]\tLoss: 0.260101\n",
      "Train Epoch: 3 [54400/60000]\tLoss: 0.117149\n",
      "Train Epoch: 3 [55040/60000]\tLoss: 0.030129\n",
      "Train Epoch: 3 [55680/60000]\tLoss: 0.217736\n",
      "Train Epoch: 3 [56320/60000]\tLoss: 0.215705\n",
      "Train Epoch: 3 [56960/60000]\tLoss: 0.254250\n",
      "Train Epoch: 3 [57600/60000]\tLoss: 0.229952\n",
      "Train Epoch: 3 [58240/60000]\tLoss: 0.048723\n",
      "Train Epoch: 3 [58880/60000]\tLoss: 0.053369\n",
      "Train Epoch: 3 [59520/60000]\tLoss: 0.061488\n",
      "Train Epoch: 4 [0/60000]\tLoss: 0.179684\n",
      "Train Epoch: 4 [640/60000]\tLoss: 0.084432\n",
      "Train Epoch: 4 [1280/60000]\tLoss: 0.287126\n",
      "Train Epoch: 4 [1920/60000]\tLoss: 0.152937\n",
      "Train Epoch: 4 [2560/60000]\tLoss: 0.259431\n",
      "Train Epoch: 4 [3200/60000]\tLoss: 0.106013\n",
      "Train Epoch: 4 [3840/60000]\tLoss: 0.093826\n",
      "Train Epoch: 4 [4480/60000]\tLoss: 0.066640\n",
      "Train Epoch: 4 [5120/60000]\tLoss: 0.187763\n",
      "Train Epoch: 4 [5760/60000]\tLoss: 0.062608\n",
      "Train Epoch: 4 [6400/60000]\tLoss: 0.139138\n",
      "Train Epoch: 4 [7040/60000]\tLoss: 0.175135\n",
      "Train Epoch: 4 [7680/60000]\tLoss: 0.026880\n",
      "Train Epoch: 4 [8320/60000]\tLoss: 0.131406\n",
      "Train Epoch: 4 [8960/60000]\tLoss: 0.104032\n",
      "Train Epoch: 4 [9600/60000]\tLoss: 0.066350\n",
      "Train Epoch: 4 [10240/60000]\tLoss: 0.129659\n",
      "Train Epoch: 4 [10880/60000]\tLoss: 0.047753\n",
      "Train Epoch: 4 [11520/60000]\tLoss: 0.125974\n",
      "Train Epoch: 4 [12160/60000]\tLoss: 0.134120\n",
      "Train Epoch: 4 [12800/60000]\tLoss: 0.211480\n",
      "Train Epoch: 4 [13440/60000]\tLoss: 0.156127\n",
      "Train Epoch: 4 [14080/60000]\tLoss: 0.204079\n",
      "Train Epoch: 4 [14720/60000]\tLoss: 0.060354\n",
      "Train Epoch: 4 [15360/60000]\tLoss: 0.209708\n",
      "Train Epoch: 4 [16000/60000]\tLoss: 0.071794\n",
      "Train Epoch: 4 [16640/60000]\tLoss: 0.226305\n",
      "Train Epoch: 4 [17280/60000]\tLoss: 0.287009\n",
      "Train Epoch: 4 [17920/60000]\tLoss: 0.209940\n",
      "Train Epoch: 4 [18560/60000]\tLoss: 0.246076\n",
      "Train Epoch: 4 [19200/60000]\tLoss: 0.325532\n",
      "Train Epoch: 4 [19840/60000]\tLoss: 0.169475\n",
      "Train Epoch: 4 [20480/60000]\tLoss: 0.100477\n",
      "Train Epoch: 4 [21120/60000]\tLoss: 0.041324\n",
      "Train Epoch: 4 [21760/60000]\tLoss: 0.108377\n",
      "Train Epoch: 4 [22400/60000]\tLoss: 0.100182\n",
      "Train Epoch: 4 [23040/60000]\tLoss: 0.071388\n",
      "Train Epoch: 4 [23680/60000]\tLoss: 0.124867\n",
      "Train Epoch: 4 [24320/60000]\tLoss: 0.161225\n",
      "Train Epoch: 4 [24960/60000]\tLoss: 0.295094\n",
      "Train Epoch: 4 [25600/60000]\tLoss: 0.067632\n",
      "Train Epoch: 4 [26240/60000]\tLoss: 0.055904\n",
      "Train Epoch: 4 [26880/60000]\tLoss: 0.095792\n",
      "Train Epoch: 4 [27520/60000]\tLoss: 0.073054\n",
      "Train Epoch: 4 [28160/60000]\tLoss: 0.280526\n",
      "Train Epoch: 4 [28800/60000]\tLoss: 0.091708\n",
      "Train Epoch: 4 [29440/60000]\tLoss: 0.043868\n",
      "Train Epoch: 4 [30080/60000]\tLoss: 0.141410\n",
      "Train Epoch: 4 [30720/60000]\tLoss: 0.114600\n",
      "Train Epoch: 4 [31360/60000]\tLoss: 0.232813\n",
      "Train Epoch: 4 [32000/60000]\tLoss: 0.278024\n",
      "Train Epoch: 4 [32640/60000]\tLoss: 0.211575\n",
      "Train Epoch: 4 [33280/60000]\tLoss: 0.166381\n",
      "Train Epoch: 4 [33920/60000]\tLoss: 0.184030\n",
      "Train Epoch: 4 [34560/60000]\tLoss: 0.063531\n",
      "Train Epoch: 4 [35200/60000]\tLoss: 0.057441\n",
      "Train Epoch: 4 [35840/60000]\tLoss: 0.037781\n",
      "Train Epoch: 4 [36480/60000]\tLoss: 0.148893\n",
      "Train Epoch: 4 [37120/60000]\tLoss: 0.041676\n",
      "Train Epoch: 4 [37760/60000]\tLoss: 0.213974\n",
      "Train Epoch: 4 [38400/60000]\tLoss: 0.037289\n",
      "Train Epoch: 4 [39040/60000]\tLoss: 0.051236\n",
      "Train Epoch: 4 [39680/60000]\tLoss: 0.120513\n",
      "Train Epoch: 4 [40320/60000]\tLoss: 0.134429\n",
      "Train Epoch: 4 [40960/60000]\tLoss: 0.037234\n",
      "Train Epoch: 4 [41600/60000]\tLoss: 0.188277\n",
      "Train Epoch: 4 [42240/60000]\tLoss: 0.313962\n",
      "Train Epoch: 4 [42880/60000]\tLoss: 0.203274\n",
      "Train Epoch: 4 [43520/60000]\tLoss: 0.177258\n",
      "Train Epoch: 4 [44160/60000]\tLoss: 0.074570\n",
      "Train Epoch: 4 [44800/60000]\tLoss: 0.104989\n",
      "Train Epoch: 4 [45440/60000]\tLoss: 0.083571\n",
      "Train Epoch: 4 [46080/60000]\tLoss: 0.118750\n",
      "Train Epoch: 4 [46720/60000]\tLoss: 0.110035\n",
      "Train Epoch: 4 [47360/60000]\tLoss: 0.127802\n",
      "Train Epoch: 4 [48000/60000]\tLoss: 0.268792\n",
      "Train Epoch: 4 [48640/60000]\tLoss: 0.162139\n",
      "Train Epoch: 4 [49280/60000]\tLoss: 0.337987\n",
      "Train Epoch: 4 [49920/60000]\tLoss: 0.123901\n",
      "Train Epoch: 4 [50560/60000]\tLoss: 0.157545\n",
      "Train Epoch: 4 [51200/60000]\tLoss: 0.174062\n",
      "Train Epoch: 4 [51840/60000]\tLoss: 0.074971\n",
      "Train Epoch: 4 [52480/60000]\tLoss: 0.131087\n",
      "Train Epoch: 4 [53120/60000]\tLoss: 0.123159\n",
      "Train Epoch: 4 [53760/60000]\tLoss: 0.174558\n",
      "Train Epoch: 4 [54400/60000]\tLoss: 0.153531\n",
      "Train Epoch: 4 [55040/60000]\tLoss: 0.053319\n",
      "Train Epoch: 4 [55680/60000]\tLoss: 0.084500\n",
      "Train Epoch: 4 [56320/60000]\tLoss: 0.150797\n",
      "Train Epoch: 4 [56960/60000]\tLoss: 0.113793\n",
      "Train Epoch: 4 [57600/60000]\tLoss: 0.048583\n",
      "Train Epoch: 4 [58240/60000]\tLoss: 0.036640\n",
      "Train Epoch: 4 [58880/60000]\tLoss: 0.363730\n",
      "Train Epoch: 4 [59520/60000]\tLoss: 0.108190\n",
      "Train Epoch: 5 [0/60000]\tLoss: 0.177067\n",
      "Train Epoch: 5 [640/60000]\tLoss: 0.028037\n",
      "Train Epoch: 5 [1280/60000]\tLoss: 0.140249\n",
      "Train Epoch: 5 [1920/60000]\tLoss: 0.211640\n",
      "Train Epoch: 5 [2560/60000]\tLoss: 0.138327\n",
      "Train Epoch: 5 [3200/60000]\tLoss: 0.113427\n",
      "Train Epoch: 5 [3840/60000]\tLoss: 0.121426\n",
      "Train Epoch: 5 [4480/60000]\tLoss: 0.070343\n",
      "Train Epoch: 5 [5120/60000]\tLoss: 0.142312\n",
      "Train Epoch: 5 [5760/60000]\tLoss: 0.110697\n",
      "Train Epoch: 5 [6400/60000]\tLoss: 0.103083\n",
      "Train Epoch: 5 [7040/60000]\tLoss: 0.200007\n",
      "Train Epoch: 5 [7680/60000]\tLoss: 0.211697\n",
      "Train Epoch: 5 [8320/60000]\tLoss: 0.182546\n",
      "Train Epoch: 5 [8960/60000]\tLoss: 0.048724\n",
      "Train Epoch: 5 [9600/60000]\tLoss: 0.243195\n",
      "Train Epoch: 5 [10240/60000]\tLoss: 0.070888\n",
      "Train Epoch: 5 [10880/60000]\tLoss: 0.105998\n",
      "Train Epoch: 5 [11520/60000]\tLoss: 0.093634\n",
      "Train Epoch: 5 [12160/60000]\tLoss: 0.051236\n",
      "Train Epoch: 5 [12800/60000]\tLoss: 0.048191\n",
      "Train Epoch: 5 [13440/60000]\tLoss: 0.062717\n",
      "Train Epoch: 5 [14080/60000]\tLoss: 0.032327\n",
      "Train Epoch: 5 [14720/60000]\tLoss: 0.148223\n",
      "Train Epoch: 5 [15360/60000]\tLoss: 0.044983\n",
      "Train Epoch: 5 [16000/60000]\tLoss: 0.066261\n",
      "Train Epoch: 5 [16640/60000]\tLoss: 0.082760\n",
      "Train Epoch: 5 [17280/60000]\tLoss: 0.072372\n",
      "Train Epoch: 5 [17920/60000]\tLoss: 0.162422\n",
      "Train Epoch: 5 [18560/60000]\tLoss: 0.081699\n",
      "Train Epoch: 5 [19200/60000]\tLoss: 0.147726\n",
      "Train Epoch: 5 [19840/60000]\tLoss: 0.034257\n",
      "Train Epoch: 5 [20480/60000]\tLoss: 0.055592\n",
      "Train Epoch: 5 [21120/60000]\tLoss: 0.177999\n",
      "Train Epoch: 5 [21760/60000]\tLoss: 0.200269\n",
      "Train Epoch: 5 [22400/60000]\tLoss: 0.143030\n",
      "Train Epoch: 5 [23040/60000]\tLoss: 0.119569\n",
      "Train Epoch: 5 [23680/60000]\tLoss: 0.046708\n",
      "Train Epoch: 5 [24320/60000]\tLoss: 0.245986\n",
      "Train Epoch: 5 [24960/60000]\tLoss: 0.085146\n",
      "Train Epoch: 5 [25600/60000]\tLoss: 0.107226\n",
      "Train Epoch: 5 [26240/60000]\tLoss: 0.036795\n",
      "Train Epoch: 5 [26880/60000]\tLoss: 0.058329\n",
      "Train Epoch: 5 [27520/60000]\tLoss: 0.087675\n",
      "Train Epoch: 5 [28160/60000]\tLoss: 0.054489\n",
      "Train Epoch: 5 [28800/60000]\tLoss: 0.357814\n",
      "Train Epoch: 5 [29440/60000]\tLoss: 0.156578\n",
      "Train Epoch: 5 [30080/60000]\tLoss: 0.112349\n",
      "Train Epoch: 5 [30720/60000]\tLoss: 0.083783\n",
      "Train Epoch: 5 [31360/60000]\tLoss: 0.026785\n",
      "Train Epoch: 5 [32000/60000]\tLoss: 0.116521\n",
      "Train Epoch: 5 [32640/60000]\tLoss: 0.012182\n",
      "Train Epoch: 5 [33280/60000]\tLoss: 0.049173\n",
      "Train Epoch: 5 [33920/60000]\tLoss: 0.060459\n",
      "Train Epoch: 5 [34560/60000]\tLoss: 0.092353\n",
      "Train Epoch: 5 [35200/60000]\tLoss: 0.120922\n",
      "Train Epoch: 5 [35840/60000]\tLoss: 0.057473\n",
      "Train Epoch: 5 [36480/60000]\tLoss: 0.110937\n",
      "Train Epoch: 5 [37120/60000]\tLoss: 0.115840\n",
      "Train Epoch: 5 [37760/60000]\tLoss: 0.131646\n",
      "Train Epoch: 5 [38400/60000]\tLoss: 0.101483\n",
      "Train Epoch: 5 [39040/60000]\tLoss: 0.160411\n",
      "Train Epoch: 5 [39680/60000]\tLoss: 0.057262\n",
      "Train Epoch: 5 [40320/60000]\tLoss: 0.011212\n",
      "Train Epoch: 5 [40960/60000]\tLoss: 0.082463\n",
      "Train Epoch: 5 [41600/60000]\tLoss: 0.064663\n",
      "Train Epoch: 5 [42240/60000]\tLoss: 0.039419\n",
      "Train Epoch: 5 [42880/60000]\tLoss: 0.061917\n",
      "Train Epoch: 5 [43520/60000]\tLoss: 0.051848\n",
      "Train Epoch: 5 [44160/60000]\tLoss: 0.128174\n",
      "Train Epoch: 5 [44800/60000]\tLoss: 0.134753\n",
      "Train Epoch: 5 [45440/60000]\tLoss: 0.203903\n",
      "Train Epoch: 5 [46080/60000]\tLoss: 0.080958\n",
      "Train Epoch: 5 [46720/60000]\tLoss: 0.234770\n",
      "Train Epoch: 5 [47360/60000]\tLoss: 0.123813\n",
      "Train Epoch: 5 [48000/60000]\tLoss: 0.031412\n",
      "Train Epoch: 5 [48640/60000]\tLoss: 0.130823\n",
      "Train Epoch: 5 [49280/60000]\tLoss: 0.193049\n",
      "Train Epoch: 5 [49920/60000]\tLoss: 0.039530\n",
      "Train Epoch: 5 [50560/60000]\tLoss: 0.131086\n",
      "Train Epoch: 5 [51200/60000]\tLoss: 0.050444\n",
      "Train Epoch: 5 [51840/60000]\tLoss: 0.104177\n",
      "Train Epoch: 5 [52480/60000]\tLoss: 0.185972\n",
      "Train Epoch: 5 [53120/60000]\tLoss: 0.344734\n",
      "Train Epoch: 5 [53760/60000]\tLoss: 0.083812\n",
      "Train Epoch: 5 [54400/60000]\tLoss: 0.119792\n",
      "Train Epoch: 5 [55040/60000]\tLoss: 0.183286\n",
      "Train Epoch: 5 [55680/60000]\tLoss: 0.109415\n",
      "Train Epoch: 5 [56320/60000]\tLoss: 0.155072\n",
      "Train Epoch: 5 [56960/60000]\tLoss: 0.165205\n",
      "Train Epoch: 5 [57600/60000]\tLoss: 0.053962\n",
      "Train Epoch: 5 [58240/60000]\tLoss: 0.060115\n",
      "Train Epoch: 5 [58880/60000]\tLoss: 0.108274\n",
      "Train Epoch: 5 [59520/60000]\tLoss: 0.118028\n",
      "Train Epoch: 6 [0/60000]\tLoss: 0.131018\n",
      "Train Epoch: 6 [640/60000]\tLoss: 0.101876\n",
      "Train Epoch: 6 [1280/60000]\tLoss: 0.098931\n",
      "Train Epoch: 6 [1920/60000]\tLoss: 0.014919\n",
      "Train Epoch: 6 [2560/60000]\tLoss: 0.042764\n",
      "Train Epoch: 6 [3200/60000]\tLoss: 0.094755\n",
      "Train Epoch: 6 [3840/60000]\tLoss: 0.107309\n",
      "Train Epoch: 6 [4480/60000]\tLoss: 0.170877\n",
      "Train Epoch: 6 [5120/60000]\tLoss: 0.039136\n",
      "Train Epoch: 6 [5760/60000]\tLoss: 0.106883\n",
      "Train Epoch: 6 [6400/60000]\tLoss: 0.052055\n",
      "Train Epoch: 6 [7040/60000]\tLoss: 0.048013\n",
      "Train Epoch: 6 [7680/60000]\tLoss: 0.178962\n",
      "Train Epoch: 6 [8320/60000]\tLoss: 0.085486\n",
      "Train Epoch: 6 [8960/60000]\tLoss: 0.036916\n",
      "Train Epoch: 6 [9600/60000]\tLoss: 0.040292\n",
      "Train Epoch: 6 [10240/60000]\tLoss: 0.126318\n",
      "Train Epoch: 6 [10880/60000]\tLoss: 0.049450\n",
      "Train Epoch: 6 [11520/60000]\tLoss: 0.177988\n",
      "Train Epoch: 6 [12160/60000]\tLoss: 0.246958\n",
      "Train Epoch: 6 [12800/60000]\tLoss: 0.142818\n",
      "Train Epoch: 6 [13440/60000]\tLoss: 0.091254\n",
      "Train Epoch: 6 [14080/60000]\tLoss: 0.042585\n",
      "Train Epoch: 6 [14720/60000]\tLoss: 0.046360\n",
      "Train Epoch: 6 [15360/60000]\tLoss: 0.126014\n",
      "Train Epoch: 6 [16000/60000]\tLoss: 0.264831\n",
      "Train Epoch: 6 [16640/60000]\tLoss: 0.159436\n",
      "Train Epoch: 6 [17280/60000]\tLoss: 0.032164\n",
      "Train Epoch: 6 [17920/60000]\tLoss: 0.158307\n",
      "Train Epoch: 6 [18560/60000]\tLoss: 0.052854\n",
      "Train Epoch: 6 [19200/60000]\tLoss: 0.048333\n",
      "Train Epoch: 6 [19840/60000]\tLoss: 0.165248\n",
      "Train Epoch: 6 [20480/60000]\tLoss: 0.127163\n",
      "Train Epoch: 6 [21120/60000]\tLoss: 0.118289\n",
      "Train Epoch: 6 [21760/60000]\tLoss: 0.117414\n",
      "Train Epoch: 6 [22400/60000]\tLoss: 0.085826\n",
      "Train Epoch: 6 [23040/60000]\tLoss: 0.042946\n",
      "Train Epoch: 6 [23680/60000]\tLoss: 0.062570\n",
      "Train Epoch: 6 [24320/60000]\tLoss: 0.065970\n",
      "Train Epoch: 6 [24960/60000]\tLoss: 0.048359\n",
      "Train Epoch: 6 [25600/60000]\tLoss: 0.059315\n",
      "Train Epoch: 6 [26240/60000]\tLoss: 0.084037\n",
      "Train Epoch: 6 [26880/60000]\tLoss: 0.133247\n",
      "Train Epoch: 6 [27520/60000]\tLoss: 0.174906\n",
      "Train Epoch: 6 [28160/60000]\tLoss: 0.216673\n",
      "Train Epoch: 6 [28800/60000]\tLoss: 0.136985\n",
      "Train Epoch: 6 [29440/60000]\tLoss: 0.072839\n",
      "Train Epoch: 6 [30080/60000]\tLoss: 0.055941\n",
      "Train Epoch: 6 [30720/60000]\tLoss: 0.213543\n",
      "Train Epoch: 6 [31360/60000]\tLoss: 0.077003\n",
      "Train Epoch: 6 [32000/60000]\tLoss: 0.121004\n",
      "Train Epoch: 6 [32640/60000]\tLoss: 0.119119\n",
      "Train Epoch: 6 [33280/60000]\tLoss: 0.119855\n",
      "Train Epoch: 6 [33920/60000]\tLoss: 0.076185\n",
      "Train Epoch: 6 [34560/60000]\tLoss: 0.174100\n",
      "Train Epoch: 6 [35200/60000]\tLoss: 0.025415\n",
      "Train Epoch: 6 [35840/60000]\tLoss: 0.226055\n",
      "Train Epoch: 6 [36480/60000]\tLoss: 0.081040\n",
      "Train Epoch: 6 [37120/60000]\tLoss: 0.056956\n",
      "Train Epoch: 6 [37760/60000]\tLoss: 0.235615\n",
      "Train Epoch: 6 [38400/60000]\tLoss: 0.087650\n",
      "Train Epoch: 6 [39040/60000]\tLoss: 0.039016\n",
      "Train Epoch: 6 [39680/60000]\tLoss: 0.040879\n",
      "Train Epoch: 6 [40320/60000]\tLoss: 0.112155\n",
      "Train Epoch: 6 [40960/60000]\tLoss: 0.203712\n",
      "Train Epoch: 6 [41600/60000]\tLoss: 0.118478\n",
      "Train Epoch: 6 [42240/60000]\tLoss: 0.226623\n",
      "Train Epoch: 6 [42880/60000]\tLoss: 0.147129\n",
      "Train Epoch: 6 [43520/60000]\tLoss: 0.049556\n",
      "Train Epoch: 6 [44160/60000]\tLoss: 0.094148\n",
      "Train Epoch: 6 [44800/60000]\tLoss: 0.128392\n",
      "Train Epoch: 6 [45440/60000]\tLoss: 0.216025\n",
      "Train Epoch: 6 [46080/60000]\tLoss: 0.127097\n",
      "Train Epoch: 6 [46720/60000]\tLoss: 0.046761\n",
      "Train Epoch: 6 [47360/60000]\tLoss: 0.049430\n",
      "Train Epoch: 6 [48000/60000]\tLoss: 0.137527\n",
      "Train Epoch: 6 [48640/60000]\tLoss: 0.091460\n",
      "Train Epoch: 6 [49280/60000]\tLoss: 0.151172\n",
      "Train Epoch: 6 [49920/60000]\tLoss: 0.092494\n",
      "Train Epoch: 6 [50560/60000]\tLoss: 0.035666\n",
      "Train Epoch: 6 [51200/60000]\tLoss: 0.044440\n",
      "Train Epoch: 6 [51840/60000]\tLoss: 0.268492\n",
      "Train Epoch: 6 [52480/60000]\tLoss: 0.053519\n",
      "Train Epoch: 6 [53120/60000]\tLoss: 0.033111\n",
      "Train Epoch: 6 [53760/60000]\tLoss: 0.034361\n",
      "Train Epoch: 6 [54400/60000]\tLoss: 0.145370\n",
      "Train Epoch: 6 [55040/60000]\tLoss: 0.102340\n",
      "Train Epoch: 6 [55680/60000]\tLoss: 0.085327\n",
      "Train Epoch: 6 [56320/60000]\tLoss: 0.011310\n",
      "Train Epoch: 6 [56960/60000]\tLoss: 0.159007\n",
      "Train Epoch: 6 [57600/60000]\tLoss: 0.107873\n",
      "Train Epoch: 6 [58240/60000]\tLoss: 0.181550\n",
      "Train Epoch: 6 [58880/60000]\tLoss: 0.098259\n",
      "Train Epoch: 6 [59520/60000]\tLoss: 0.216930\n",
      "Train Epoch: 7 [0/60000]\tLoss: 0.079767\n",
      "Train Epoch: 7 [640/60000]\tLoss: 0.058752\n",
      "Train Epoch: 7 [1280/60000]\tLoss: 0.108883\n",
      "Train Epoch: 7 [1920/60000]\tLoss: 0.093359\n",
      "Train Epoch: 7 [2560/60000]\tLoss: 0.076918\n",
      "Train Epoch: 7 [3200/60000]\tLoss: 0.099515\n",
      "Train Epoch: 7 [3840/60000]\tLoss: 0.044816\n",
      "Train Epoch: 7 [4480/60000]\tLoss: 0.096678\n",
      "Train Epoch: 7 [5120/60000]\tLoss: 0.051088\n",
      "Train Epoch: 7 [5760/60000]\tLoss: 0.091792\n",
      "Train Epoch: 7 [6400/60000]\tLoss: 0.186444\n",
      "Train Epoch: 7 [7040/60000]\tLoss: 0.139465\n",
      "Train Epoch: 7 [7680/60000]\tLoss: 0.163299\n",
      "Train Epoch: 7 [8320/60000]\tLoss: 0.037423\n",
      "Train Epoch: 7 [8960/60000]\tLoss: 0.015820\n",
      "Train Epoch: 7 [9600/60000]\tLoss: 0.105084\n",
      "Train Epoch: 7 [10240/60000]\tLoss: 0.087312\n",
      "Train Epoch: 7 [10880/60000]\tLoss: 0.026526\n",
      "Train Epoch: 7 [11520/60000]\tLoss: 0.095830\n",
      "Train Epoch: 7 [12160/60000]\tLoss: 0.205661\n",
      "Train Epoch: 7 [12800/60000]\tLoss: 0.081742\n",
      "Train Epoch: 7 [13440/60000]\tLoss: 0.033341\n",
      "Train Epoch: 7 [14080/60000]\tLoss: 0.125456\n",
      "Train Epoch: 7 [14720/60000]\tLoss: 0.129649\n",
      "Train Epoch: 7 [15360/60000]\tLoss: 0.045930\n",
      "Train Epoch: 7 [16000/60000]\tLoss: 0.030972\n",
      "Train Epoch: 7 [16640/60000]\tLoss: 0.100835\n",
      "Train Epoch: 7 [17280/60000]\tLoss: 0.159788\n",
      "Train Epoch: 7 [17920/60000]\tLoss: 0.051146\n",
      "Train Epoch: 7 [18560/60000]\tLoss: 0.074235\n",
      "Train Epoch: 7 [19200/60000]\tLoss: 0.059312\n",
      "Train Epoch: 7 [19840/60000]\tLoss: 0.023853\n",
      "Train Epoch: 7 [20480/60000]\tLoss: 0.052962\n",
      "Train Epoch: 7 [21120/60000]\tLoss: 0.245148\n",
      "Train Epoch: 7 [21760/60000]\tLoss: 0.199754\n",
      "Train Epoch: 7 [22400/60000]\tLoss: 0.029283\n",
      "Train Epoch: 7 [23040/60000]\tLoss: 0.138415\n",
      "Train Epoch: 7 [23680/60000]\tLoss: 0.269003\n",
      "Train Epoch: 7 [24320/60000]\tLoss: 0.077984\n",
      "Train Epoch: 7 [24960/60000]\tLoss: 0.248000\n",
      "Train Epoch: 7 [25600/60000]\tLoss: 0.041389\n",
      "Train Epoch: 7 [26240/60000]\tLoss: 0.027871\n",
      "Train Epoch: 7 [26880/60000]\tLoss: 0.048835\n",
      "Train Epoch: 7 [27520/60000]\tLoss: 0.065261\n",
      "Train Epoch: 7 [28160/60000]\tLoss: 0.039421\n",
      "Train Epoch: 7 [28800/60000]\tLoss: 0.062706\n",
      "Train Epoch: 7 [29440/60000]\tLoss: 0.148555\n",
      "Train Epoch: 7 [30080/60000]\tLoss: 0.106773\n",
      "Train Epoch: 7 [30720/60000]\tLoss: 0.145908\n",
      "Train Epoch: 7 [31360/60000]\tLoss: 0.161595\n",
      "Train Epoch: 7 [32000/60000]\tLoss: 0.075816\n",
      "Train Epoch: 7 [32640/60000]\tLoss: 0.045780\n",
      "Train Epoch: 7 [33280/60000]\tLoss: 0.108935\n",
      "Train Epoch: 7 [33920/60000]\tLoss: 0.015139\n",
      "Train Epoch: 7 [34560/60000]\tLoss: 0.095816\n",
      "Train Epoch: 7 [35200/60000]\tLoss: 0.013880\n",
      "Train Epoch: 7 [35840/60000]\tLoss: 0.018898\n",
      "Train Epoch: 7 [36480/60000]\tLoss: 0.066935\n",
      "Train Epoch: 7 [37120/60000]\tLoss: 0.128250\n",
      "Train Epoch: 7 [37760/60000]\tLoss: 0.173380\n",
      "Train Epoch: 7 [38400/60000]\tLoss: 0.149140\n",
      "Train Epoch: 7 [39040/60000]\tLoss: 0.059300\n",
      "Train Epoch: 7 [39680/60000]\tLoss: 0.061453\n",
      "Train Epoch: 7 [40320/60000]\tLoss: 0.112149\n",
      "Train Epoch: 7 [40960/60000]\tLoss: 0.079787\n",
      "Train Epoch: 7 [41600/60000]\tLoss: 0.099423\n",
      "Train Epoch: 7 [42240/60000]\tLoss: 0.194321\n",
      "Train Epoch: 7 [42880/60000]\tLoss: 0.076024\n",
      "Train Epoch: 7 [43520/60000]\tLoss: 0.049361\n",
      "Train Epoch: 7 [44160/60000]\tLoss: 0.285421\n",
      "Train Epoch: 7 [44800/60000]\tLoss: 0.019923\n",
      "Train Epoch: 7 [45440/60000]\tLoss: 0.064464\n",
      "Train Epoch: 7 [46080/60000]\tLoss: 0.037543\n",
      "Train Epoch: 7 [46720/60000]\tLoss: 0.181945\n",
      "Train Epoch: 7 [47360/60000]\tLoss: 0.064627\n",
      "Train Epoch: 7 [48000/60000]\tLoss: 0.146576\n",
      "Train Epoch: 7 [48640/60000]\tLoss: 0.079291\n",
      "Train Epoch: 7 [49280/60000]\tLoss: 0.070566\n",
      "Train Epoch: 7 [49920/60000]\tLoss: 0.124480\n",
      "Train Epoch: 7 [50560/60000]\tLoss: 0.080154\n",
      "Train Epoch: 7 [51200/60000]\tLoss: 0.155586\n",
      "Train Epoch: 7 [51840/60000]\tLoss: 0.034659\n",
      "Train Epoch: 7 [52480/60000]\tLoss: 0.185758\n",
      "Train Epoch: 7 [53120/60000]\tLoss: 0.188924\n",
      "Train Epoch: 7 [53760/60000]\tLoss: 0.058956\n",
      "Train Epoch: 7 [54400/60000]\tLoss: 0.040109\n",
      "Train Epoch: 7 [55040/60000]\tLoss: 0.048603\n",
      "Train Epoch: 7 [55680/60000]\tLoss: 0.059743\n",
      "Train Epoch: 7 [56320/60000]\tLoss: 0.160940\n",
      "Train Epoch: 7 [56960/60000]\tLoss: 0.079071\n",
      "Train Epoch: 7 [57600/60000]\tLoss: 0.064118\n",
      "Train Epoch: 7 [58240/60000]\tLoss: 0.031922\n",
      "Train Epoch: 7 [58880/60000]\tLoss: 0.214876\n",
      "Train Epoch: 7 [59520/60000]\tLoss: 0.169885\n",
      "Train Epoch: 8 [0/60000]\tLoss: 0.119171\n",
      "Train Epoch: 8 [640/60000]\tLoss: 0.024029\n",
      "Train Epoch: 8 [1280/60000]\tLoss: 0.025114\n",
      "Train Epoch: 8 [1920/60000]\tLoss: 0.036738\n",
      "Train Epoch: 8 [2560/60000]\tLoss: 0.091492\n",
      "Train Epoch: 8 [3200/60000]\tLoss: 0.194539\n",
      "Train Epoch: 8 [3840/60000]\tLoss: 0.091162\n",
      "Train Epoch: 8 [4480/60000]\tLoss: 0.132496\n",
      "Train Epoch: 8 [5120/60000]\tLoss: 0.050965\n",
      "Train Epoch: 8 [5760/60000]\tLoss: 0.144334\n",
      "Train Epoch: 8 [6400/60000]\tLoss: 0.044226\n",
      "Train Epoch: 8 [7040/60000]\tLoss: 0.071641\n",
      "Train Epoch: 8 [7680/60000]\tLoss: 0.183585\n",
      "Train Epoch: 8 [8320/60000]\tLoss: 0.175082\n",
      "Train Epoch: 8 [8960/60000]\tLoss: 0.084248\n",
      "Train Epoch: 8 [9600/60000]\tLoss: 0.154516\n",
      "Train Epoch: 8 [10240/60000]\tLoss: 0.052033\n",
      "Train Epoch: 8 [10880/60000]\tLoss: 0.072706\n",
      "Train Epoch: 8 [11520/60000]\tLoss: 0.037091\n",
      "Train Epoch: 8 [12160/60000]\tLoss: 0.100300\n",
      "Train Epoch: 8 [12800/60000]\tLoss: 0.036755\n",
      "Train Epoch: 8 [13440/60000]\tLoss: 0.058556\n",
      "Train Epoch: 8 [14080/60000]\tLoss: 0.118979\n",
      "Train Epoch: 8 [14720/60000]\tLoss: 0.042840\n",
      "Train Epoch: 8 [15360/60000]\tLoss: 0.068587\n",
      "Train Epoch: 8 [16000/60000]\tLoss: 0.047887\n",
      "Train Epoch: 8 [16640/60000]\tLoss: 0.086882\n",
      "Train Epoch: 8 [17280/60000]\tLoss: 0.185901\n",
      "Train Epoch: 8 [17920/60000]\tLoss: 0.321837\n",
      "Train Epoch: 8 [18560/60000]\tLoss: 0.072990\n",
      "Train Epoch: 8 [19200/60000]\tLoss: 0.040622\n",
      "Train Epoch: 8 [19840/60000]\tLoss: 0.204352\n",
      "Train Epoch: 8 [20480/60000]\tLoss: 0.063428\n",
      "Train Epoch: 8 [21120/60000]\tLoss: 0.086477\n",
      "Train Epoch: 8 [21760/60000]\tLoss: 0.075432\n",
      "Train Epoch: 8 [22400/60000]\tLoss: 0.055545\n",
      "Train Epoch: 8 [23040/60000]\tLoss: 0.050742\n",
      "Train Epoch: 8 [23680/60000]\tLoss: 0.056742\n",
      "Train Epoch: 8 [24320/60000]\tLoss: 0.067232\n",
      "Train Epoch: 8 [24960/60000]\tLoss: 0.028196\n",
      "Train Epoch: 8 [25600/60000]\tLoss: 0.120716\n",
      "Train Epoch: 8 [26240/60000]\tLoss: 0.080986\n",
      "Train Epoch: 8 [26880/60000]\tLoss: 0.096545\n",
      "Train Epoch: 8 [27520/60000]\tLoss: 0.042738\n",
      "Train Epoch: 8 [28160/60000]\tLoss: 0.132910\n",
      "Train Epoch: 8 [28800/60000]\tLoss: 0.043888\n",
      "Train Epoch: 8 [29440/60000]\tLoss: 0.029903\n",
      "Train Epoch: 8 [30080/60000]\tLoss: 0.218379\n",
      "Train Epoch: 8 [30720/60000]\tLoss: 0.133265\n",
      "Train Epoch: 8 [31360/60000]\tLoss: 0.072264\n",
      "Train Epoch: 8 [32000/60000]\tLoss: 0.031638\n",
      "Train Epoch: 8 [32640/60000]\tLoss: 0.031980\n",
      "Train Epoch: 8 [33280/60000]\tLoss: 0.158899\n",
      "Train Epoch: 8 [33920/60000]\tLoss: 0.058820\n",
      "Train Epoch: 8 [34560/60000]\tLoss: 0.047649\n",
      "Train Epoch: 8 [35200/60000]\tLoss: 0.035804\n",
      "Train Epoch: 8 [35840/60000]\tLoss: 0.195995\n",
      "Train Epoch: 8 [36480/60000]\tLoss: 0.068110\n",
      "Train Epoch: 8 [37120/60000]\tLoss: 0.017325\n",
      "Train Epoch: 8 [37760/60000]\tLoss: 0.049196\n",
      "Train Epoch: 8 [38400/60000]\tLoss: 0.051840\n",
      "Train Epoch: 8 [39040/60000]\tLoss: 0.076317\n",
      "Train Epoch: 8 [39680/60000]\tLoss: 0.098583\n",
      "Train Epoch: 8 [40320/60000]\tLoss: 0.117527\n",
      "Train Epoch: 8 [40960/60000]\tLoss: 0.270166\n",
      "Train Epoch: 8 [41600/60000]\tLoss: 0.120459\n",
      "Train Epoch: 8 [42240/60000]\tLoss: 0.061157\n",
      "Train Epoch: 8 [42880/60000]\tLoss: 0.132475\n",
      "Train Epoch: 8 [43520/60000]\tLoss: 0.255440\n",
      "Train Epoch: 8 [44160/60000]\tLoss: 0.092885\n",
      "Train Epoch: 8 [44800/60000]\tLoss: 0.076238\n",
      "Train Epoch: 8 [45440/60000]\tLoss: 0.192575\n",
      "Train Epoch: 8 [46080/60000]\tLoss: 0.052787\n",
      "Train Epoch: 8 [46720/60000]\tLoss: 0.117108\n",
      "Train Epoch: 8 [47360/60000]\tLoss: 0.049464\n",
      "Train Epoch: 8 [48000/60000]\tLoss: 0.030389\n",
      "Train Epoch: 8 [48640/60000]\tLoss: 0.090346\n",
      "Train Epoch: 8 [49280/60000]\tLoss: 0.123956\n",
      "Train Epoch: 8 [49920/60000]\tLoss: 0.026361\n",
      "Train Epoch: 8 [50560/60000]\tLoss: 0.086961\n",
      "Train Epoch: 8 [51200/60000]\tLoss: 0.220127\n",
      "Train Epoch: 8 [51840/60000]\tLoss: 0.037223\n",
      "Train Epoch: 8 [52480/60000]\tLoss: 0.049604\n",
      "Train Epoch: 8 [53120/60000]\tLoss: 0.181578\n",
      "Train Epoch: 8 [53760/60000]\tLoss: 0.007611\n",
      "Train Epoch: 8 [54400/60000]\tLoss: 0.197549\n",
      "Train Epoch: 8 [55040/60000]\tLoss: 0.242110\n",
      "Train Epoch: 8 [55680/60000]\tLoss: 0.032405\n",
      "Train Epoch: 8 [56320/60000]\tLoss: 0.109742\n",
      "Train Epoch: 8 [56960/60000]\tLoss: 0.027858\n",
      "Train Epoch: 8 [57600/60000]\tLoss: 0.153384\n",
      "Train Epoch: 8 [58240/60000]\tLoss: 0.114952\n",
      "Train Epoch: 8 [58880/60000]\tLoss: 0.256771\n",
      "Train Epoch: 8 [59520/60000]\tLoss: 0.070858\n",
      "Train Epoch: 9 [0/60000]\tLoss: 0.028364\n",
      "Train Epoch: 9 [640/60000]\tLoss: 0.135960\n",
      "Train Epoch: 9 [1280/60000]\tLoss: 0.133292\n",
      "Train Epoch: 9 [1920/60000]\tLoss: 0.015982\n",
      "Train Epoch: 9 [2560/60000]\tLoss: 0.035282\n",
      "Train Epoch: 9 [3200/60000]\tLoss: 0.101878\n",
      "Train Epoch: 9 [3840/60000]\tLoss: 0.124510\n",
      "Train Epoch: 9 [4480/60000]\tLoss: 0.058168\n",
      "Train Epoch: 9 [5120/60000]\tLoss: 0.038652\n",
      "Train Epoch: 9 [5760/60000]\tLoss: 0.043657\n",
      "Train Epoch: 9 [6400/60000]\tLoss: 0.102131\n",
      "Train Epoch: 9 [7040/60000]\tLoss: 0.112424\n",
      "Train Epoch: 9 [7680/60000]\tLoss: 0.015419\n",
      "Train Epoch: 9 [8320/60000]\tLoss: 0.075250\n",
      "Train Epoch: 9 [8960/60000]\tLoss: 0.072672\n",
      "Train Epoch: 9 [9600/60000]\tLoss: 0.024283\n",
      "Train Epoch: 9 [10240/60000]\tLoss: 0.128913\n",
      "Train Epoch: 9 [10880/60000]\tLoss: 0.069221\n",
      "Train Epoch: 9 [11520/60000]\tLoss: 0.180824\n",
      "Train Epoch: 9 [12160/60000]\tLoss: 0.146686\n",
      "Train Epoch: 9 [12800/60000]\tLoss: 0.097323\n",
      "Train Epoch: 9 [13440/60000]\tLoss: 0.063556\n",
      "Train Epoch: 9 [14080/60000]\tLoss: 0.124586\n",
      "Train Epoch: 9 [14720/60000]\tLoss: 0.026176\n",
      "Train Epoch: 9 [15360/60000]\tLoss: 0.083629\n",
      "Train Epoch: 9 [16000/60000]\tLoss: 0.078250\n",
      "Train Epoch: 9 [16640/60000]\tLoss: 0.030433\n",
      "Train Epoch: 9 [17280/60000]\tLoss: 0.041765\n",
      "Train Epoch: 9 [17920/60000]\tLoss: 0.162314\n",
      "Train Epoch: 9 [18560/60000]\tLoss: 0.043733\n",
      "Train Epoch: 9 [19200/60000]\tLoss: 0.032154\n",
      "Train Epoch: 9 [19840/60000]\tLoss: 0.096797\n",
      "Train Epoch: 9 [20480/60000]\tLoss: 0.139358\n",
      "Train Epoch: 9 [21120/60000]\tLoss: 0.023832\n",
      "Train Epoch: 9 [21760/60000]\tLoss: 0.049608\n",
      "Train Epoch: 9 [22400/60000]\tLoss: 0.052685\n",
      "Train Epoch: 9 [23040/60000]\tLoss: 0.059027\n",
      "Train Epoch: 9 [23680/60000]\tLoss: 0.071745\n",
      "Train Epoch: 9 [24320/60000]\tLoss: 0.021836\n",
      "Train Epoch: 9 [24960/60000]\tLoss: 0.210770\n",
      "Train Epoch: 9 [25600/60000]\tLoss: 0.045908\n",
      "Train Epoch: 9 [26240/60000]\tLoss: 0.062716\n",
      "Train Epoch: 9 [26880/60000]\tLoss: 0.049284\n",
      "Train Epoch: 9 [27520/60000]\tLoss: 0.074632\n",
      "Train Epoch: 9 [28160/60000]\tLoss: 0.109196\n",
      "Train Epoch: 9 [28800/60000]\tLoss: 0.214418\n",
      "Train Epoch: 9 [29440/60000]\tLoss: 0.025727\n",
      "Train Epoch: 9 [30080/60000]\tLoss: 0.110755\n",
      "Train Epoch: 9 [30720/60000]\tLoss: 0.114948\n",
      "Train Epoch: 9 [31360/60000]\tLoss: 0.070357\n",
      "Train Epoch: 9 [32000/60000]\tLoss: 0.020509\n",
      "Train Epoch: 9 [32640/60000]\tLoss: 0.124151\n",
      "Train Epoch: 9 [33280/60000]\tLoss: 0.058024\n",
      "Train Epoch: 9 [33920/60000]\tLoss: 0.078043\n",
      "Train Epoch: 9 [34560/60000]\tLoss: 0.090809\n",
      "Train Epoch: 9 [35200/60000]\tLoss: 0.049811\n",
      "Train Epoch: 9 [35840/60000]\tLoss: 0.020177\n",
      "Train Epoch: 9 [36480/60000]\tLoss: 0.052364\n",
      "Train Epoch: 9 [37120/60000]\tLoss: 0.049749\n",
      "Train Epoch: 9 [37760/60000]\tLoss: 0.082402\n",
      "Train Epoch: 9 [38400/60000]\tLoss: 0.078544\n",
      "Train Epoch: 9 [39040/60000]\tLoss: 0.043089\n",
      "Train Epoch: 9 [39680/60000]\tLoss: 0.035614\n",
      "Train Epoch: 9 [40320/60000]\tLoss: 0.098486\n",
      "Train Epoch: 9 [40960/60000]\tLoss: 0.086967\n",
      "Train Epoch: 9 [41600/60000]\tLoss: 0.010648\n",
      "Train Epoch: 9 [42240/60000]\tLoss: 0.451704\n",
      "Train Epoch: 9 [42880/60000]\tLoss: 0.043173\n",
      "Train Epoch: 9 [43520/60000]\tLoss: 0.069502\n",
      "Train Epoch: 9 [44160/60000]\tLoss: 0.042634\n",
      "Train Epoch: 9 [44800/60000]\tLoss: 0.035118\n",
      "Train Epoch: 9 [45440/60000]\tLoss: 0.026166\n",
      "Train Epoch: 9 [46080/60000]\tLoss: 0.088413\n",
      "Train Epoch: 9 [46720/60000]\tLoss: 0.057436\n",
      "Train Epoch: 9 [47360/60000]\tLoss: 0.204681\n",
      "Train Epoch: 9 [48000/60000]\tLoss: 0.103572\n",
      "Train Epoch: 9 [48640/60000]\tLoss: 0.269976\n",
      "Train Epoch: 9 [49280/60000]\tLoss: 0.215020\n",
      "Train Epoch: 9 [49920/60000]\tLoss: 0.091408\n",
      "Train Epoch: 9 [50560/60000]\tLoss: 0.173801\n",
      "Train Epoch: 9 [51200/60000]\tLoss: 0.034174\n",
      "Train Epoch: 9 [51840/60000]\tLoss: 0.149251\n",
      "Train Epoch: 9 [52480/60000]\tLoss: 0.071037\n",
      "Train Epoch: 9 [53120/60000]\tLoss: 0.042335\n",
      "Train Epoch: 9 [53760/60000]\tLoss: 0.229200\n",
      "Train Epoch: 9 [54400/60000]\tLoss: 0.017864\n",
      "Train Epoch: 9 [55040/60000]\tLoss: 0.056014\n",
      "Train Epoch: 9 [55680/60000]\tLoss: 0.061039\n",
      "Train Epoch: 9 [56320/60000]\tLoss: 0.033108\n",
      "Train Epoch: 9 [56960/60000]\tLoss: 0.021756\n",
      "Train Epoch: 9 [57600/60000]\tLoss: 0.258451\n",
      "Train Epoch: 9 [58240/60000]\tLoss: 0.141790\n",
      "Train Epoch: 9 [58880/60000]\tLoss: 0.052707\n",
      "Train Epoch: 9 [59520/60000]\tLoss: 0.053482\n",
      "Train Epoch: 10 [0/60000]\tLoss: 0.035628\n",
      "Train Epoch: 10 [640/60000]\tLoss: 0.146743\n",
      "Train Epoch: 10 [1280/60000]\tLoss: 0.097441\n",
      "Train Epoch: 10 [1920/60000]\tLoss: 0.039634\n",
      "Train Epoch: 10 [2560/60000]\tLoss: 0.154960\n",
      "Train Epoch: 10 [3200/60000]\tLoss: 0.070018\n",
      "Train Epoch: 10 [3840/60000]\tLoss: 0.040743\n",
      "Train Epoch: 10 [4480/60000]\tLoss: 0.163148\n",
      "Train Epoch: 10 [5120/60000]\tLoss: 0.030014\n",
      "Train Epoch: 10 [5760/60000]\tLoss: 0.139679\n",
      "Train Epoch: 10 [6400/60000]\tLoss: 0.047169\n",
      "Train Epoch: 10 [7040/60000]\tLoss: 0.027505\n",
      "Train Epoch: 10 [7680/60000]\tLoss: 0.029494\n",
      "Train Epoch: 10 [8320/60000]\tLoss: 0.031160\n",
      "Train Epoch: 10 [8960/60000]\tLoss: 0.077153\n",
      "Train Epoch: 10 [9600/60000]\tLoss: 0.082701\n",
      "Train Epoch: 10 [10240/60000]\tLoss: 0.069163\n",
      "Train Epoch: 10 [10880/60000]\tLoss: 0.125765\n",
      "Train Epoch: 10 [11520/60000]\tLoss: 0.175016\n",
      "Train Epoch: 10 [12160/60000]\tLoss: 0.035612\n",
      "Train Epoch: 10 [12800/60000]\tLoss: 0.057174\n",
      "Train Epoch: 10 [13440/60000]\tLoss: 0.062241\n",
      "Train Epoch: 10 [14080/60000]\tLoss: 0.108288\n",
      "Train Epoch: 10 [14720/60000]\tLoss: 0.109123\n",
      "Train Epoch: 10 [15360/60000]\tLoss: 0.083237\n",
      "Train Epoch: 10 [16000/60000]\tLoss: 0.047204\n",
      "Train Epoch: 10 [16640/60000]\tLoss: 0.111965\n",
      "Train Epoch: 10 [17280/60000]\tLoss: 0.069359\n",
      "Train Epoch: 10 [17920/60000]\tLoss: 0.040525\n",
      "Train Epoch: 10 [18560/60000]\tLoss: 0.013278\n",
      "Train Epoch: 10 [19200/60000]\tLoss: 0.027339\n",
      "Train Epoch: 10 [19840/60000]\tLoss: 0.093921\n",
      "Train Epoch: 10 [20480/60000]\tLoss: 0.146760\n",
      "Train Epoch: 10 [21120/60000]\tLoss: 0.044855\n",
      "Train Epoch: 10 [21760/60000]\tLoss: 0.032665\n",
      "Train Epoch: 10 [22400/60000]\tLoss: 0.114621\n",
      "Train Epoch: 10 [23040/60000]\tLoss: 0.060840\n",
      "Train Epoch: 10 [23680/60000]\tLoss: 0.074523\n",
      "Train Epoch: 10 [24320/60000]\tLoss: 0.037168\n",
      "Train Epoch: 10 [24960/60000]\tLoss: 0.144033\n",
      "Train Epoch: 10 [25600/60000]\tLoss: 0.010416\n",
      "Train Epoch: 10 [26240/60000]\tLoss: 0.081366\n",
      "Train Epoch: 10 [26880/60000]\tLoss: 0.032043\n",
      "Train Epoch: 10 [27520/60000]\tLoss: 0.052292\n",
      "Train Epoch: 10 [28160/60000]\tLoss: 0.050165\n",
      "Train Epoch: 10 [28800/60000]\tLoss: 0.054068\n",
      "Train Epoch: 10 [29440/60000]\tLoss: 0.031024\n",
      "Train Epoch: 10 [30080/60000]\tLoss: 0.024148\n",
      "Train Epoch: 10 [30720/60000]\tLoss: 0.096199\n",
      "Train Epoch: 10 [31360/60000]\tLoss: 0.027278\n",
      "Train Epoch: 10 [32000/60000]\tLoss: 0.128888\n",
      "Train Epoch: 10 [32640/60000]\tLoss: 0.065504\n",
      "Train Epoch: 10 [33280/60000]\tLoss: 0.051250\n",
      "Train Epoch: 10 [33920/60000]\tLoss: 0.083891\n",
      "Train Epoch: 10 [34560/60000]\tLoss: 0.460004\n",
      "Train Epoch: 10 [35200/60000]\tLoss: 0.130572\n",
      "Train Epoch: 10 [35840/60000]\tLoss: 0.020043\n",
      "Train Epoch: 10 [36480/60000]\tLoss: 0.058062\n",
      "Train Epoch: 10 [37120/60000]\tLoss: 0.059535\n",
      "Train Epoch: 10 [37760/60000]\tLoss: 0.116688\n",
      "Train Epoch: 10 [38400/60000]\tLoss: 0.028431\n",
      "Train Epoch: 10 [39040/60000]\tLoss: 0.099351\n",
      "Train Epoch: 10 [39680/60000]\tLoss: 0.037013\n",
      "Train Epoch: 10 [40320/60000]\tLoss: 0.162087\n",
      "Train Epoch: 10 [40960/60000]\tLoss: 0.201544\n",
      "Train Epoch: 10 [41600/60000]\tLoss: 0.060036\n",
      "Train Epoch: 10 [42240/60000]\tLoss: 0.023956\n",
      "Train Epoch: 10 [42880/60000]\tLoss: 0.099462\n",
      "Train Epoch: 10 [43520/60000]\tLoss: 0.264112\n",
      "Train Epoch: 10 [44160/60000]\tLoss: 0.128104\n",
      "Train Epoch: 10 [44800/60000]\tLoss: 0.074970\n",
      "Train Epoch: 10 [45440/60000]\tLoss: 0.016375\n",
      "Train Epoch: 10 [46080/60000]\tLoss: 0.093601\n",
      "Train Epoch: 10 [46720/60000]\tLoss: 0.080302\n",
      "Train Epoch: 10 [47360/60000]\tLoss: 0.020875\n",
      "Train Epoch: 10 [48000/60000]\tLoss: 0.064537\n",
      "Train Epoch: 10 [48640/60000]\tLoss: 0.064032\n",
      "Train Epoch: 10 [49280/60000]\tLoss: 0.179821\n",
      "Train Epoch: 10 [49920/60000]\tLoss: 0.090605\n",
      "Train Epoch: 10 [50560/60000]\tLoss: 0.119332\n",
      "Train Epoch: 10 [51200/60000]\tLoss: 0.064834\n",
      "Train Epoch: 10 [51840/60000]\tLoss: 0.047554\n",
      "Train Epoch: 10 [52480/60000]\tLoss: 0.184867\n",
      "Train Epoch: 10 [53120/60000]\tLoss: 0.024794\n",
      "Train Epoch: 10 [53760/60000]\tLoss: 0.040233\n",
      "Train Epoch: 10 [54400/60000]\tLoss: 0.063633\n",
      "Train Epoch: 10 [55040/60000]\tLoss: 0.012492\n",
      "Train Epoch: 10 [55680/60000]\tLoss: 0.147033\n",
      "Train Epoch: 10 [56320/60000]\tLoss: 0.048232\n",
      "Train Epoch: 10 [56960/60000]\tLoss: 0.135738\n",
      "Train Epoch: 10 [57600/60000]\tLoss: 0.131457\n",
      "Train Epoch: 10 [58240/60000]\tLoss: 0.040777\n",
      "Train Epoch: 10 [58880/60000]\tLoss: 0.050397\n",
      "Train Epoch: 10 [59520/60000]\tLoss: 0.081772\n",
      "\n",
      "Test set: Avg. loss: 0.0388, Accuracy: 9872/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and test Model 2\n",
    "\n",
    "# Create network\n",
    "model2 = Net2()\n",
    "# Initialize model weights\n",
    "model2.apply(weights_init)\n",
    "# Define optimizer\n",
    "optimizer = optim.SGD(model2.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "# Get initial performance\n",
    "test(model2)\n",
    "# Train for ten epochs\n",
    "n_epochs = 10\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  train(epoch, model2, optimizer)\n",
    "accuracy2 = test(model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFbCnAUmTwyx"
   },
   "source": [
    "## III. Results\n",
    "\n",
    "Here we train the CNN model and apply it to the test set. There are 10 epochs in training. There is no validation set here, we simply take the model at the end of the training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "JgAKHjLbqm3S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 Accuracy: 92.54%\n",
      "Model 2 Accuracy: 98.72%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model 1 Accuracy: {round(float(accuracy1.numpy()),2)}%\")\n",
    "print(f\"Model 2 Accuracy: {round(float(accuracy2.numpy()),2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "8hG1l1rSulbg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\codeamon\\AppData\\Local\\Temp\\ipykernel_106904\\1795650187.py:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.log_softmax(x)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run network on data we got before and show predictions\n",
    "output = model(example_data)\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(10):\n",
    "  plt.subplot(5,5,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Prediction: {}\".format(\n",
    "    output.data.max(1, keepdim=True)[1][i].item()))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "zUHLA7qru5cQ"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run network on data we got before and show predictions\n",
    "output = model2(example_data)\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(10):\n",
    "  plt.subplot(5,5,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Prediction: {}\".format(\n",
    "    output.data.max(1, keepdim=True)[1][i].item()))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0w7iym1T2QY"
   },
   "source": [
    "# IV. Conclusion and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this study, the object aim to classify handwritten digits from widly-used MNIST dataset by implementing custom convlutional neural networks (CNNs) in PyTorch.\n",
    "\n",
    "1. **Import Libraries**\n",
    "\n",
    "   Import essential libraries such as PyTorch, Torchvision, Matplotlib, and others required for building and training the models.\n",
    "\n",
    "2. **Load and Preprocess MNIST Dataset**\n",
    "   \n",
    "   The MNIST dataset is loaded from Torchvision libaray. The dataset consists of grayscale images of handwritten digits from 0 to 9, with corresponding labels. Data loaders are created for training and testing sets, facilitating batch processing for efficient model training. Apply neccessary transformations, such as converting images to tensors and normalizing pixel values based on the dataset's mean and standard deviation, are applied to ensure consistent input for the neural network.\n",
    "\n",
    "3. **Visualize Dataset Sample**\n",
    "   \n",
    "   To gain insight into the dataset, sample images from the first batch of the test set are displayed, along with their ground truth labels. This visualization helps us understand the diversity and the structure of the data.\n",
    "\n",
    "4. **Define Neural Network Architectures**:\n",
    "\n",
    "   Two neural networks, Net and Net2, are constructed for classifying MNIST digits. Each network has a different level of complexity, with Net2 incorporating a more sophisticated architecture, rendering it more capable of capturing intricate patterns within the MNIST dataset compared to Net. Two neural network architectures are defined as below.\n",
    "\n",
    "   - **Model 1 (`Net`)**: A simple CNN with one convolutional layer, a dropout layer, and a fully connected layer.\n",
    "   \n",
    "      ```bash=\n",
    "      ----------------------------------------------------------------\n",
    "            Layer (type)               Output Shape         Param #\n",
    "      ================================================================\n",
    "                  Conv2d-1            [-1, 2, 26, 26]              20\n",
    "               Dropout2d-2            [-1, 2, 26, 26]               0\n",
    "               MaxPool2d-3            [-1, 2, 13, 13]               0\n",
    "                    ReLU-4            [-1, 2, 13, 13]               0\n",
    "                  Linear-5                   [-1, 10]           3,390\n",
    "              LogSoftmax-6                   [-1, 10]               0\n",
    "      ================================================================\n",
    "      Total params: 3,410\n",
    "      Trainable params: 3,410\n",
    "      Non-trainable params: 0\n",
    "      ----------------------------------------------------------------\n",
    "      ```\n",
    "\n",
    "   - **Model 2 (`Net2`)**: A more complex CNN with two convolutional layers, dropout, pooling layers, and two fully connected layers.\n",
    "\n",
    "      ```bash=\n",
    "      ----------------------------------------------------------------\n",
    "            Layer (type)               Output Shape         Param #\n",
    "      ================================================================\n",
    "                  Conv2d-1           [-1, 10, 24, 24]             260\n",
    "                    ReLU-2           [-1, 10, 24, 24]               0\n",
    "               MaxPool2d-3           [-1, 10, 12, 12]               0\n",
    "                  Conv2d-4             [-1, 20, 8, 8]           5,020\n",
    "                    ReLU-5             [-1, 20, 8, 8]               0\n",
    "               MaxPool2d-6             [-1, 20, 4, 4]               0\n",
    "               Dropout2d-7             [-1, 20, 4, 4]               0\n",
    "                  Linear-8                   [-1, 50]          16,050\n",
    "                    ReLU-9                   [-1, 50]               0\n",
    "                 Linear-10                   [-1, 10]             510\n",
    "             LogSoftmax-11                   [-1, 10]               0\n",
    "      ================================================================\n",
    "      Total params: 21,840\n",
    "      Trainable params: 21,840\n",
    "      Non-trainable params: 0\n",
    "      ----------------------------------------------------------------\n",
    "      ```\n",
    "\n",
    "5. **Set Random Seed**\n",
    "\n",
    "   Fix the seed for reproducibility of results across runs.\n",
    "\n",
    "6. **Initialize Models and Set Hyperparameters**\n",
    "\n",
    "   Instantiate the models, apply weight initialization based on Kaiming He initialization, and set SGD with momentum as the optimizer to accelerate training and achieve faster convergence. Effective weight initialization can improve model convergence during the training, while the optimizer, using SGD with momentum, updates model parameters by minimizing the loss through computed gradients to accelerate training.\n",
    "\n",
    "7. **Train and Evaluate the Models**\n",
    "   \n",
    "   For each epoch\n",
    "\n",
    "   - Loop over the training data in batches.\n",
    "   - Perform forward pass to compute predictions.\n",
    "   - Compute the loss using negative log-likelihood.\n",
    "   - Backpropagate the loss and update the model weights using the optimizer.\n",
    "   - Print training progress and loss at intervals.\n",
    "\n",
    "   After training, evaluate each model on the test dataset\n",
    "\n",
    "   - Calculate the average loss and accuracy.\n",
    "   - Print out the performance metrics.\n",
    "\n",
    "8. **Display Results**\n",
    "\n",
    "   - Print the final accuracy of both models.\n",
    "   - Visualize predictions by the models on sample test images.\n",
    "\n",
    "9. **Conclusion and Discussion**\n",
    "\n",
    "   The test results reveal that Net2 outperforms Net in both accuracy and average loss, suggesting it is a more effective model for classifying MNIST handwritten digits task.\n",
    "\n",
    "   - Accuracy comparison: Net achieves an accuracy of 92.54%, and Net2 has an accuracy of 98.72%. This difference in accuracy (over 6%) is significant and implies that Net2 generalizes better to unseen data.\n",
    "   - Loss comparison: Net has an average loss of 0.2689, while Net2 has a much lower average loss of 0.0388. This significant reduction in loss for Net2 indicates not only greater prediction accuracy but also a higher confidence in its predictions.\n",
    "\n",
    "   In conclusion, Net2 provides a more accurate and reliable performance compared to Net, making it the preferable choice for deployment in applications requiring high accuracy. The difference in loss and accuracy between the two models suggest that Net2 has a more optimal architecture, better tuning, and other improvements that enhance its predictive power on this dataset."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
