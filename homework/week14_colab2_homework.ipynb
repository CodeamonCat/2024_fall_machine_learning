{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/udlbook/udlbook/blob/main/Notebooks/Chap12/12_2_Multihead_Self_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9vk9Elugvmi"
   },
   "source": [
    "# **Week 14: Colab Experiment on Multhead Self-Attention**\n",
    "\n",
    "This notebook builds a multihead self-attention mechanism as in figure 12.6\n",
    "\n",
    "Work through the cells below, running each cell in turn. In various places you will see the words \"TO DO\". Follow the instructions at these places and make predictions about what is going to happen or write code to complete the functions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "OLComQyvCIJ7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OJkkoNqCVK2"
   },
   "source": [
    "The multihead self-attention mechanism maps $N$ inputs $\\mathbf{x}_{n}\\in\\mathbb{R}^{D}$ and returns $N$ outputs $\\mathbf{x}'_{n}\\in \\mathbb{R}^{D}$.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oAygJwLiCSri",
    "outputId": "5e6501a4-4061-423e-f7ee-6dee930b9ebe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.789  0.437  0.096 -1.863 -0.277 -0.355]\n",
      " [-0.083 -0.627 -0.044 -0.477 -1.314  0.885]\n",
      " [ 0.881  1.71   0.05  -0.405 -0.545 -1.546]\n",
      " [ 0.982 -1.101 -1.185 -0.206  1.486  0.237]\n",
      " [-1.024 -0.713  0.625 -0.161 -0.769 -0.23 ]\n",
      " [ 0.745  1.976 -1.244 -0.626 -0.804 -2.419]\n",
      " [-0.924 -1.024  1.124 -0.132 -1.623  0.647]\n",
      " [-0.356 -1.743 -0.597 -0.589 -0.874  0.03 ]]\n"
     ]
    }
   ],
   "source": [
    "# Set seed so we get the same random numbers\n",
    "np.random.seed(3)\n",
    "# Number of inputs\n",
    "N = 6\n",
    "# Number of dimensions of each input\n",
    "D = 8\n",
    "# Create an empty list\n",
    "X = np.random.normal(size=(D,N))\n",
    "# Print X\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2iHFbtKMaDp"
   },
   "source": [
    "We'll use two heads.  We'll need the weights and biases for the keys, queries, and values (equations 12.2 and 12.4).  We'll use two heads, and (as in the figure), we'll make the queries keys and values of size D/H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "79TSK7oLMobe"
   },
   "outputs": [],
   "source": [
    "# Number of heads\n",
    "H = 2\n",
    "# QDV dimension\n",
    "H_D = int(D/H)\n",
    "\n",
    "# Set seed so we get the same random numbers\n",
    "np.random.seed(0)\n",
    "\n",
    "# Choose random values for the parameters for the first head\n",
    "omega_q1 = np.random.normal(size=(H_D,D))\n",
    "omega_k1 = np.random.normal(size=(H_D,D))\n",
    "omega_v1 = np.random.normal(size=(H_D,D))\n",
    "beta_q1 = np.random.normal(size=(H_D,1))\n",
    "beta_k1 = np.random.normal(size=(H_D,1))\n",
    "beta_v1 = np.random.normal(size=(H_D,1))\n",
    "\n",
    "# Choose random values for the parameters for the second head\n",
    "omega_q2 = np.random.normal(size=(H_D,D))\n",
    "omega_k2 = np.random.normal(size=(H_D,D))\n",
    "omega_v2 = np.random.normal(size=(H_D,D))\n",
    "beta_q2 = np.random.normal(size=(H_D,1))\n",
    "beta_k2 = np.random.normal(size=(H_D,1))\n",
    "beta_v2 = np.random.normal(size=(H_D,1))\n",
    "\n",
    "# Choose random values for the parameters\n",
    "omega_c = np.random.normal(size=(D,D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxaKQtP3Ng6R"
   },
   "source": [
    "Now let's compute the multiscale self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "obaQBdUAMXXv"
   },
   "outputs": [],
   "source": [
    "# Define softmax operation that works independently on each column\n",
    "def softmax_cols(data_in):\n",
    "  # Exponentiate all of the values\n",
    "  exp_values = np.exp(data_in) ;\n",
    "  # Sum over columns\n",
    "  denom = np.sum(exp_values, axis = 0);\n",
    "  # Replicate denominator to N rows\n",
    "  denom = np.matmul(np.ones((data_in.shape[0],1)), denom[np.newaxis,:])\n",
    "  # Compute softmax\n",
    "  softmax = exp_values / denom\n",
    "  # return the answer\n",
    "  return softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "gb2WvQ3SiH8r"
   },
   "outputs": [],
   "source": [
    " # Now let's compute self attention in matrix form\n",
    "def multihead_scaled_self_attention(X,omega_v1, omega_q1, omega_k1, beta_v1, beta_q1, beta_k1, omega_v2, omega_q2, omega_k2, beta_v2, beta_q2, beta_k2, omega_c):\n",
    "\n",
    "  # TODO Write the multihead scaled self-attention mechanism.\n",
    "  # 1. Compute queries, keys, and values\n",
    "  # 2. Apply softmax to calculate attentions and weight values by attentions\n",
    "  # 3. Concatenate the self-attentions and apply linear transformation to combine them\n",
    "  # Replace this line\n",
    "  X_prime = np.zeros_like(X) ;\n",
    "\n",
    "  # Compute queries, keys, and values for both heads\n",
    "  Q1 = np.dot(omega_q1, X) + beta_q1\n",
    "  K1 = np.dot(omega_k1, X) + beta_k1\n",
    "  V1 = np.dot(omega_v1, X) + beta_v1\n",
    "\n",
    "  Q2 = np.dot(omega_q2, X) + beta_q2\n",
    "  K2 = np.dot(omega_k2, X) + beta_k2\n",
    "  V2 = np.dot(omega_v2, X) + beta_v2\n",
    "\n",
    "  # Compute attention weights\n",
    "  attention_weights1 = softmax_cols(np.dot(K1.T, Q1) / np.sqrt(H_D))\n",
    "  attention_weights2 = softmax_cols(np.dot(K2.T, Q2) / np.sqrt(H_D))\n",
    "\n",
    "  # Compute weighted values\n",
    "  weighted_values1 = np.dot(V1, attention_weights1)\n",
    "  weighted_values2 = np.dot(V2, attention_weights2)\n",
    "\n",
    "  # Concatenate the weighted values from both heads\n",
    "  concatenated_values = np.concatenate((weighted_values1, weighted_values2), axis=0)\n",
    "\n",
    "  # Apply the final linear transformation\n",
    "  X_prime = np.dot(omega_c, concatenated_values)\n",
    "\n",
    "  return X_prime\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MUOJbgJskUpl",
    "outputId": "4480b9bb-eee1-4a92-8705-c69b22de3d20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your answer:\n",
      "[[-21.207  -5.373 -20.933  -9.179 -11.319 -17.812]\n",
      " [ -1.995   7.906 -10.516   3.452   9.863  -7.24 ]\n",
      " [  5.479   1.115   9.244   0.453   5.656   7.089]\n",
      " [ -7.413  -7.416   0.363  -5.573  -6.736  -0.848]\n",
      " [-11.261  -9.937  -4.848  -8.915 -13.378  -5.761]\n",
      " [  3.548  10.036  -2.244   1.604  12.113  -2.557]\n",
      " [  4.888  -5.814   2.407   3.228  -4.232   3.71 ]\n",
      " [  1.248  18.894  -6.409   3.224  19.717  -5.629]]\n",
      "True values:\n",
      "[[-21.207  -5.373 -20.933  -9.179 -11.319 -17.812]\n",
      " [ -1.995   7.906 -10.516   3.452   9.863  -7.24 ]\n",
      " [  5.479   1.115   9.244   0.453   5.656   7.089]\n",
      " [ -7.413  -7.416   0.363  -5.573  -6.736  -0.848]\n",
      " [-11.261  -9.937  -4.848  -8.915 -13.378  -5.761]\n",
      " [  3.548  10.036  -2.244   1.604  12.113  -2.557]\n",
      " [  4.888  -5.814   2.407   3.228  -4.232   3.71 ]\n",
      " [  1.248  18.894  -6.409   3.224  19.717  -5.629]]\n"
     ]
    }
   ],
   "source": [
    "# Run the self attention mechanism\n",
    "X_prime = multihead_scaled_self_attention(X,omega_v1, omega_q1, omega_k1, beta_v1, beta_q1, beta_k1, omega_v2, omega_q2, omega_k2, beta_v2, beta_q2, beta_k2, omega_c)\n",
    "\n",
    "# Print out the results\n",
    "np.set_printoptions(precision=3)\n",
    "print(\"Your answer:\")\n",
    "print(X_prime)\n",
    "\n",
    "print(\"True values:\")\n",
    "print(\"[[-21.207  -5.373 -20.933  -9.179 -11.319 -17.812]\")\n",
    "print(\" [ -1.995   7.906 -10.516   3.452   9.863  -7.24 ]\")\n",
    "print(\" [  5.479   1.115   9.244   0.453   5.656   7.089]\")\n",
    "print(\" [ -7.413  -7.416   0.363  -5.573  -6.736  -0.848]\")\n",
    "print(\" [-11.261  -9.937  -4.848  -8.915 -13.378  -5.761]\")\n",
    "print(\" [  3.548  10.036  -2.244   1.604  12.113  -2.557]\")\n",
    "print(\" [  4.888  -5.814   2.407   3.228  -4.232   3.71 ]\")\n",
    "print(\" [  1.248  18.894  -6.409   3.224  19.717  -5.629]]\")\n",
    "\n",
    "# If your answers don't match, then make sure that you are doing the scaling, and make sure the scaling value is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion and Discussion\n",
    "\n",
    "The purpose of this task is to implement a multi-head self-attention mechanism from scratch, demonstrating how multiple attention heads can capture different aspects of relationships between input vectors.\n",
    "\n",
    "1. Initialize inputs and parameters by generating random input vectors and initialize weights and biases for the query, key, and value transformations.\n",
    "2. Implement a column-wise softmax function `softmax_cols` to normalize attention scores across each column independently.\n",
    "3. Implement multi-head scaled self-attention in the matrix form based on the equation below. \n",
    "$$\n",
    "\\begin{align*}\n",
    "V_h&=\\beta_{vh}1^T+\\Omega_{vh}\\mathbf{X} \\\\\n",
    "Q_h&=\\beta_{qh}1^T+\\Omega_{qh}\\mathbf{X} \\\\\n",
    "K_h&=\\beta_{kh}1^T+\\Omega_{kh}\\mathbf{X} \\\\\n",
    "MhSa[\\mathbf{X}]&=\\Omega_C[Sa_1[\\mathbf{X}]^T,Sa_2[\\mathbf{X}]^T,\\dots,Sa_H[\\mathbf{X}]^T]^T\n",
    "\\end{align*}\n",
    "$$\n",
    "4. Compare the computed results with the expected true values to verify the correctness of the implementation."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
