{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b7758c2-9824-4b5a-9f0b-6b657cab79f1",
   "metadata": {
    "id": "0b7758c2-9824-4b5a-9f0b-6b657cab79f1"
   },
   "source": [
    "# Week 14: Colab Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2001f926-2262-4c66-8abb-5eb2dd13766d",
   "metadata": {
    "id": "2001f926-2262-4c66-8abb-5eb2dd13766d"
   },
   "source": [
    "# I. Introduction\n",
    "In this exercise, we first train a transformer using the Wikitext-2 dataset and then use the model to generate new text with the length specified by the user.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aa8f46-ff8b-47be-9bcc-12d5df8ec4d6",
   "metadata": {
    "id": "c7aa8f46-ff8b-47be-9bcc-12d5df8ec4d6"
   },
   "source": [
    "# II. Methods\n",
    "\n",
    "What is the model architecture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4fb45f3e-9428-4353-a8cf-0ee4ae37cfa7",
   "metadata": {
    "id": "4fb45f3e-9428-4353-a8cf-0ee4ae37cfa7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a0ea711-14fe-4d85-892a-4f6c871bb81b",
   "metadata": {
    "id": "2a0ea711-14fe-4d85-892a-4f6c871bb81b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment one of the following that works for you.\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "# device = torch.device(\"mps\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "024af44e-ee91-4d32-aa01-feb09f9dddcf",
   "metadata": {
    "id": "024af44e-ee91-4d32-aa01-feb09f9dddcf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "\n",
    "emsize = 200 # size of word embeddings\n",
    "nhead = 2\n",
    "nhid = 200\n",
    "nlayers = 2\n",
    "dropout = 0.2\n",
    "# lr = 20 # initial learning rate\n",
    "lr = 5\n",
    "epochs=10 # upper epoch limit\n",
    "\n",
    "bptt=35 #sequence length\n",
    "clip=0.25 #gradient clipping\n",
    "log_interval=200 # report interval\n",
    "\n",
    "save='model.pt' #path to save the final model\n",
    "\n",
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(0)\n",
    "\n",
    "eval_batch_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e435b94-c544-447b-8213-21f4c64fafef",
   "metadata": {
    "id": "5e435b94-c544-447b-8213-21f4c64fafef"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "pHGuUFWZ7mpw",
   "metadata": {
    "id": "pHGuUFWZ7mpw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/codeamon/desktop/ML/homework\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "import sys\n",
    "# sys.path.append('/content/drive/MyDrive/Week14/Week14/') # Change to your own path\n",
    "sys.path.append(os.getcwd())\n",
    "print(os.getcwd())\n",
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "382d8c86-7df9-4653-af82-b2515861e362",
   "metadata": {
    "id": "382d8c86-7df9-4653-af82-b2515861e362",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# corpus = data.Corpus('/content/drive/My Drive/Week14/Week14/data/wikitext-2')\n",
    "corpus = data.Corpus('../data/wikitext-2') \n",
    "\n",
    "def batchify(data, bsz):\n",
    "    nbatch = data.size(0) // bsz\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "train_data = batchify(corpus.train, batch_size)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)\n",
    "ntokens = len(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f968342b-ee5f-41b7-aa97-6b7eaefc3eda",
   "metadata": {
    "id": "f968342b-ee5f-41b7-aa97-6b7eaefc3eda"
   },
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a036d973-53ba-4f4b-abfa-188da6008ab3",
   "metadata": {
    "id": "a036d973-53ba-4f4b-abfa-188da6008ab3"
   },
   "outputs": [],
   "source": [
    "# Define positional encoding used in the transformer model\n",
    "\n",
    "#################################################################################################\n",
    "# [TODO]: Build a positional encoding function that can be used in the TransformerModel below\n",
    "#################################################################################################\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "724cbe76-35db-422e-9de9-21a2ff4f5798",
   "metadata": {
    "id": "724cbe76-35db-422e-9de9-21a2ff4f5798"
   },
   "outputs": [],
   "source": [
    "# Define the transformer model\n",
    "\n",
    "class TransformerModel(nn.Transformer):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__(d_model=ninp, nhead=nhead, dim_feedforward=nhid, num_encoder_layers=nlayers)\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout) # This is what you had constructed above\n",
    "\n",
    "        # Define the encoder layers\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "\n",
    "        self.input_emb = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        return torch.log(torch.tril(torch.ones(sz,sz)))\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.input_emb.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.bias)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, src, has_mask=True):\n",
    "        if has_mask:\n",
    "            device = src.device\n",
    "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "                mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "                self.src_mask = mask\n",
    "        else:\n",
    "            self.src_mask = None\n",
    "\n",
    "        src = self.input_emb(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.encoder(src, mask=self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return F.log_softmax(output, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c8a12a5-50b2-4f78-9f2a-c98e3a746e30",
   "metadata": {
    "id": "8c8a12a5-50b2-4f78-9f2a-c98e3a746e30"
   },
   "outputs": [],
   "source": [
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7378de-eeac-445c-8f15-768681785fcd",
   "metadata": {
    "id": "6a7378de-eeac-445c-8f15-768681785fcd"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1facb91b-6a0d-42b2-9cec-600fc15f27d6",
   "metadata": {
    "id": "1facb91b-6a0d-42b2-9cec-600fc15f27d6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2983 batches | lr 5.00 | ms/batch  2.87 | loss  7.59 | ppl  1978.65\n",
      "| epoch   1 |   400/ 2983 batches | lr 5.00 | ms/batch  2.81 | loss  6.79 | ppl   892.93\n",
      "| epoch   1 |   600/ 2983 batches | lr 5.00 | ms/batch  2.82 | loss  6.50 | ppl   663.33\n",
      "| epoch   1 |   800/ 2983 batches | lr 5.00 | ms/batch  2.81 | loss  6.36 | ppl   575.48\n",
      "| epoch   1 |  1000/ 2983 batches | lr 5.00 | ms/batch  2.83 | loss  6.25 | ppl   520.25\n",
      "| epoch   1 |  1200/ 2983 batches | lr 5.00 | ms/batch  2.79 | loss  6.22 | ppl   503.55\n",
      "| epoch   1 |  1400/ 2983 batches | lr 5.00 | ms/batch  2.78 | loss  6.14 | ppl   465.24\n",
      "| epoch   1 |  1600/ 2983 batches | lr 5.00 | ms/batch  2.79 | loss  6.15 | ppl   470.38\n",
      "| epoch   1 |  1800/ 2983 batches | lr 5.00 | ms/batch  2.79 | loss  6.03 | ppl   415.69\n",
      "| epoch   1 |  2000/ 2983 batches | lr 5.00 | ms/batch  2.79 | loss  6.03 | ppl   415.04\n",
      "| epoch   1 |  2200/ 2983 batches | lr 5.00 | ms/batch  2.79 | loss  5.92 | ppl   372.02\n",
      "| epoch   1 |  2400/ 2983 batches | lr 5.00 | ms/batch  2.79 | loss  5.93 | ppl   376.49\n",
      "| epoch   1 |  2600/ 2983 batches | lr 5.00 | ms/batch  2.79 | loss  5.93 | ppl   377.47\n",
      "| epoch   1 |  2800/ 2983 batches | lr 5.00 | ms/batch  2.79 | loss  5.84 | ppl   344.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time:  8.68s | valid loss  5.74 | valid ppl   311.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2983 batches | lr 5.00 | ms/batch  2.94 | loss  5.79 | ppl   327.31\n",
      "| epoch   2 |   400/ 2983 batches | lr 5.00 | ms/batch  2.81 | loss  5.76 | ppl   317.73\n",
      "| epoch   2 |   600/ 2983 batches | lr 5.00 | ms/batch  2.80 | loss  5.61 | ppl   274.47\n",
      "| epoch   2 |   800/ 2983 batches | lr 5.00 | ms/batch  2.80 | loss  5.62 | ppl   277.03\n",
      "| epoch   2 |  1000/ 2983 batches | lr 5.00 | ms/batch  2.80 | loss  5.60 | ppl   270.31\n",
      "| epoch   2 |  1200/ 2983 batches | lr 5.00 | ms/batch  2.81 | loss  5.61 | ppl   273.85\n",
      "| epoch   2 |  1400/ 2983 batches | lr 5.00 | ms/batch  2.81 | loss  5.62 | ppl   274.92\n",
      "| epoch   2 |  1600/ 2983 batches | lr 5.00 | ms/batch  2.81 | loss  5.66 | ppl   285.76\n",
      "| epoch   2 |  1800/ 2983 batches | lr 5.00 | ms/batch  2.81 | loss  5.55 | ppl   256.55\n",
      "| epoch   2 |  2000/ 2983 batches | lr 5.00 | ms/batch  2.81 | loss  5.58 | ppl   264.28\n",
      "| epoch   2 |  2200/ 2983 batches | lr 5.00 | ms/batch  2.80 | loss  5.48 | ppl   240.68\n",
      "| epoch   2 |  2400/ 2983 batches | lr 5.00 | ms/batch  2.77 | loss  5.51 | ppl   248.14\n",
      "| epoch   2 |  2600/ 2983 batches | lr 5.00 | ms/batch  2.77 | loss  5.53 | ppl   250.95\n",
      "| epoch   2 |  2800/ 2983 batches | lr 5.00 | ms/batch  2.77 | loss  5.45 | ppl   233.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time:  8.69s | valid loss  5.53 | valid ppl   251.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2983 batches | lr 5.00 | ms/batch  2.84 | loss  5.46 | ppl   234.80\n",
      "| epoch   3 |   400/ 2983 batches | lr 5.00 | ms/batch  2.79 | loss  5.46 | ppl   234.67\n",
      "| epoch   3 |   600/ 2983 batches | lr 5.00 | ms/batch  2.81 | loss  5.29 | ppl   198.13\n",
      "| epoch   3 |   800/ 2983 batches | lr 5.00 | ms/batch  2.80 | loss  5.33 | ppl   206.27\n",
      "| epoch   3 |  1000/ 2983 batches | lr 5.00 | ms/batch  2.81 | loss  5.31 | ppl   201.80\n",
      "| epoch   3 |  1200/ 2983 batches | lr 5.00 | ms/batch  2.81 | loss  5.33 | ppl   205.85\n",
      "| epoch   3 |  1400/ 2983 batches | lr 5.00 | ms/batch  2.81 | loss  5.36 | ppl   211.77\n",
      "| epoch   3 |  1600/ 2983 batches | lr 5.00 | ms/batch  2.81 | loss  5.40 | ppl   221.84\n",
      "| epoch   3 |  1800/ 2983 batches | lr 5.00 | ms/batch  2.81 | loss  5.29 | ppl   198.39\n",
      "| epoch   3 |  2000/ 2983 batches | lr 5.00 | ms/batch  2.81 | loss  5.33 | ppl   206.75\n",
      "| epoch   3 |  2200/ 2983 batches | lr 5.00 | ms/batch  2.81 | loss  5.23 | ppl   186.48\n",
      "| epoch   3 |  2400/ 2983 batches | lr 5.00 | ms/batch  2.81 | loss  5.27 | ppl   194.30\n",
      "| epoch   3 |  2600/ 2983 batches | lr 5.00 | ms/batch  2.81 | loss  5.29 | ppl   198.22\n",
      "| epoch   3 |  2800/ 2983 batches | lr 5.00 | ms/batch  2.82 | loss  5.22 | ppl   184.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time:  8.71s | valid loss  5.45 | valid ppl   231.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 2983 batches | lr 5.00 | ms/batch  2.87 | loss  5.24 | ppl   189.40\n",
      "| epoch   4 |   400/ 2983 batches | lr 5.00 | ms/batch  2.80 | loss  5.25 | ppl   190.76\n",
      "| epoch   4 |   600/ 2983 batches | lr 5.00 | ms/batch  2.79 | loss  5.07 | ppl   159.95\n",
      "| epoch   4 |   800/ 2983 batches | lr 5.00 | ms/batch  2.79 | loss  5.13 | ppl   168.89\n",
      "| epoch   4 |  1000/ 2983 batches | lr 5.00 | ms/batch  2.80 | loss  5.11 | ppl   164.96\n",
      "| epoch   4 |  1200/ 2983 batches | lr 5.00 | ms/batch  2.79 | loss  5.13 | ppl   169.28\n",
      "| epoch   4 |  1400/ 2983 batches | lr 5.00 | ms/batch  2.79 | loss  5.17 | ppl   176.35\n",
      "| epoch   4 |  1600/ 2983 batches | lr 5.00 | ms/batch  2.80 | loss  5.22 | ppl   184.54\n",
      "| epoch   4 |  1800/ 2983 batches | lr 5.00 | ms/batch  2.82 | loss  5.11 | ppl   166.31\n",
      "| epoch   4 |  2000/ 2983 batches | lr 5.00 | ms/batch  2.80 | loss  5.16 | ppl   173.32\n",
      "| epoch   4 |  2200/ 2983 batches | lr 5.00 | ms/batch  2.79 | loss  5.05 | ppl   155.48\n",
      "| epoch   4 |  2400/ 2983 batches | lr 5.00 | ms/batch  2.79 | loss  5.10 | ppl   163.30\n",
      "| epoch   4 |  2600/ 2983 batches | lr 5.00 | ms/batch  2.79 | loss  5.12 | ppl   167.48\n",
      "| epoch   4 |  2800/ 2983 batches | lr 5.00 | ms/batch  2.79 | loss  5.05 | ppl   155.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time:  8.67s | valid loss  5.39 | valid ppl   218.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 2983 batches | lr 5.00 | ms/batch  2.80 | loss  5.08 | ppl   160.73\n",
      "| epoch   5 |   400/ 2983 batches | lr 5.00 | ms/batch  2.77 | loss  5.09 | ppl   163.05\n",
      "| epoch   5 |   600/ 2983 batches | lr 5.00 | ms/batch  2.79 | loss  4.92 | ppl   136.37\n",
      "| epoch   5 |   800/ 2983 batches | lr 5.00 | ms/batch  2.77 | loss  4.97 | ppl   144.58\n",
      "| epoch   5 |  1000/ 2983 batches | lr 5.00 | ms/batch  2.77 | loss  4.96 | ppl   142.53\n",
      "| epoch   5 |  1200/ 2983 batches | lr 5.00 | ms/batch  2.77 | loss  4.98 | ppl   145.55\n",
      "| epoch   5 |  1400/ 2983 batches | lr 5.00 | ms/batch  2.80 | loss  5.03 | ppl   152.68\n",
      "| epoch   5 |  1600/ 2983 batches | lr 5.00 | ms/batch  2.82 | loss  5.08 | ppl   160.85\n",
      "| epoch   5 |  1800/ 2983 batches | lr 5.00 | ms/batch  2.82 | loss  4.98 | ppl   144.78\n",
      "| epoch   5 |  2000/ 2983 batches | lr 5.00 | ms/batch  2.82 | loss  5.01 | ppl   150.54\n",
      "| epoch   5 |  2200/ 2983 batches | lr 5.00 | ms/batch  2.96 | loss  4.91 | ppl   135.54\n",
      "| epoch   5 |  2400/ 2983 batches | lr 5.00 | ms/batch  2.80 | loss  4.96 | ppl   142.00\n",
      "| epoch   5 |  2600/ 2983 batches | lr 5.00 | ms/batch  2.80 | loss  4.98 | ppl   145.92\n",
      "| epoch   5 |  2800/ 2983 batches | lr 5.00 | ms/batch  2.79 | loss  4.91 | ppl   136.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time:  8.68s | valid loss  5.37 | valid ppl   214.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   200/ 2983 batches | lr 5.00 | ms/batch  2.84 | loss  4.95 | ppl   141.02\n",
      "| epoch   6 |   400/ 2983 batches | lr 5.00 | ms/batch  2.82 | loss  4.97 | ppl   143.70\n",
      "| epoch   6 |   600/ 2983 batches | lr 5.00 | ms/batch  2.82 | loss  4.79 | ppl   120.08\n",
      "| epoch   6 |   800/ 2983 batches | lr 5.00 | ms/batch  2.82 | loss  4.85 | ppl   127.47\n",
      "| epoch   6 |  1000/ 2983 batches | lr 5.00 | ms/batch  2.81 | loss  4.84 | ppl   126.30\n",
      "| epoch   6 |  1200/ 2983 batches | lr 5.00 | ms/batch  2.79 | loss  4.87 | ppl   129.75\n",
      "| epoch   6 |  1400/ 2983 batches | lr 5.00 | ms/batch  2.77 | loss  4.91 | ppl   135.75\n",
      "| epoch   6 |  1600/ 2983 batches | lr 5.00 | ms/batch  2.80 | loss  4.96 | ppl   142.59\n",
      "| epoch   6 |  1800/ 2983 batches | lr 5.00 | ms/batch  2.80 | loss  4.87 | ppl   129.69\n",
      "| epoch   6 |  2000/ 2983 batches | lr 5.00 | ms/batch  2.80 | loss  4.90 | ppl   134.54\n",
      "| epoch   6 |  2200/ 2983 batches | lr 5.00 | ms/batch  2.80 | loss  4.79 | ppl   120.73\n",
      "| epoch   6 |  2400/ 2983 batches | lr 5.00 | ms/batch  2.80 | loss  4.84 | ppl   126.49\n",
      "| epoch   6 |  2600/ 2983 batches | lr 5.00 | ms/batch  2.80 | loss  4.87 | ppl   130.83\n",
      "| epoch   6 |  2800/ 2983 batches | lr 5.00 | ms/batch  2.80 | loss  4.81 | ppl   122.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time:  8.68s | valid loss  5.36 | valid ppl   212.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   200/ 2983 batches | lr 5.00 | ms/batch  2.84 | loss  4.84 | ppl   126.35\n",
      "| epoch   7 |   400/ 2983 batches | lr 5.00 | ms/batch  2.82 | loss  4.86 | ppl   128.77\n",
      "| epoch   7 |   600/ 2983 batches | lr 5.00 | ms/batch  2.81 | loss  4.69 | ppl   108.41\n",
      "| epoch   7 |   800/ 2983 batches | lr 5.00 | ms/batch  2.80 | loss  4.75 | ppl   115.06\n",
      "| epoch   7 |  1000/ 2983 batches | lr 5.00 | ms/batch  2.81 | loss  4.74 | ppl   114.57\n",
      "| epoch   7 |  1200/ 2983 batches | lr 5.00 | ms/batch  2.80 | loss  4.77 | ppl   117.50\n",
      "| epoch   7 |  1400/ 2983 batches | lr 5.00 | ms/batch  2.80 | loss  4.81 | ppl   122.40\n",
      "| epoch   7 |  1600/ 2983 batches | lr 5.00 | ms/batch  2.81 | loss  4.86 | ppl   129.37\n",
      "| epoch   7 |  1800/ 2983 batches | lr 5.00 | ms/batch  2.79 | loss  4.77 | ppl   117.57\n",
      "| epoch   7 |  2000/ 2983 batches | lr 5.00 | ms/batch  2.80 | loss  4.80 | ppl   121.98\n",
      "| epoch   7 |  2200/ 2983 batches | lr 5.00 | ms/batch  2.80 | loss  4.70 | ppl   109.49\n",
      "| epoch   7 |  2400/ 2983 batches | lr 5.00 | ms/batch  2.80 | loss  4.75 | ppl   115.45\n",
      "| epoch   7 |  2600/ 2983 batches | lr 5.00 | ms/batch  2.79 | loss  4.78 | ppl   118.76\n",
      "| epoch   7 |  2800/ 2983 batches | lr 5.00 | ms/batch  2.79 | loss  4.72 | ppl   111.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time:  8.68s | valid loss  5.36 | valid ppl   212.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   200/ 2983 batches | lr 5.00 | ms/batch  2.84 | loss  4.75 | ppl   115.79\n",
      "| epoch   8 |   400/ 2983 batches | lr 5.00 | ms/batch  2.81 | loss  4.77 | ppl   117.61\n",
      "| epoch   8 |   600/ 2983 batches | lr 5.00 | ms/batch  2.92 | loss  4.59 | ppl    98.75\n",
      "| epoch   8 |   800/ 2983 batches | lr 5.00 | ms/batch  2.80 | loss  4.65 | ppl   105.07\n",
      "| epoch   8 |  1000/ 2983 batches | lr 5.00 | ms/batch  2.79 | loss  4.66 | ppl   105.73\n",
      "| epoch   8 |  1200/ 2983 batches | lr 5.00 | ms/batch  2.80 | loss  4.68 | ppl   107.93\n",
      "| epoch   8 |  1400/ 2983 batches | lr 5.00 | ms/batch  2.79 | loss  4.73 | ppl   112.81\n",
      "| epoch   8 |  1600/ 2983 batches | lr 5.00 | ms/batch  2.79 | loss  4.78 | ppl   118.60\n",
      "| epoch   8 |  1800/ 2983 batches | lr 5.00 | ms/batch  2.79 | loss  4.69 | ppl   108.40\n",
      "| epoch   8 |  2000/ 2983 batches | lr 5.00 | ms/batch  2.79 | loss  4.72 | ppl   111.90\n",
      "| epoch   8 |  2200/ 2983 batches | lr 5.00 | ms/batch  2.79 | loss  4.61 | ppl   100.81\n",
      "| epoch   8 |  2400/ 2983 batches | lr 5.00 | ms/batch  2.82 | loss  4.66 | ppl   106.14\n",
      "| epoch   8 |  2600/ 2983 batches | lr 5.00 | ms/batch  2.81 | loss  4.69 | ppl   109.29\n",
      "| epoch   8 |  2800/ 2983 batches | lr 5.00 | ms/batch  2.79 | loss  4.63 | ppl   102.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time:  8.69s | valid loss  5.39 | valid ppl   218.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   200/ 2983 batches | lr 1.25 | ms/batch  2.81 | loss  4.64 | ppl   103.92\n",
      "| epoch   9 |   400/ 2983 batches | lr 1.25 | ms/batch  2.80 | loss  4.63 | ppl   102.27\n",
      "| epoch   9 |   600/ 2983 batches | lr 1.25 | ms/batch  2.80 | loss  4.44 | ppl    85.02\n",
      "| epoch   9 |   800/ 2983 batches | lr 1.25 | ms/batch  2.81 | loss  4.49 | ppl    89.31\n",
      "| epoch   9 |  1000/ 2983 batches | lr 1.25 | ms/batch  2.81 | loss  4.48 | ppl    87.92\n",
      "| epoch   9 |  1200/ 2983 batches | lr 1.25 | ms/batch  2.80 | loss  4.48 | ppl    88.61\n",
      "| epoch   9 |  1400/ 2983 batches | lr 1.25 | ms/batch  2.80 | loss  4.51 | ppl    91.32\n",
      "| epoch   9 |  1600/ 2983 batches | lr 1.25 | ms/batch  2.80 | loss  4.55 | ppl    94.80\n",
      "| epoch   9 |  1800/ 2983 batches | lr 1.25 | ms/batch  2.80 | loss  4.45 | ppl    85.93\n",
      "| epoch   9 |  2000/ 2983 batches | lr 1.25 | ms/batch  2.82 | loss  4.48 | ppl    87.97\n",
      "| epoch   9 |  2200/ 2983 batches | lr 1.25 | ms/batch  2.81 | loss  4.35 | ppl    77.67\n",
      "| epoch   9 |  2400/ 2983 batches | lr 1.25 | ms/batch  2.80 | loss  4.39 | ppl    80.40\n",
      "| epoch   9 |  2600/ 2983 batches | lr 1.25 | ms/batch  2.80 | loss  4.41 | ppl    81.89\n",
      "| epoch   9 |  2800/ 2983 batches | lr 1.25 | ms/batch  2.80 | loss  4.34 | ppl    76.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time:  8.67s | valid loss  5.28 | valid ppl   196.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   200/ 2983 batches | lr 1.25 | ms/batch  2.84 | loss  4.51 | ppl    90.93\n",
      "| epoch  10 |   400/ 2983 batches | lr 1.25 | ms/batch  2.82 | loss  4.52 | ppl    91.93\n",
      "| epoch  10 |   600/ 2983 batches | lr 1.25 | ms/batch  2.82 | loss  4.34 | ppl    77.03\n",
      "| epoch  10 |   800/ 2983 batches | lr 1.25 | ms/batch  2.82 | loss  4.41 | ppl    81.93\n",
      "| epoch  10 |  1000/ 2983 batches | lr 1.25 | ms/batch  2.82 | loss  4.40 | ppl    81.49\n",
      "| epoch  10 |  1200/ 2983 batches | lr 1.25 | ms/batch  2.82 | loss  4.41 | ppl    82.49\n",
      "| epoch  10 |  1400/ 2983 batches | lr 1.25 | ms/batch  2.82 | loss  4.45 | ppl    85.56\n",
      "| epoch  10 |  1600/ 2983 batches | lr 1.25 | ms/batch  2.82 | loss  4.49 | ppl    88.98\n",
      "| epoch  10 |  1800/ 2983 batches | lr 1.25 | ms/batch  2.82 | loss  4.40 | ppl    81.15\n",
      "| epoch  10 |  2000/ 2983 batches | lr 1.25 | ms/batch  2.83 | loss  4.43 | ppl    83.66\n",
      "| epoch  10 |  2200/ 2983 batches | lr 1.25 | ms/batch  2.83 | loss  4.31 | ppl    74.19\n",
      "| epoch  10 |  2400/ 2983 batches | lr 1.25 | ms/batch  2.83 | loss  4.35 | ppl    77.61\n",
      "| epoch  10 |  2600/ 2983 batches | lr 1.25 | ms/batch  2.82 | loss  4.37 | ppl    78.98\n",
      "| epoch  10 |  2800/ 2983 batches | lr 1.25 | ms/batch  2.82 | loss  4.31 | ppl    74.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time:  8.74s | valid loss  5.27 | valid ppl   195.01\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_641835/2712397787.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  5.19 | test ppl   178.85\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target\n",
    "\n",
    "\n",
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output = model(data)\n",
    "            output = output.view(-1, ntokens)\n",
    "\n",
    "            total_loss += len(data) * criterion(output, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)\n",
    "\n",
    "\n",
    "def train():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        model.zero_grad()\n",
    "        output = model(data)\n",
    "        output = output.view(-1, ntokens)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(p.grad, alpha=-lr)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // bptt, lr,\n",
    "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "# Loop over epochs.\n",
    "best_val_loss = None\n",
    "\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "try:\n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss = evaluate(val_data)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open(save, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "            lr /= 4.0\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')\n",
    "\n",
    "# Load the best saved model.\n",
    "with open(save, 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "\n",
    "\n",
    "# Run on test data.\n",
    "test_loss = evaluate(test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d482a0-034b-4565-8e22-12e93d7171fe",
   "metadata": {
    "id": "83d482a0-034b-4565-8e22-12e93d7171fe"
   },
   "source": [
    "# III. Results\n",
    "Here we generate text of length 100 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2d803f87-c278-4a49-a73d-05f9d396d2dd",
   "metadata": {
    "id": "2d803f87-c278-4a49-a73d-05f9d396d2dd",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_641835/2648163791.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(f, map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
       "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Linear(in_features=200, out_features=33278, bias=True)\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (input_emb): Embedding(33278, 200)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words = 100\n",
    "temperature = 1\n",
    "\n",
    "\n",
    "g = torch.Generator().manual_seed(0)\n",
    "initial_state = g.get_state()\n",
    "\n",
    "with open('./model.pt', 'rb') as f:\n",
    "    model = torch.load(f, map_location=device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0b9eef62-b17a-495d-9f7a-24cdd230f445",
   "metadata": {
    "id": "0b9eef62-b17a-495d-9f7a-24cdd230f445",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Africa against Maryland , Ireland , Georgia in Scotland , Spain , and when 2002 was replaced by Union Mech Div , bridges were the largest user in which land . Among those who developed Tigernach records other players are large numbers were advocated such as a poor starlings were slowed @-@ enhancing the island 's wasps , West Branch as Dr. Patrick 's colours are also linking South Wales are still bears a big fewer males drowned . By 1997 USD ) returns to South Africa in such as part of greater concern to them thereafter , making the \n"
     ]
    }
   ],
   "source": [
    "g.set_state(initial_state)\n",
    "input = torch.randint(ntokens, (1, 1), dtype=torch.long, generator=g).to(device)\n",
    "\n",
    "\n",
    "generated_text = \"\"\n",
    "\n",
    "# Dictionary to map indices to words\n",
    "idx_to_word = corpus.dictionary.idx2word\n",
    "\n",
    "##################################################################################\n",
    "# [TODO] Fill out this section to use the transfer model to generate new text\n",
    "##################################################################################\n",
    "with torch.no_grad():\n",
    "    for i in range(num_words):\n",
    "        # 1. Predict next word probabilities.\n",
    "        output = model(input)\n",
    "        output = output[-1, 0, :]\n",
    "        # 2. Scale probabilities with temperature.\n",
    "        word_weights = output / temperature\n",
    "        word_probs = F.softmax(word_weights, dim=-1)\n",
    "        # 3. Sample the next word index.\n",
    "        next_word_idx = torch.multinomial(word_probs, num_samples=1).item()\n",
    "        # 4. Add sampled word to the input.\n",
    "        input = torch.cat([input, torch.tensor([[next_word_idx]], device=device)], dim=0)\n",
    "        # 5. Find the word for the index.\n",
    "        word = idx_to_word[next_word_idx]\n",
    "        # 6. Add word to the output text.\n",
    "        generated_text += word + \" \"\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ebf804-996b-4884-957a-bfa68a33baac",
   "metadata": {
    "id": "96ebf804-996b-4884-957a-bfa68a33baac"
   },
   "source": [
    "# IV. Conclusion and Discussion\n",
    "\n",
    "What did you find and learn in this excercise?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05923e4b",
   "metadata": {},
   "source": [
    "The purpose of the task is to generate new text sequences using a trained language model. It starts with an initial random word and builds a sequence by predicting and sampling subsequent words, ultimately producing a coherent piece of text. The model we used to train is a standard transformer module based on the paper `Attention is All You Need`.\n",
    "\n",
    "1. Import required libraries, initialize random state, and hyperparameters.\n",
    "2. Load `wikitext-2` dataset and split into training, testing, and validation sets.\n",
    "3. Build a transformer model architecture introduced by paper `Attention is All You Need` with some modifications. Since the language modeling task is to assign a probability for the likelihood of a given word to follow a sequence of words, only the encoder layers are trained in this task. Moreover, the PositionalEncoding module adds information about the relative or absolute positions of tokens in the sequence. The positional encodings are designed to have the same dimension as the embeddings, allowing them to be summed together. In this case, sine and cosine functions with varying frequencies are used.\n",
    "$$\n",
    "\\begin{align*}\n",
    "PE_{(\\text{pos},2i)}&=sin(pos/10000^{2i/d_{model}}) \\\\\n",
    "PE_{(\\text{pos},2i+1)}&=cos(pos/10000^{2i/d_{model}})\n",
    "\\end{align*}\n",
    "$$\n",
    "4. After building the model, we train the model on the `wikitext-2` dataset. The model, based on a transformer architecture with encoder layers, learns to predict the next word in a sequence. It processes input sequences by incorporating positional encodings, which use sine and cosine functions to encode the positions of words in the sequence. This allows the model to handle varying sequence lengths. During training, the model's predictions are compared to the actual next words using a loss function (like cross-entropy), and the model's parameters are adjusted through backpropagation.\n",
    "5. Once the model is trained, it can generate text by starting with a random word. The model predicts the next word in the sequence, appends it to the input, and continues predicting and appending words to generate a coherent piece of text.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
